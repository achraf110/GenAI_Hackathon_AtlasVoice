{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jpFn1dE3Yive",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773e302d-afc6-4040-ac08-4eb9c3c35568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# After running this cell, you will be prompted to authorize access to your Google Drive.\n",
        "# Follow the link, copy the authorization code, and paste it in the input field.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "xnlWckHOYrGA"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.metrics\n",
        "import os\n",
        "from glob import glob\n",
        "import sklearn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "u8OomNnnYvud"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def _framing(x, win_len, overlap):\n",
        "    \"\"\"\n",
        "    Divides input signal into overlapping frames of equal length\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : input signal\n",
        "    win_len : frame length\n",
        "    overlap : overlapping length\n",
        "    Returns\n",
        "    -------\n",
        "    a list of frames\n",
        "    \"\"\"\n",
        "    n_frames = int(np.round((len(x) - overlap) / (win_len - overlap)))\n",
        "    x_frames = np.zeros((n_frames, win_len))\n",
        "    for i in range(n_frames):\n",
        "        start = i * (win_len - overlap)\n",
        "        end = start + win_len\n",
        "        x_frames[i] = x[start:end] if len(x[start:end]) == win_len else np.append(x[start:end], np.zeros(win_len % len(x[start:end])))\n",
        "    return x_frames\n",
        "\n",
        "def _deframing(x, x_vad, win_len, overlap):\n",
        "    \"\"\"\n",
        "    Combines overlapping frames\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : original input signal\n",
        "    x_vad : list of voice-activated frames\n",
        "    win_len : frame length\n",
        "    overlap : overlapping length\n",
        "    Returns\n",
        "    -------\n",
        "    a signal of length equal to the original input signal\n",
        "    \"\"\"\n",
        "    x_voice = np.zeros((len(x)))\n",
        "    for i in range(len(x_vad)):\n",
        "        start = i * (win_len - overlap)\n",
        "        end = i * (win_len - overlap) + win_len\n",
        "        x_voice[start:end] = x_vad[i]\n",
        "    for i in range(len(x_voice)):\n",
        "        if x_voice[i]:\n",
        "            x_voice[i] = x[i]\n",
        "    return np.trim_zeros(x_voice)\n",
        "\n",
        "def _calculate_nrg(x_frames):\n",
        "    \"\"\"\n",
        "    Calculates frame energy\n",
        "    Parameters\n",
        "    ----------\n",
        "    x_frames : list of frames\n",
        "    Returns\n",
        "    -------\n",
        "    a list of normalized energies of respective frames\n",
        "    \"\"\"\n",
        "    nrgs = np.diagonal(np.dot(x_frames, x_frames.T))\n",
        "    log_nrgs = np.log(nrgs)\n",
        "    norm_nrgs = (2 * (log_nrgs - min(log_nrgs)) / (max(log_nrgs) - min(log_nrgs))) - 1\n",
        "    return norm_nrgs\n",
        "\n",
        "def VAD(x, fs, db, nrg_th=0, context=5):\n",
        "    \"\"\"\n",
        "    Performs voice activity detection by comparing frame energy to a threshold\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : original input signal\n",
        "    fs : sampling rate\n",
        "    db : Database ID (RAVDESS:1 and SAVEE:2)\n",
        "    nrg_th: energy threshold\n",
        "    context: number of neighboring frames to be used in comparison\n",
        "    Returns\n",
        "    -------\n",
        "    a list of voice-activated frames; where 1 indicates the presence of voice and 0 indicates the absence of voice\n",
        "    \"\"\"\n",
        "    percent_th = 0.4 if db == 1 else 0.3\n",
        "    win_len = int(fs * 0.025)\n",
        "    overlap = int(win_len * 0.25)\n",
        "    x_frames = _framing(x, win_len, overlap)\n",
        "    n_frames = int(x_frames.shape[0])\n",
        "    x_nrgs = _calculate_nrg(x_frames)\n",
        "    x_vad = np.zeros((n_frames, 1))\n",
        "    for i in range(n_frames):\n",
        "        start = max(0, i - context)\n",
        "        end = min(n_frames, i + context)\n",
        "        num_nrgs_above_th = np.sum(x_nrgs[start:end, ...] > nrg_th)\n",
        "        total_num_nrgs = end - start + 1\n",
        "        x_vad[i] = (num_nrgs_above_th / total_num_nrgs) > percent_th\n",
        "    return _deframing(x, x_vad, win_len, overlap)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "fs = 44100  # Replace with your actual sampling rate\n",
        "db = 1  # Replace with your database ID\n",
        "nrg_th = 0.5  # Replace with your energy threshold\n",
        "context = 5  # Replace with your context size\n",
        "\n",
        "# Replace 'your_audio_signal' with your actual audio signal\n",
        "your_audio_signal = np.random.randn(10000)\n",
        "\n",
        "resulting_signal = VAD(your_audio_signal, fs, db, nrg_th, context)\n",
        "resulting_signal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK5wrBfNU763",
        "outputId": "d210b5aa-f184-4d1a-869f-713a1dbaf7f1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K45trBTY1Ss",
        "outputId": "b44fc052-8b91-4ad4-c045-7040e0bdb47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'surprised', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'sadness', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'happiness', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust']\n",
            "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "database_path = '/content/drive/MyDrive/Database/'\n",
        "\n",
        "audio_paths = glob('{}/**/*.wav'.format(database_path), recursive=True)\n",
        "audio_paths = [x.replace(os.sep, '/') for x in audio_paths]\n",
        "# Extracting subdirectory names as labels\n",
        "labels = [os.path.basename(os.path.dirname(x)) for x in audio_paths]\n",
        "\n",
        "# Optionally, you can encode the labels to numerical values\n",
        "label_encode = {label: i for i, label in enumerate(set(labels))}\n",
        "encoded_labels = [label_encode[label] for label in labels]\n",
        "\n",
        "# Print or use the labels and encoded_labels as needed\n",
        "print(labels)\n",
        "print(encoded_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "XXwAZUt0ZJOP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "sample_rate = 16000\n",
        "FRAME_LENGTH = int(0.025 * sample_rate)\n",
        "HOP_LENGTH = int(0.25 * FRAME_LENGTH)\n",
        "\n",
        "n_classes = len(set(labels))\n",
        "n_mfccs = 19\n",
        "n_audio = len(audio_paths)\n",
        "X = np.empty((n_audio, n_mfccs + 4), dtype=np.float32)\n",
        "Y = np.empty((n_audio, n_classes), dtype=np.uint8)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)\n",
        "Y = to_categorical(y_encoded, num_classes=n_classes)\n",
        "\n",
        "for i, (path, label) in enumerate(zip(audio_paths, y_encoded)):\n",
        "    audio, _ = librosa.load(path, sr=sample_rate, res_type='fft', offset=0.5)\n",
        "    waveform = VAD(audio, sample_rate, int(db))\n",
        "    waveform_pad = np.zeros((int(sample_rate * 5,)))\n",
        "    waveform_pad[:len(waveform)] = waveform\n",
        "    mfccs = np.mean(librosa.feature.mfcc(y=waveform_pad, sr=sample_rate, n_mfcc=n_mfccs, n_fft=1024, win_length=FRAME_LENGTH, hop_length=HOP_LENGTH, window='hamming', n_mels=128, fmax=sample_rate/2).T, axis=0)\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=waveform_pad, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "    rmse = np.mean(librosa.feature.rms(y=waveform_pad, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "    pitch, magnitude = librosa.piptrack(y=waveform, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming')\n",
        "    pitch = np.mean(pitch[np.where(magnitude > 0)])\n",
        "    centroid = np.mean(librosa.feature.spectral_centroid(y=waveform, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming'))\n",
        "    X[i, ...] = np.append(mfccs, (rmse, centroid, zcr, pitch))\n",
        "    Y[i, ...] = to_categorical(label, num_classes=n_classes)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X = X[..., np.newaxis]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "iXhPmJwJZpoV"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.05, random_state=42)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.025, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UPRNp0mZ1Ye",
        "outputId": "ad4ba198-d8bc-49f6-810e-26f03eb7a585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_36 (Conv1D)          (None, 23, 32)            320       \n",
            "                                                                 \n",
            " batch_normalization_36 (Ba  (None, 23, 32)            128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_36 (Activation)  (None, 23, 32)            0         \n",
            "                                                                 \n",
            " max_pooling1d_36 (MaxPooli  (None, 11, 32)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 11, 32)            0         \n",
            "                                                                 \n",
            " conv1d_37 (Conv1D)          (None, 11, 64)            14400     \n",
            "                                                                 \n",
            " batch_normalization_37 (Ba  (None, 11, 64)            256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_37 (Activation)  (None, 11, 64)            0         \n",
            "                                                                 \n",
            " max_pooling1d_37 (MaxPooli  (None, 5, 64)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 5, 64)             0         \n",
            "                                                                 \n",
            " conv1d_38 (Conv1D)          (None, 5, 128)            41088     \n",
            "                                                                 \n",
            " batch_normalization_38 (Ba  (None, 5, 128)            512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_38 (Activation)  (None, 5, 128)            0         \n",
            "                                                                 \n",
            " max_pooling1d_38 (MaxPooli  (None, 2, 128)            0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 2, 128)            0         \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 2, 32)             20608     \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 8)                 520       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 77832 (304.03 KB)\n",
            "Trainable params: 77384 (302.28 KB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same', input_shape=(n_mfccs + 4, 1)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('elu'),\n",
        "        tf.keras.layers.MaxPool1D(pool_size=2, strides=2),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv1D(filters=64, kernel_size=7, strides=1, padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('elu'),\n",
        "        tf.keras.layers.MaxPool1D(pool_size=2, strides=2),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('elu'),\n",
        "        tf.keras.layers.MaxPool1D(pool_size=2, strides=2),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        tf.keras.layers.Dense(units=n_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "YQO7nCJube7U"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.01, decay=1e-3, momentum=0.8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoEMITmabyrq",
        "outputId": "2f19baad-0838-46cf-ec5b-80b4172ad6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1100\n",
            "22/22 [==============================] - 6s 78ms/step - loss: 2.0832 - categorical_accuracy: 0.1331 - val_loss: 2.0509 - val_categorical_accuracy: 0.2329\n",
            "Epoch 2/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 2.0085 - categorical_accuracy: 0.2256 - val_loss: 2.0179 - val_categorical_accuracy: 0.3562\n",
            "Epoch 3/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 1.9677 - categorical_accuracy: 0.2286 - val_loss: 1.9853 - val_categorical_accuracy: 0.3151\n",
            "Epoch 4/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.9362 - categorical_accuracy: 0.2678 - val_loss: 1.9444 - val_categorical_accuracy: 0.3699\n",
            "Epoch 5/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.8927 - categorical_accuracy: 0.3047 - val_loss: 1.9114 - val_categorical_accuracy: 0.3836\n",
            "Epoch 6/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.8648 - categorical_accuracy: 0.3151 - val_loss: 1.8725 - val_categorical_accuracy: 0.4110\n",
            "Epoch 7/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.8304 - categorical_accuracy: 0.3328 - val_loss: 1.8335 - val_categorical_accuracy: 0.3973\n",
            "Epoch 8/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.8060 - categorical_accuracy: 0.3491 - val_loss: 1.7939 - val_categorical_accuracy: 0.3836\n",
            "Epoch 9/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.7804 - categorical_accuracy: 0.3521 - val_loss: 1.7498 - val_categorical_accuracy: 0.4110\n",
            "Epoch 10/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.7335 - categorical_accuracy: 0.3676 - val_loss: 1.7042 - val_categorical_accuracy: 0.4110\n",
            "Epoch 11/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.7283 - categorical_accuracy: 0.3772 - val_loss: 1.6648 - val_categorical_accuracy: 0.3973\n",
            "Epoch 12/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.6810 - categorical_accuracy: 0.3957 - val_loss: 1.6107 - val_categorical_accuracy: 0.4384\n",
            "Epoch 13/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.6682 - categorical_accuracy: 0.3994 - val_loss: 1.5639 - val_categorical_accuracy: 0.4658\n",
            "Epoch 14/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.6489 - categorical_accuracy: 0.3987 - val_loss: 1.5307 - val_categorical_accuracy: 0.4795\n",
            "Epoch 15/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.6156 - categorical_accuracy: 0.4253 - val_loss: 1.4842 - val_categorical_accuracy: 0.5068\n",
            "Epoch 16/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.5878 - categorical_accuracy: 0.4305 - val_loss: 1.4786 - val_categorical_accuracy: 0.4110\n",
            "Epoch 17/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.5683 - categorical_accuracy: 0.4371 - val_loss: 1.4474 - val_categorical_accuracy: 0.4932\n",
            "Epoch 18/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.5640 - categorical_accuracy: 0.4357 - val_loss: 1.4085 - val_categorical_accuracy: 0.4795\n",
            "Epoch 19/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.5388 - categorical_accuracy: 0.4334 - val_loss: 1.3896 - val_categorical_accuracy: 0.4795\n",
            "Epoch 20/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.5289 - categorical_accuracy: 0.4438 - val_loss: 1.3533 - val_categorical_accuracy: 0.4658\n",
            "Epoch 21/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.5043 - categorical_accuracy: 0.4541 - val_loss: 1.3492 - val_categorical_accuracy: 0.5205\n",
            "Epoch 22/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.4901 - categorical_accuracy: 0.4512 - val_loss: 1.3387 - val_categorical_accuracy: 0.5342\n",
            "Epoch 23/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.4758 - categorical_accuracy: 0.4541 - val_loss: 1.3062 - val_categorical_accuracy: 0.5479\n",
            "Epoch 24/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.4593 - categorical_accuracy: 0.4734 - val_loss: 1.2790 - val_categorical_accuracy: 0.5616\n",
            "Epoch 25/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.4811 - categorical_accuracy: 0.4675 - val_loss: 1.2815 - val_categorical_accuracy: 0.5342\n",
            "Epoch 26/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.4365 - categorical_accuracy: 0.4726 - val_loss: 1.2790 - val_categorical_accuracy: 0.5890\n",
            "Epoch 27/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.4027 - categorical_accuracy: 0.4911 - val_loss: 1.2581 - val_categorical_accuracy: 0.5616\n",
            "Epoch 28/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.4153 - categorical_accuracy: 0.4689 - val_loss: 1.2702 - val_categorical_accuracy: 0.5616\n",
            "Epoch 29/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.3987 - categorical_accuracy: 0.4911 - val_loss: 1.2712 - val_categorical_accuracy: 0.5753\n",
            "Epoch 30/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.3825 - categorical_accuracy: 0.4911 - val_loss: 1.2203 - val_categorical_accuracy: 0.5753\n",
            "Epoch 31/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 1.3749 - categorical_accuracy: 0.5000 - val_loss: 1.2472 - val_categorical_accuracy: 0.5616\n",
            "Epoch 32/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.3602 - categorical_accuracy: 0.4926 - val_loss: 1.2521 - val_categorical_accuracy: 0.5890\n",
            "Epoch 33/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.3224 - categorical_accuracy: 0.5266 - val_loss: 1.2041 - val_categorical_accuracy: 0.5616\n",
            "Epoch 34/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.3707 - categorical_accuracy: 0.5000 - val_loss: 1.2129 - val_categorical_accuracy: 0.5753\n",
            "Epoch 35/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.3374 - categorical_accuracy: 0.5163 - val_loss: 1.2263 - val_categorical_accuracy: 0.5753\n",
            "Epoch 36/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.3289 - categorical_accuracy: 0.5059 - val_loss: 1.2163 - val_categorical_accuracy: 0.6164\n",
            "Epoch 37/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 1.3146 - categorical_accuracy: 0.5141 - val_loss: 1.1974 - val_categorical_accuracy: 0.5616\n",
            "Epoch 38/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 1.3273 - categorical_accuracy: 0.5126 - val_loss: 1.2060 - val_categorical_accuracy: 0.5616\n",
            "Epoch 39/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.2880 - categorical_accuracy: 0.5422 - val_loss: 1.2309 - val_categorical_accuracy: 0.5616\n",
            "Epoch 40/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.3153 - categorical_accuracy: 0.5163 - val_loss: 1.1959 - val_categorical_accuracy: 0.5616\n",
            "Epoch 41/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.2901 - categorical_accuracy: 0.5251 - val_loss: 1.1638 - val_categorical_accuracy: 0.5890\n",
            "Epoch 42/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.2715 - categorical_accuracy: 0.5466 - val_loss: 1.1747 - val_categorical_accuracy: 0.6027\n",
            "Epoch 43/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.2721 - categorical_accuracy: 0.5251 - val_loss: 1.1888 - val_categorical_accuracy: 0.5890\n",
            "Epoch 44/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.2607 - categorical_accuracy: 0.5429 - val_loss: 1.2087 - val_categorical_accuracy: 0.5616\n",
            "Epoch 45/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.2271 - categorical_accuracy: 0.5355 - val_loss: 1.1734 - val_categorical_accuracy: 0.5890\n",
            "Epoch 46/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.2024 - categorical_accuracy: 0.5407 - val_loss: 1.1556 - val_categorical_accuracy: 0.6027\n",
            "Epoch 47/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.2129 - categorical_accuracy: 0.5577 - val_loss: 1.1488 - val_categorical_accuracy: 0.6027\n",
            "Epoch 48/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.2389 - categorical_accuracy: 0.5296 - val_loss: 1.1558 - val_categorical_accuracy: 0.5753\n",
            "Epoch 49/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.2166 - categorical_accuracy: 0.5533 - val_loss: 1.1340 - val_categorical_accuracy: 0.6164\n",
            "Epoch 50/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.1911 - categorical_accuracy: 0.5592 - val_loss: 1.1411 - val_categorical_accuracy: 0.5753\n",
            "Epoch 51/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.2128 - categorical_accuracy: 0.5451 - val_loss: 1.1658 - val_categorical_accuracy: 0.6027\n",
            "Epoch 52/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1882 - categorical_accuracy: 0.5629 - val_loss: 1.1630 - val_categorical_accuracy: 0.5890\n",
            "Epoch 53/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1903 - categorical_accuracy: 0.5540 - val_loss: 1.1065 - val_categorical_accuracy: 0.6438\n",
            "Epoch 54/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.1706 - categorical_accuracy: 0.5695 - val_loss: 1.1509 - val_categorical_accuracy: 0.6027\n",
            "Epoch 55/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1786 - categorical_accuracy: 0.5444 - val_loss: 1.1307 - val_categorical_accuracy: 0.6438\n",
            "Epoch 56/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.1836 - categorical_accuracy: 0.5540 - val_loss: 1.1229 - val_categorical_accuracy: 0.6164\n",
            "Epoch 57/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.1938 - categorical_accuracy: 0.5599 - val_loss: 1.1281 - val_categorical_accuracy: 0.6164\n",
            "Epoch 58/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.1816 - categorical_accuracy: 0.5791 - val_loss: 1.1131 - val_categorical_accuracy: 0.6575\n",
            "Epoch 59/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1430 - categorical_accuracy: 0.5925 - val_loss: 1.1285 - val_categorical_accuracy: 0.6301\n",
            "Epoch 60/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1590 - categorical_accuracy: 0.5666 - val_loss: 1.1321 - val_categorical_accuracy: 0.6027\n",
            "Epoch 61/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1574 - categorical_accuracy: 0.5754 - val_loss: 1.1012 - val_categorical_accuracy: 0.5890\n",
            "Epoch 62/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1159 - categorical_accuracy: 0.5777 - val_loss: 1.0951 - val_categorical_accuracy: 0.6301\n",
            "Epoch 63/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0979 - categorical_accuracy: 0.5984 - val_loss: 1.1027 - val_categorical_accuracy: 0.6438\n",
            "Epoch 64/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.1431 - categorical_accuracy: 0.5747 - val_loss: 1.1219 - val_categorical_accuracy: 0.6301\n",
            "Epoch 65/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0956 - categorical_accuracy: 0.6050 - val_loss: 1.1158 - val_categorical_accuracy: 0.6301\n",
            "Epoch 66/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.1369 - categorical_accuracy: 0.5754 - val_loss: 1.0959 - val_categorical_accuracy: 0.6438\n",
            "Epoch 67/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.1278 - categorical_accuracy: 0.5799 - val_loss: 1.0736 - val_categorical_accuracy: 0.6712\n",
            "Epoch 68/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.0752 - categorical_accuracy: 0.6013 - val_loss: 1.0807 - val_categorical_accuracy: 0.6164\n",
            "Epoch 69/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.1010 - categorical_accuracy: 0.5917 - val_loss: 1.0962 - val_categorical_accuracy: 0.5890\n",
            "Epoch 70/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.0775 - categorical_accuracy: 0.6183 - val_loss: 1.0787 - val_categorical_accuracy: 0.6301\n",
            "Epoch 71/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.0964 - categorical_accuracy: 0.5932 - val_loss: 1.0783 - val_categorical_accuracy: 0.6301\n",
            "Epoch 72/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.0724 - categorical_accuracy: 0.6065 - val_loss: 1.0638 - val_categorical_accuracy: 0.6301\n",
            "Epoch 73/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 1.1019 - categorical_accuracy: 0.6006 - val_loss: 1.0746 - val_categorical_accuracy: 0.5753\n",
            "Epoch 74/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 1.0721 - categorical_accuracy: 0.6102 - val_loss: 1.0686 - val_categorical_accuracy: 0.6301\n",
            "Epoch 75/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 1.0756 - categorical_accuracy: 0.6161 - val_loss: 1.0896 - val_categorical_accuracy: 0.5890\n",
            "Epoch 76/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 1.0433 - categorical_accuracy: 0.6139 - val_loss: 1.1010 - val_categorical_accuracy: 0.6164\n",
            "Epoch 77/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0725 - categorical_accuracy: 0.5976 - val_loss: 1.0448 - val_categorical_accuracy: 0.6301\n",
            "Epoch 78/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0678 - categorical_accuracy: 0.6102 - val_loss: 1.0496 - val_categorical_accuracy: 0.6301\n",
            "Epoch 79/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0040 - categorical_accuracy: 0.6413 - val_loss: 1.0476 - val_categorical_accuracy: 0.6301\n",
            "Epoch 80/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0478 - categorical_accuracy: 0.6139 - val_loss: 1.0554 - val_categorical_accuracy: 0.6301\n",
            "Epoch 81/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0321 - categorical_accuracy: 0.6317 - val_loss: 1.0611 - val_categorical_accuracy: 0.6438\n",
            "Epoch 82/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0386 - categorical_accuracy: 0.6109 - val_loss: 1.0415 - val_categorical_accuracy: 0.6575\n",
            "Epoch 83/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0493 - categorical_accuracy: 0.6183 - val_loss: 1.0457 - val_categorical_accuracy: 0.6438\n",
            "Epoch 84/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0393 - categorical_accuracy: 0.6198 - val_loss: 1.0659 - val_categorical_accuracy: 0.6575\n",
            "Epoch 85/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0186 - categorical_accuracy: 0.6176 - val_loss: 1.0310 - val_categorical_accuracy: 0.6438\n",
            "Epoch 86/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0286 - categorical_accuracy: 0.6169 - val_loss: 1.0323 - val_categorical_accuracy: 0.6438\n",
            "Epoch 87/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0166 - categorical_accuracy: 0.6302 - val_loss: 1.0260 - val_categorical_accuracy: 0.6301\n",
            "Epoch 88/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0123 - categorical_accuracy: 0.6361 - val_loss: 1.0258 - val_categorical_accuracy: 0.6849\n",
            "Epoch 89/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9990 - categorical_accuracy: 0.6324 - val_loss: 1.0065 - val_categorical_accuracy: 0.6712\n",
            "Epoch 90/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9723 - categorical_accuracy: 0.6450 - val_loss: 1.0220 - val_categorical_accuracy: 0.6712\n",
            "Epoch 91/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9921 - categorical_accuracy: 0.6361 - val_loss: 1.0271 - val_categorical_accuracy: 0.6438\n",
            "Epoch 92/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9761 - categorical_accuracy: 0.6472 - val_loss: 1.0273 - val_categorical_accuracy: 0.6575\n",
            "Epoch 93/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9949 - categorical_accuracy: 0.6361 - val_loss: 1.0492 - val_categorical_accuracy: 0.6575\n",
            "Epoch 94/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0019 - categorical_accuracy: 0.6413 - val_loss: 1.0295 - val_categorical_accuracy: 0.6712\n",
            "Epoch 95/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9648 - categorical_accuracy: 0.6405 - val_loss: 1.0241 - val_categorical_accuracy: 0.6575\n",
            "Epoch 96/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0071 - categorical_accuracy: 0.6287 - val_loss: 0.9919 - val_categorical_accuracy: 0.6438\n",
            "Epoch 97/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9419 - categorical_accuracy: 0.6546 - val_loss: 0.9948 - val_categorical_accuracy: 0.6575\n",
            "Epoch 98/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9979 - categorical_accuracy: 0.6413 - val_loss: 1.0142 - val_categorical_accuracy: 0.6575\n",
            "Epoch 99/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9626 - categorical_accuracy: 0.6561 - val_loss: 0.9973 - val_categorical_accuracy: 0.6575\n",
            "Epoch 100/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9487 - categorical_accuracy: 0.6701 - val_loss: 0.9953 - val_categorical_accuracy: 0.6575\n",
            "Epoch 101/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9610 - categorical_accuracy: 0.6561 - val_loss: 0.9972 - val_categorical_accuracy: 0.6849\n",
            "Epoch 102/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.9581 - categorical_accuracy: 0.6590 - val_loss: 0.9856 - val_categorical_accuracy: 0.6575\n",
            "Epoch 103/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9625 - categorical_accuracy: 0.6575 - val_loss: 0.9927 - val_categorical_accuracy: 0.6986\n",
            "Epoch 104/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.9414 - categorical_accuracy: 0.6531 - val_loss: 0.9905 - val_categorical_accuracy: 0.6849\n",
            "Epoch 105/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.9557 - categorical_accuracy: 0.6479 - val_loss: 0.9871 - val_categorical_accuracy: 0.6575\n",
            "Epoch 106/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.9258 - categorical_accuracy: 0.6797 - val_loss: 0.9940 - val_categorical_accuracy: 0.6575\n",
            "Epoch 107/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.9544 - categorical_accuracy: 0.6605 - val_loss: 0.9918 - val_categorical_accuracy: 0.6575\n",
            "Epoch 108/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9258 - categorical_accuracy: 0.6738 - val_loss: 0.9970 - val_categorical_accuracy: 0.6712\n",
            "Epoch 109/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9643 - categorical_accuracy: 0.6324 - val_loss: 0.9805 - val_categorical_accuracy: 0.6575\n",
            "Epoch 110/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9079 - categorical_accuracy: 0.6723 - val_loss: 1.0051 - val_categorical_accuracy: 0.6575\n",
            "Epoch 111/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9357 - categorical_accuracy: 0.6620 - val_loss: 0.9740 - val_categorical_accuracy: 0.6575\n",
            "Epoch 112/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9258 - categorical_accuracy: 0.6509 - val_loss: 0.9751 - val_categorical_accuracy: 0.6712\n",
            "Epoch 113/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.9229 - categorical_accuracy: 0.6575 - val_loss: 0.9821 - val_categorical_accuracy: 0.6575\n",
            "Epoch 114/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9265 - categorical_accuracy: 0.6649 - val_loss: 0.9708 - val_categorical_accuracy: 0.6712\n",
            "Epoch 115/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9038 - categorical_accuracy: 0.6753 - val_loss: 0.9875 - val_categorical_accuracy: 0.6575\n",
            "Epoch 116/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9110 - categorical_accuracy: 0.6701 - val_loss: 0.9622 - val_categorical_accuracy: 0.7123\n",
            "Epoch 117/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8844 - categorical_accuracy: 0.6812 - val_loss: 0.9552 - val_categorical_accuracy: 0.6712\n",
            "Epoch 118/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9013 - categorical_accuracy: 0.6649 - val_loss: 0.9618 - val_categorical_accuracy: 0.6849\n",
            "Epoch 119/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9125 - categorical_accuracy: 0.6635 - val_loss: 0.9592 - val_categorical_accuracy: 0.7123\n",
            "Epoch 120/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8884 - categorical_accuracy: 0.6871 - val_loss: 0.9782 - val_categorical_accuracy: 0.6712\n",
            "Epoch 121/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8817 - categorical_accuracy: 0.6709 - val_loss: 0.9703 - val_categorical_accuracy: 0.6575\n",
            "Epoch 122/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8823 - categorical_accuracy: 0.6790 - val_loss: 0.9598 - val_categorical_accuracy: 0.6712\n",
            "Epoch 123/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8862 - categorical_accuracy: 0.6760 - val_loss: 0.9474 - val_categorical_accuracy: 0.6849\n",
            "Epoch 124/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8782 - categorical_accuracy: 0.6916 - val_loss: 0.9856 - val_categorical_accuracy: 0.6575\n",
            "Epoch 125/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9004 - categorical_accuracy: 0.6716 - val_loss: 0.9699 - val_categorical_accuracy: 0.6712\n",
            "Epoch 126/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8684 - categorical_accuracy: 0.6871 - val_loss: 0.9708 - val_categorical_accuracy: 0.6849\n",
            "Epoch 127/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8706 - categorical_accuracy: 0.6871 - val_loss: 0.9789 - val_categorical_accuracy: 0.6575\n",
            "Epoch 128/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8848 - categorical_accuracy: 0.6842 - val_loss: 0.9570 - val_categorical_accuracy: 0.6438\n",
            "Epoch 129/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8614 - categorical_accuracy: 0.6908 - val_loss: 0.9444 - val_categorical_accuracy: 0.6712\n",
            "Epoch 130/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8626 - categorical_accuracy: 0.6908 - val_loss: 0.9508 - val_categorical_accuracy: 0.7123\n",
            "Epoch 131/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8571 - categorical_accuracy: 0.6849 - val_loss: 0.9516 - val_categorical_accuracy: 0.6849\n",
            "Epoch 132/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8754 - categorical_accuracy: 0.6746 - val_loss: 0.9352 - val_categorical_accuracy: 0.7123\n",
            "Epoch 133/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8255 - categorical_accuracy: 0.7108 - val_loss: 0.9393 - val_categorical_accuracy: 0.6986\n",
            "Epoch 134/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.8648 - categorical_accuracy: 0.6879 - val_loss: 0.9502 - val_categorical_accuracy: 0.6712\n",
            "Epoch 135/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.8663 - categorical_accuracy: 0.7064 - val_loss: 0.9675 - val_categorical_accuracy: 0.6575\n",
            "Epoch 136/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8203 - categorical_accuracy: 0.7093 - val_loss: 0.9527 - val_categorical_accuracy: 0.6712\n",
            "Epoch 137/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8398 - categorical_accuracy: 0.7078 - val_loss: 0.9466 - val_categorical_accuracy: 0.6986\n",
            "Epoch 138/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.8463 - categorical_accuracy: 0.6982 - val_loss: 0.9442 - val_categorical_accuracy: 0.7123\n",
            "Epoch 139/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8258 - categorical_accuracy: 0.7041 - val_loss: 0.9511 - val_categorical_accuracy: 0.7123\n",
            "Epoch 140/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8509 - categorical_accuracy: 0.6886 - val_loss: 0.9521 - val_categorical_accuracy: 0.7123\n",
            "Epoch 141/1100\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.8392 - categorical_accuracy: 0.6997 - val_loss: 0.9468 - val_categorical_accuracy: 0.7397\n",
            "Epoch 142/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.8292 - categorical_accuracy: 0.7197 - val_loss: 0.9469 - val_categorical_accuracy: 0.7123\n",
            "Epoch 143/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8595 - categorical_accuracy: 0.6893 - val_loss: 0.9385 - val_categorical_accuracy: 0.7123\n",
            "Epoch 144/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.8380 - categorical_accuracy: 0.6901 - val_loss: 0.9468 - val_categorical_accuracy: 0.6438\n",
            "Epoch 145/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.8286 - categorical_accuracy: 0.6923 - val_loss: 0.9644 - val_categorical_accuracy: 0.6575\n",
            "Epoch 146/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.7977 - categorical_accuracy: 0.7212 - val_loss: 0.9408 - val_categorical_accuracy: 0.6849\n",
            "Epoch 147/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7951 - categorical_accuracy: 0.7182 - val_loss: 0.9242 - val_categorical_accuracy: 0.6986\n",
            "Epoch 148/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8321 - categorical_accuracy: 0.6975 - val_loss: 0.9056 - val_categorical_accuracy: 0.7123\n",
            "Epoch 149/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7940 - categorical_accuracy: 0.7182 - val_loss: 0.9264 - val_categorical_accuracy: 0.6712\n",
            "Epoch 150/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8118 - categorical_accuracy: 0.7034 - val_loss: 0.9054 - val_categorical_accuracy: 0.6986\n",
            "Epoch 151/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8517 - categorical_accuracy: 0.6857 - val_loss: 0.9228 - val_categorical_accuracy: 0.6849\n",
            "Epoch 152/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8053 - categorical_accuracy: 0.7160 - val_loss: 0.9278 - val_categorical_accuracy: 0.6849\n",
            "Epoch 153/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7995 - categorical_accuracy: 0.7078 - val_loss: 0.9208 - val_categorical_accuracy: 0.6849\n",
            "Epoch 154/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.7933 - categorical_accuracy: 0.7167 - val_loss: 0.9042 - val_categorical_accuracy: 0.7123\n",
            "Epoch 155/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8054 - categorical_accuracy: 0.7056 - val_loss: 0.9043 - val_categorical_accuracy: 0.6986\n",
            "Epoch 156/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7958 - categorical_accuracy: 0.7234 - val_loss: 0.9012 - val_categorical_accuracy: 0.7123\n",
            "Epoch 157/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7952 - categorical_accuracy: 0.7249 - val_loss: 0.9113 - val_categorical_accuracy: 0.6849\n",
            "Epoch 158/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7698 - categorical_accuracy: 0.7345 - val_loss: 0.8961 - val_categorical_accuracy: 0.6849\n",
            "Epoch 159/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7991 - categorical_accuracy: 0.7049 - val_loss: 0.9174 - val_categorical_accuracy: 0.6986\n",
            "Epoch 160/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8003 - categorical_accuracy: 0.7034 - val_loss: 0.9029 - val_categorical_accuracy: 0.7397\n",
            "Epoch 161/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8102 - categorical_accuracy: 0.7078 - val_loss: 0.9088 - val_categorical_accuracy: 0.7671\n",
            "Epoch 162/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7509 - categorical_accuracy: 0.7345 - val_loss: 0.9080 - val_categorical_accuracy: 0.7260\n",
            "Epoch 163/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8218 - categorical_accuracy: 0.6908 - val_loss: 0.8904 - val_categorical_accuracy: 0.7534\n",
            "Epoch 164/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7998 - categorical_accuracy: 0.7115 - val_loss: 0.9100 - val_categorical_accuracy: 0.7260\n",
            "Epoch 165/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8000 - categorical_accuracy: 0.7108 - val_loss: 0.9031 - val_categorical_accuracy: 0.7123\n",
            "Epoch 166/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7979 - categorical_accuracy: 0.7034 - val_loss: 0.8810 - val_categorical_accuracy: 0.6712\n",
            "Epoch 167/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7381 - categorical_accuracy: 0.7441 - val_loss: 0.8720 - val_categorical_accuracy: 0.6986\n",
            "Epoch 168/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7727 - categorical_accuracy: 0.7271 - val_loss: 0.8651 - val_categorical_accuracy: 0.7123\n",
            "Epoch 169/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7491 - categorical_accuracy: 0.7256 - val_loss: 0.8586 - val_categorical_accuracy: 0.7123\n",
            "Epoch 170/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7539 - categorical_accuracy: 0.7345 - val_loss: 0.8850 - val_categorical_accuracy: 0.6986\n",
            "Epoch 171/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7788 - categorical_accuracy: 0.7093 - val_loss: 0.8796 - val_categorical_accuracy: 0.7123\n",
            "Epoch 172/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7302 - categorical_accuracy: 0.7382 - val_loss: 0.8616 - val_categorical_accuracy: 0.6986\n",
            "Epoch 173/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7520 - categorical_accuracy: 0.7234 - val_loss: 0.8712 - val_categorical_accuracy: 0.7260\n",
            "Epoch 174/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7460 - categorical_accuracy: 0.7389 - val_loss: 0.8634 - val_categorical_accuracy: 0.7260\n",
            "Epoch 175/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7852 - categorical_accuracy: 0.7249 - val_loss: 0.8532 - val_categorical_accuracy: 0.7123\n",
            "Epoch 176/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7797 - categorical_accuracy: 0.7071 - val_loss: 0.8421 - val_categorical_accuracy: 0.7397\n",
            "Epoch 177/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.7529 - categorical_accuracy: 0.7352 - val_loss: 0.8415 - val_categorical_accuracy: 0.7397\n",
            "Epoch 178/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7592 - categorical_accuracy: 0.7234 - val_loss: 0.8436 - val_categorical_accuracy: 0.7260\n",
            "Epoch 179/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7643 - categorical_accuracy: 0.7249 - val_loss: 0.8591 - val_categorical_accuracy: 0.7260\n",
            "Epoch 180/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.7402 - categorical_accuracy: 0.7433 - val_loss: 0.8495 - val_categorical_accuracy: 0.7397\n",
            "Epoch 181/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7307 - categorical_accuracy: 0.7389 - val_loss: 0.8523 - val_categorical_accuracy: 0.7397\n",
            "Epoch 182/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.7525 - categorical_accuracy: 0.7241 - val_loss: 0.8499 - val_categorical_accuracy: 0.7397\n",
            "Epoch 183/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.7251 - categorical_accuracy: 0.7293 - val_loss: 0.8467 - val_categorical_accuracy: 0.7397\n",
            "Epoch 184/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.7271 - categorical_accuracy: 0.7507 - val_loss: 0.8484 - val_categorical_accuracy: 0.7260\n",
            "Epoch 185/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7496 - categorical_accuracy: 0.7337 - val_loss: 0.8407 - val_categorical_accuracy: 0.7260\n",
            "Epoch 186/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.7181 - categorical_accuracy: 0.7537 - val_loss: 0.8359 - val_categorical_accuracy: 0.7260\n",
            "Epoch 187/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.7501 - categorical_accuracy: 0.7226 - val_loss: 0.8308 - val_categorical_accuracy: 0.7397\n",
            "Epoch 188/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7565 - categorical_accuracy: 0.7256 - val_loss: 0.8237 - val_categorical_accuracy: 0.7534\n",
            "Epoch 189/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.7407 - categorical_accuracy: 0.7278 - val_loss: 0.8409 - val_categorical_accuracy: 0.7123\n",
            "Epoch 190/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7506 - categorical_accuracy: 0.7241 - val_loss: 0.8325 - val_categorical_accuracy: 0.7397\n",
            "Epoch 191/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7150 - categorical_accuracy: 0.7470 - val_loss: 0.8281 - val_categorical_accuracy: 0.7397\n",
            "Epoch 192/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7439 - categorical_accuracy: 0.7308 - val_loss: 0.8250 - val_categorical_accuracy: 0.7397\n",
            "Epoch 193/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7482 - categorical_accuracy: 0.7300 - val_loss: 0.8129 - val_categorical_accuracy: 0.7534\n",
            "Epoch 194/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6947 - categorical_accuracy: 0.7493 - val_loss: 0.8137 - val_categorical_accuracy: 0.7260\n",
            "Epoch 195/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7313 - categorical_accuracy: 0.7389 - val_loss: 0.8323 - val_categorical_accuracy: 0.7534\n",
            "Epoch 196/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7258 - categorical_accuracy: 0.7433 - val_loss: 0.8295 - val_categorical_accuracy: 0.7671\n",
            "Epoch 197/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7012 - categorical_accuracy: 0.7589 - val_loss: 0.8302 - val_categorical_accuracy: 0.7397\n",
            "Epoch 198/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7201 - categorical_accuracy: 0.7470 - val_loss: 0.8237 - val_categorical_accuracy: 0.7260\n",
            "Epoch 199/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7119 - categorical_accuracy: 0.7611 - val_loss: 0.8006 - val_categorical_accuracy: 0.7671\n",
            "Epoch 200/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6954 - categorical_accuracy: 0.7507 - val_loss: 0.8239 - val_categorical_accuracy: 0.7397\n",
            "Epoch 201/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7035 - categorical_accuracy: 0.7611 - val_loss: 0.8155 - val_categorical_accuracy: 0.7397\n",
            "Epoch 202/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7091 - categorical_accuracy: 0.7411 - val_loss: 0.7995 - val_categorical_accuracy: 0.7534\n",
            "Epoch 203/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6739 - categorical_accuracy: 0.7729 - val_loss: 0.8038 - val_categorical_accuracy: 0.7534\n",
            "Epoch 204/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7105 - categorical_accuracy: 0.7396 - val_loss: 0.7926 - val_categorical_accuracy: 0.7671\n",
            "Epoch 205/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7125 - categorical_accuracy: 0.7426 - val_loss: 0.7932 - val_categorical_accuracy: 0.7534\n",
            "Epoch 206/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6994 - categorical_accuracy: 0.7470 - val_loss: 0.7979 - val_categorical_accuracy: 0.7534\n",
            "Epoch 207/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6983 - categorical_accuracy: 0.7604 - val_loss: 0.8008 - val_categorical_accuracy: 0.7534\n",
            "Epoch 208/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.6729 - categorical_accuracy: 0.7744 - val_loss: 0.8058 - val_categorical_accuracy: 0.7260\n",
            "Epoch 209/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7019 - categorical_accuracy: 0.7470 - val_loss: 0.8055 - val_categorical_accuracy: 0.7534\n",
            "Epoch 210/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6986 - categorical_accuracy: 0.7456 - val_loss: 0.7991 - val_categorical_accuracy: 0.7534\n",
            "Epoch 211/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7256 - categorical_accuracy: 0.7404 - val_loss: 0.7944 - val_categorical_accuracy: 0.7945\n",
            "Epoch 212/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6897 - categorical_accuracy: 0.7544 - val_loss: 0.7885 - val_categorical_accuracy: 0.7534\n",
            "Epoch 213/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6750 - categorical_accuracy: 0.7500 - val_loss: 0.7979 - val_categorical_accuracy: 0.7671\n",
            "Epoch 214/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6889 - categorical_accuracy: 0.7589 - val_loss: 0.7918 - val_categorical_accuracy: 0.7534\n",
            "Epoch 215/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.6933 - categorical_accuracy: 0.7581 - val_loss: 0.7831 - val_categorical_accuracy: 0.7808\n",
            "Epoch 216/1100\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.6980 - categorical_accuracy: 0.7396 - val_loss: 0.7850 - val_categorical_accuracy: 0.7671\n",
            "Epoch 217/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7004 - categorical_accuracy: 0.7589 - val_loss: 0.7862 - val_categorical_accuracy: 0.7808\n",
            "Epoch 218/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.6701 - categorical_accuracy: 0.7633 - val_loss: 0.7983 - val_categorical_accuracy: 0.7671\n",
            "Epoch 219/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.6766 - categorical_accuracy: 0.7626 - val_loss: 0.7901 - val_categorical_accuracy: 0.7945\n",
            "Epoch 220/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.6771 - categorical_accuracy: 0.7589 - val_loss: 0.7920 - val_categorical_accuracy: 0.7671\n",
            "Epoch 221/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.6971 - categorical_accuracy: 0.7581 - val_loss: 0.7763 - val_categorical_accuracy: 0.8082\n",
            "Epoch 222/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6886 - categorical_accuracy: 0.7478 - val_loss: 0.7755 - val_categorical_accuracy: 0.7671\n",
            "Epoch 223/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.6646 - categorical_accuracy: 0.7604 - val_loss: 0.7709 - val_categorical_accuracy: 0.8082\n",
            "Epoch 224/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6720 - categorical_accuracy: 0.7596 - val_loss: 0.7777 - val_categorical_accuracy: 0.7671\n",
            "Epoch 225/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.6714 - categorical_accuracy: 0.7641 - val_loss: 0.7722 - val_categorical_accuracy: 0.7397\n",
            "Epoch 226/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6780 - categorical_accuracy: 0.7618 - val_loss: 0.7717 - val_categorical_accuracy: 0.7397\n",
            "Epoch 227/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6863 - categorical_accuracy: 0.7559 - val_loss: 0.7730 - val_categorical_accuracy: 0.7397\n",
            "Epoch 228/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6792 - categorical_accuracy: 0.7641 - val_loss: 0.7710 - val_categorical_accuracy: 0.7671\n",
            "Epoch 229/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6757 - categorical_accuracy: 0.7567 - val_loss: 0.7743 - val_categorical_accuracy: 0.7671\n",
            "Epoch 230/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6700 - categorical_accuracy: 0.7862 - val_loss: 0.7707 - val_categorical_accuracy: 0.7671\n",
            "Epoch 231/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6632 - categorical_accuracy: 0.7685 - val_loss: 0.7745 - val_categorical_accuracy: 0.7671\n",
            "Epoch 232/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6821 - categorical_accuracy: 0.7567 - val_loss: 0.7685 - val_categorical_accuracy: 0.7808\n",
            "Epoch 233/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6669 - categorical_accuracy: 0.7737 - val_loss: 0.7691 - val_categorical_accuracy: 0.7808\n",
            "Epoch 234/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6661 - categorical_accuracy: 0.7552 - val_loss: 0.7723 - val_categorical_accuracy: 0.7397\n",
            "Epoch 235/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6484 - categorical_accuracy: 0.7759 - val_loss: 0.7679 - val_categorical_accuracy: 0.7534\n",
            "Epoch 236/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6858 - categorical_accuracy: 0.7544 - val_loss: 0.7742 - val_categorical_accuracy: 0.7534\n",
            "Epoch 237/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6461 - categorical_accuracy: 0.7877 - val_loss: 0.7671 - val_categorical_accuracy: 0.7534\n",
            "Epoch 238/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6688 - categorical_accuracy: 0.7596 - val_loss: 0.7567 - val_categorical_accuracy: 0.7671\n",
            "Epoch 239/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6644 - categorical_accuracy: 0.7648 - val_loss: 0.7500 - val_categorical_accuracy: 0.7671\n",
            "Epoch 240/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6508 - categorical_accuracy: 0.7781 - val_loss: 0.7559 - val_categorical_accuracy: 0.7808\n",
            "Epoch 241/1100\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.6447 - categorical_accuracy: 0.7692 - val_loss: 0.7573 - val_categorical_accuracy: 0.7808\n",
            "Epoch 242/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6305 - categorical_accuracy: 0.7840 - val_loss: 0.7446 - val_categorical_accuracy: 0.7945\n",
            "Epoch 243/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6762 - categorical_accuracy: 0.7700 - val_loss: 0.7350 - val_categorical_accuracy: 0.7945\n",
            "Epoch 244/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6215 - categorical_accuracy: 0.7811 - val_loss: 0.7416 - val_categorical_accuracy: 0.7671\n",
            "Epoch 245/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6607 - categorical_accuracy: 0.7707 - val_loss: 0.7361 - val_categorical_accuracy: 0.7945\n",
            "Epoch 246/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6351 - categorical_accuracy: 0.7788 - val_loss: 0.7478 - val_categorical_accuracy: 0.7945\n",
            "Epoch 247/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6436 - categorical_accuracy: 0.7714 - val_loss: 0.7449 - val_categorical_accuracy: 0.8082\n",
            "Epoch 248/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.6103 - categorical_accuracy: 0.7944 - val_loss: 0.7598 - val_categorical_accuracy: 0.7945\n",
            "Epoch 249/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6453 - categorical_accuracy: 0.7729 - val_loss: 0.7624 - val_categorical_accuracy: 0.7808\n",
            "Epoch 250/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.6102 - categorical_accuracy: 0.7862 - val_loss: 0.7652 - val_categorical_accuracy: 0.7671\n",
            "Epoch 251/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6168 - categorical_accuracy: 0.7892 - val_loss: 0.7695 - val_categorical_accuracy: 0.7671\n",
            "Epoch 252/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.6744 - categorical_accuracy: 0.7559 - val_loss: 0.7655 - val_categorical_accuracy: 0.7945\n",
            "Epoch 253/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6262 - categorical_accuracy: 0.7899 - val_loss: 0.7624 - val_categorical_accuracy: 0.7808\n",
            "Epoch 254/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.6412 - categorical_accuracy: 0.7685 - val_loss: 0.7698 - val_categorical_accuracy: 0.7671\n",
            "Epoch 255/1100\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.6159 - categorical_accuracy: 0.7862 - val_loss: 0.7661 - val_categorical_accuracy: 0.7671\n",
            "Epoch 256/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.6336 - categorical_accuracy: 0.7685 - val_loss: 0.7580 - val_categorical_accuracy: 0.7671\n",
            "Epoch 257/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.6299 - categorical_accuracy: 0.7788 - val_loss: 0.7317 - val_categorical_accuracy: 0.7808\n",
            "Epoch 258/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.6244 - categorical_accuracy: 0.7751 - val_loss: 0.7389 - val_categorical_accuracy: 0.7945\n",
            "Epoch 259/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6125 - categorical_accuracy: 0.7788 - val_loss: 0.7404 - val_categorical_accuracy: 0.7945\n",
            "Epoch 260/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.6111 - categorical_accuracy: 0.7899 - val_loss: 0.7299 - val_categorical_accuracy: 0.7945\n",
            "Epoch 261/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6489 - categorical_accuracy: 0.7707 - val_loss: 0.7368 - val_categorical_accuracy: 0.7808\n",
            "Epoch 262/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6241 - categorical_accuracy: 0.7766 - val_loss: 0.7508 - val_categorical_accuracy: 0.7945\n",
            "Epoch 263/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6352 - categorical_accuracy: 0.7759 - val_loss: 0.7485 - val_categorical_accuracy: 0.7808\n",
            "Epoch 264/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6224 - categorical_accuracy: 0.7825 - val_loss: 0.7332 - val_categorical_accuracy: 0.8082\n",
            "Epoch 265/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6411 - categorical_accuracy: 0.7737 - val_loss: 0.7462 - val_categorical_accuracy: 0.8082\n",
            "Epoch 266/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5754 - categorical_accuracy: 0.8151 - val_loss: 0.7617 - val_categorical_accuracy: 0.7808\n",
            "Epoch 267/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6329 - categorical_accuracy: 0.7707 - val_loss: 0.7556 - val_categorical_accuracy: 0.7671\n",
            "Epoch 268/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6377 - categorical_accuracy: 0.7788 - val_loss: 0.7469 - val_categorical_accuracy: 0.7808\n",
            "Epoch 269/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5796 - categorical_accuracy: 0.7899 - val_loss: 0.7431 - val_categorical_accuracy: 0.7945\n",
            "Epoch 270/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6018 - categorical_accuracy: 0.7907 - val_loss: 0.7378 - val_categorical_accuracy: 0.8082\n",
            "Epoch 271/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6123 - categorical_accuracy: 0.7922 - val_loss: 0.7457 - val_categorical_accuracy: 0.7945\n",
            "Epoch 272/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6114 - categorical_accuracy: 0.7877 - val_loss: 0.7609 - val_categorical_accuracy: 0.7945\n",
            "Epoch 273/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5939 - categorical_accuracy: 0.7959 - val_loss: 0.7483 - val_categorical_accuracy: 0.8082\n",
            "Epoch 274/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6152 - categorical_accuracy: 0.7877 - val_loss: 0.7480 - val_categorical_accuracy: 0.8082\n",
            "Epoch 275/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6202 - categorical_accuracy: 0.7825 - val_loss: 0.7492 - val_categorical_accuracy: 0.7945\n",
            "Epoch 276/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5929 - categorical_accuracy: 0.7907 - val_loss: 0.7449 - val_categorical_accuracy: 0.7534\n",
            "Epoch 277/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5899 - categorical_accuracy: 0.7981 - val_loss: 0.7288 - val_categorical_accuracy: 0.7808\n",
            "Epoch 278/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.6130 - categorical_accuracy: 0.7936 - val_loss: 0.7456 - val_categorical_accuracy: 0.7671\n",
            "Epoch 279/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6047 - categorical_accuracy: 0.7929 - val_loss: 0.7434 - val_categorical_accuracy: 0.7671\n",
            "Epoch 280/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5740 - categorical_accuracy: 0.7996 - val_loss: 0.7469 - val_categorical_accuracy: 0.7671\n",
            "Epoch 281/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5952 - categorical_accuracy: 0.7840 - val_loss: 0.7437 - val_categorical_accuracy: 0.7397\n",
            "Epoch 282/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5715 - categorical_accuracy: 0.8040 - val_loss: 0.7521 - val_categorical_accuracy: 0.7534\n",
            "Epoch 283/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5874 - categorical_accuracy: 0.7840 - val_loss: 0.7450 - val_categorical_accuracy: 0.7397\n",
            "Epoch 284/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6029 - categorical_accuracy: 0.7966 - val_loss: 0.7583 - val_categorical_accuracy: 0.7534\n",
            "Epoch 285/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6190 - categorical_accuracy: 0.7818 - val_loss: 0.7632 - val_categorical_accuracy: 0.7671\n",
            "Epoch 286/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5866 - categorical_accuracy: 0.8033 - val_loss: 0.7456 - val_categorical_accuracy: 0.7808\n",
            "Epoch 287/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5889 - categorical_accuracy: 0.7922 - val_loss: 0.7448 - val_categorical_accuracy: 0.7945\n",
            "Epoch 288/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5833 - categorical_accuracy: 0.7929 - val_loss: 0.7362 - val_categorical_accuracy: 0.7808\n",
            "Epoch 289/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.6353 - categorical_accuracy: 0.7811 - val_loss: 0.7377 - val_categorical_accuracy: 0.7808\n",
            "Epoch 290/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5826 - categorical_accuracy: 0.7885 - val_loss: 0.7433 - val_categorical_accuracy: 0.7808\n",
            "Epoch 291/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6367 - categorical_accuracy: 0.7774 - val_loss: 0.7492 - val_categorical_accuracy: 0.7671\n",
            "Epoch 292/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5793 - categorical_accuracy: 0.7996 - val_loss: 0.7407 - val_categorical_accuracy: 0.7808\n",
            "Epoch 293/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5879 - categorical_accuracy: 0.7914 - val_loss: 0.7351 - val_categorical_accuracy: 0.7945\n",
            "Epoch 294/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.5767 - categorical_accuracy: 0.8003 - val_loss: 0.7420 - val_categorical_accuracy: 0.7808\n",
            "Epoch 295/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.5758 - categorical_accuracy: 0.7929 - val_loss: 0.7442 - val_categorical_accuracy: 0.7671\n",
            "Epoch 296/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5911 - categorical_accuracy: 0.7899 - val_loss: 0.7464 - val_categorical_accuracy: 0.7671\n",
            "Epoch 297/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5652 - categorical_accuracy: 0.8121 - val_loss: 0.7501 - val_categorical_accuracy: 0.7534\n",
            "Epoch 298/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6103 - categorical_accuracy: 0.7907 - val_loss: 0.7415 - val_categorical_accuracy: 0.7945\n",
            "Epoch 299/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5577 - categorical_accuracy: 0.8107 - val_loss: 0.7331 - val_categorical_accuracy: 0.7671\n",
            "Epoch 300/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5632 - categorical_accuracy: 0.7936 - val_loss: 0.7365 - val_categorical_accuracy: 0.7671\n",
            "Epoch 301/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5794 - categorical_accuracy: 0.7996 - val_loss: 0.7393 - val_categorical_accuracy: 0.7808\n",
            "Epoch 302/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5866 - categorical_accuracy: 0.7966 - val_loss: 0.7524 - val_categorical_accuracy: 0.7671\n",
            "Epoch 303/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5569 - categorical_accuracy: 0.8033 - val_loss: 0.7552 - val_categorical_accuracy: 0.7945\n",
            "Epoch 304/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5601 - categorical_accuracy: 0.8018 - val_loss: 0.7564 - val_categorical_accuracy: 0.7671\n",
            "Epoch 305/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5943 - categorical_accuracy: 0.7944 - val_loss: 0.7572 - val_categorical_accuracy: 0.7671\n",
            "Epoch 306/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6004 - categorical_accuracy: 0.7781 - val_loss: 0.7549 - val_categorical_accuracy: 0.7808\n",
            "Epoch 307/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5901 - categorical_accuracy: 0.7944 - val_loss: 0.7485 - val_categorical_accuracy: 0.7671\n",
            "Epoch 308/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5982 - categorical_accuracy: 0.7914 - val_loss: 0.7506 - val_categorical_accuracy: 0.7671\n",
            "Epoch 309/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5658 - categorical_accuracy: 0.7981 - val_loss: 0.7339 - val_categorical_accuracy: 0.7671\n",
            "Epoch 310/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5774 - categorical_accuracy: 0.8010 - val_loss: 0.7368 - val_categorical_accuracy: 0.7671\n",
            "Epoch 311/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5694 - categorical_accuracy: 0.8099 - val_loss: 0.7426 - val_categorical_accuracy: 0.7671\n",
            "Epoch 312/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5667 - categorical_accuracy: 0.7966 - val_loss: 0.7508 - val_categorical_accuracy: 0.7671\n",
            "Epoch 313/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5843 - categorical_accuracy: 0.7929 - val_loss: 0.7454 - val_categorical_accuracy: 0.7808\n",
            "Epoch 314/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5929 - categorical_accuracy: 0.7855 - val_loss: 0.7476 - val_categorical_accuracy: 0.7808\n",
            "Epoch 315/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5626 - categorical_accuracy: 0.8033 - val_loss: 0.7492 - val_categorical_accuracy: 0.7534\n",
            "Epoch 316/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5656 - categorical_accuracy: 0.8070 - val_loss: 0.7493 - val_categorical_accuracy: 0.7808\n",
            "Epoch 317/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5549 - categorical_accuracy: 0.8084 - val_loss: 0.7335 - val_categorical_accuracy: 0.7808\n",
            "Epoch 318/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5441 - categorical_accuracy: 0.8010 - val_loss: 0.7314 - val_categorical_accuracy: 0.7534\n",
            "Epoch 319/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5645 - categorical_accuracy: 0.8040 - val_loss: 0.7298 - val_categorical_accuracy: 0.7671\n",
            "Epoch 320/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5727 - categorical_accuracy: 0.8025 - val_loss: 0.7314 - val_categorical_accuracy: 0.7397\n",
            "Epoch 321/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5698 - categorical_accuracy: 0.8040 - val_loss: 0.7200 - val_categorical_accuracy: 0.7808\n",
            "Epoch 322/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5765 - categorical_accuracy: 0.7959 - val_loss: 0.7058 - val_categorical_accuracy: 0.7808\n",
            "Epoch 323/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5326 - categorical_accuracy: 0.8210 - val_loss: 0.7130 - val_categorical_accuracy: 0.7945\n",
            "Epoch 324/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5425 - categorical_accuracy: 0.8114 - val_loss: 0.7303 - val_categorical_accuracy: 0.7671\n",
            "Epoch 325/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5592 - categorical_accuracy: 0.8084 - val_loss: 0.7361 - val_categorical_accuracy: 0.7671\n",
            "Epoch 326/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5644 - categorical_accuracy: 0.7951 - val_loss: 0.7282 - val_categorical_accuracy: 0.7671\n",
            "Epoch 327/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5586 - categorical_accuracy: 0.8010 - val_loss: 0.7333 - val_categorical_accuracy: 0.7397\n",
            "Epoch 328/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5583 - categorical_accuracy: 0.8151 - val_loss: 0.7333 - val_categorical_accuracy: 0.7671\n",
            "Epoch 329/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.5595 - categorical_accuracy: 0.8018 - val_loss: 0.7319 - val_categorical_accuracy: 0.7671\n",
            "Epoch 330/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.5386 - categorical_accuracy: 0.8040 - val_loss: 0.7205 - val_categorical_accuracy: 0.7808\n",
            "Epoch 331/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.5460 - categorical_accuracy: 0.8107 - val_loss: 0.7136 - val_categorical_accuracy: 0.7945\n",
            "Epoch 332/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5481 - categorical_accuracy: 0.8136 - val_loss: 0.7099 - val_categorical_accuracy: 0.7945\n",
            "Epoch 333/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5712 - categorical_accuracy: 0.7936 - val_loss: 0.7141 - val_categorical_accuracy: 0.7808\n",
            "Epoch 334/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5585 - categorical_accuracy: 0.8077 - val_loss: 0.7126 - val_categorical_accuracy: 0.7808\n",
            "Epoch 335/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5427 - categorical_accuracy: 0.8121 - val_loss: 0.7173 - val_categorical_accuracy: 0.7808\n",
            "Epoch 336/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5574 - categorical_accuracy: 0.8010 - val_loss: 0.7155 - val_categorical_accuracy: 0.7945\n",
            "Epoch 337/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5268 - categorical_accuracy: 0.8151 - val_loss: 0.7216 - val_categorical_accuracy: 0.7808\n",
            "Epoch 338/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5550 - categorical_accuracy: 0.8099 - val_loss: 0.7198 - val_categorical_accuracy: 0.7808\n",
            "Epoch 339/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5511 - categorical_accuracy: 0.8040 - val_loss: 0.7162 - val_categorical_accuracy: 0.7945\n",
            "Epoch 340/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5515 - categorical_accuracy: 0.8107 - val_loss: 0.6916 - val_categorical_accuracy: 0.8082\n",
            "Epoch 341/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5315 - categorical_accuracy: 0.8136 - val_loss: 0.6984 - val_categorical_accuracy: 0.8082\n",
            "Epoch 342/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5557 - categorical_accuracy: 0.8143 - val_loss: 0.7038 - val_categorical_accuracy: 0.8082\n",
            "Epoch 343/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5492 - categorical_accuracy: 0.8092 - val_loss: 0.7170 - val_categorical_accuracy: 0.8082\n",
            "Epoch 344/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5479 - categorical_accuracy: 0.8143 - val_loss: 0.7234 - val_categorical_accuracy: 0.7808\n",
            "Epoch 345/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5816 - categorical_accuracy: 0.8040 - val_loss: 0.7150 - val_categorical_accuracy: 0.7945\n",
            "Epoch 346/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5251 - categorical_accuracy: 0.8180 - val_loss: 0.7054 - val_categorical_accuracy: 0.8082\n",
            "Epoch 347/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5465 - categorical_accuracy: 0.8062 - val_loss: 0.7080 - val_categorical_accuracy: 0.7945\n",
            "Epoch 348/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5366 - categorical_accuracy: 0.8099 - val_loss: 0.7022 - val_categorical_accuracy: 0.8082\n",
            "Epoch 349/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5343 - categorical_accuracy: 0.8084 - val_loss: 0.6973 - val_categorical_accuracy: 0.8082\n",
            "Epoch 350/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5407 - categorical_accuracy: 0.8129 - val_loss: 0.7091 - val_categorical_accuracy: 0.8082\n",
            "Epoch 351/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5369 - categorical_accuracy: 0.8232 - val_loss: 0.7154 - val_categorical_accuracy: 0.7808\n",
            "Epoch 352/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5619 - categorical_accuracy: 0.8025 - val_loss: 0.7202 - val_categorical_accuracy: 0.7945\n",
            "Epoch 353/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5379 - categorical_accuracy: 0.8077 - val_loss: 0.7128 - val_categorical_accuracy: 0.7945\n",
            "Epoch 354/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5232 - categorical_accuracy: 0.8195 - val_loss: 0.7032 - val_categorical_accuracy: 0.7945\n",
            "Epoch 355/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5318 - categorical_accuracy: 0.8099 - val_loss: 0.7099 - val_categorical_accuracy: 0.7945\n",
            "Epoch 356/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5212 - categorical_accuracy: 0.8306 - val_loss: 0.7083 - val_categorical_accuracy: 0.7945\n",
            "Epoch 357/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5239 - categorical_accuracy: 0.8143 - val_loss: 0.7098 - val_categorical_accuracy: 0.8219\n",
            "Epoch 358/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5448 - categorical_accuracy: 0.8062 - val_loss: 0.7005 - val_categorical_accuracy: 0.8082\n",
            "Epoch 359/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5331 - categorical_accuracy: 0.8070 - val_loss: 0.7048 - val_categorical_accuracy: 0.8082\n",
            "Epoch 360/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5400 - categorical_accuracy: 0.8040 - val_loss: 0.7073 - val_categorical_accuracy: 0.8082\n",
            "Epoch 361/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.5280 - categorical_accuracy: 0.8188 - val_loss: 0.7258 - val_categorical_accuracy: 0.7945\n",
            "Epoch 362/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5373 - categorical_accuracy: 0.8136 - val_loss: 0.7256 - val_categorical_accuracy: 0.7945\n",
            "Epoch 363/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5357 - categorical_accuracy: 0.8232 - val_loss: 0.7284 - val_categorical_accuracy: 0.8082\n",
            "Epoch 364/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.5461 - categorical_accuracy: 0.8143 - val_loss: 0.7146 - val_categorical_accuracy: 0.8082\n",
            "Epoch 365/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.5417 - categorical_accuracy: 0.8092 - val_loss: 0.7096 - val_categorical_accuracy: 0.8082\n",
            "Epoch 366/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5143 - categorical_accuracy: 0.8277 - val_loss: 0.7033 - val_categorical_accuracy: 0.7945\n",
            "Epoch 367/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.4999 - categorical_accuracy: 0.8336 - val_loss: 0.7078 - val_categorical_accuracy: 0.8082\n",
            "Epoch 368/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.5210 - categorical_accuracy: 0.8262 - val_loss: 0.7102 - val_categorical_accuracy: 0.7945\n",
            "Epoch 369/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5326 - categorical_accuracy: 0.8107 - val_loss: 0.7095 - val_categorical_accuracy: 0.7945\n",
            "Epoch 370/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5289 - categorical_accuracy: 0.8099 - val_loss: 0.7127 - val_categorical_accuracy: 0.7808\n",
            "Epoch 371/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5472 - categorical_accuracy: 0.8047 - val_loss: 0.7200 - val_categorical_accuracy: 0.7808\n",
            "Epoch 372/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5066 - categorical_accuracy: 0.8336 - val_loss: 0.7101 - val_categorical_accuracy: 0.7945\n",
            "Epoch 373/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5237 - categorical_accuracy: 0.8158 - val_loss: 0.7084 - val_categorical_accuracy: 0.8082\n",
            "Epoch 374/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5267 - categorical_accuracy: 0.8099 - val_loss: 0.7045 - val_categorical_accuracy: 0.7945\n",
            "Epoch 375/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4953 - categorical_accuracy: 0.8284 - val_loss: 0.6921 - val_categorical_accuracy: 0.8082\n",
            "Epoch 376/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5043 - categorical_accuracy: 0.8284 - val_loss: 0.6977 - val_categorical_accuracy: 0.8082\n",
            "Epoch 377/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5541 - categorical_accuracy: 0.8136 - val_loss: 0.7040 - val_categorical_accuracy: 0.7945\n",
            "Epoch 378/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5366 - categorical_accuracy: 0.8084 - val_loss: 0.7020 - val_categorical_accuracy: 0.7945\n",
            "Epoch 379/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5377 - categorical_accuracy: 0.8018 - val_loss: 0.7040 - val_categorical_accuracy: 0.7945\n",
            "Epoch 380/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4920 - categorical_accuracy: 0.8306 - val_loss: 0.6973 - val_categorical_accuracy: 0.8082\n",
            "Epoch 381/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5446 - categorical_accuracy: 0.8114 - val_loss: 0.6942 - val_categorical_accuracy: 0.8082\n",
            "Epoch 382/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5300 - categorical_accuracy: 0.8180 - val_loss: 0.6943 - val_categorical_accuracy: 0.8082\n",
            "Epoch 383/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4850 - categorical_accuracy: 0.8336 - val_loss: 0.7002 - val_categorical_accuracy: 0.8219\n",
            "Epoch 384/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5294 - categorical_accuracy: 0.8099 - val_loss: 0.7063 - val_categorical_accuracy: 0.8082\n",
            "Epoch 385/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5289 - categorical_accuracy: 0.8203 - val_loss: 0.7011 - val_categorical_accuracy: 0.8082\n",
            "Epoch 386/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5023 - categorical_accuracy: 0.8232 - val_loss: 0.7005 - val_categorical_accuracy: 0.8082\n",
            "Epoch 387/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4910 - categorical_accuracy: 0.8291 - val_loss: 0.6984 - val_categorical_accuracy: 0.8219\n",
            "Epoch 388/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5263 - categorical_accuracy: 0.8254 - val_loss: 0.7108 - val_categorical_accuracy: 0.8082\n",
            "Epoch 389/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5377 - categorical_accuracy: 0.8084 - val_loss: 0.7027 - val_categorical_accuracy: 0.8082\n",
            "Epoch 390/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5215 - categorical_accuracy: 0.8247 - val_loss: 0.7027 - val_categorical_accuracy: 0.8082\n",
            "Epoch 391/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4977 - categorical_accuracy: 0.8291 - val_loss: 0.7016 - val_categorical_accuracy: 0.8082\n",
            "Epoch 392/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5000 - categorical_accuracy: 0.8284 - val_loss: 0.6996 - val_categorical_accuracy: 0.7945\n",
            "Epoch 393/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5038 - categorical_accuracy: 0.8277 - val_loss: 0.6962 - val_categorical_accuracy: 0.8082\n",
            "Epoch 394/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5060 - categorical_accuracy: 0.8277 - val_loss: 0.6970 - val_categorical_accuracy: 0.8082\n",
            "Epoch 395/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5025 - categorical_accuracy: 0.8262 - val_loss: 0.6971 - val_categorical_accuracy: 0.8082\n",
            "Epoch 396/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4937 - categorical_accuracy: 0.8291 - val_loss: 0.7036 - val_categorical_accuracy: 0.8082\n",
            "Epoch 397/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5019 - categorical_accuracy: 0.8299 - val_loss: 0.7065 - val_categorical_accuracy: 0.8082\n",
            "Epoch 398/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5190 - categorical_accuracy: 0.8195 - val_loss: 0.7059 - val_categorical_accuracy: 0.7945\n",
            "Epoch 399/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4743 - categorical_accuracy: 0.8476 - val_loss: 0.7038 - val_categorical_accuracy: 0.8082\n",
            "Epoch 400/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.5045 - categorical_accuracy: 0.8314 - val_loss: 0.6963 - val_categorical_accuracy: 0.8082\n",
            "Epoch 401/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.5074 - categorical_accuracy: 0.8180 - val_loss: 0.6870 - val_categorical_accuracy: 0.8082\n",
            "Epoch 402/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.5179 - categorical_accuracy: 0.8203 - val_loss: 0.6900 - val_categorical_accuracy: 0.7945\n",
            "Epoch 403/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4943 - categorical_accuracy: 0.8306 - val_loss: 0.6837 - val_categorical_accuracy: 0.8082\n",
            "Epoch 404/1100\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 0.4987 - categorical_accuracy: 0.8336 - val_loss: 0.6926 - val_categorical_accuracy: 0.8082\n",
            "Epoch 405/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5048 - categorical_accuracy: 0.8321 - val_loss: 0.6945 - val_categorical_accuracy: 0.8356\n",
            "Epoch 406/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5063 - categorical_accuracy: 0.8195 - val_loss: 0.6991 - val_categorical_accuracy: 0.7945\n",
            "Epoch 407/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5140 - categorical_accuracy: 0.8254 - val_loss: 0.7060 - val_categorical_accuracy: 0.7945\n",
            "Epoch 408/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5182 - categorical_accuracy: 0.8277 - val_loss: 0.7018 - val_categorical_accuracy: 0.8219\n",
            "Epoch 409/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5095 - categorical_accuracy: 0.8232 - val_loss: 0.6923 - val_categorical_accuracy: 0.8219\n",
            "Epoch 410/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4911 - categorical_accuracy: 0.8247 - val_loss: 0.6866 - val_categorical_accuracy: 0.8219\n",
            "Epoch 411/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4753 - categorical_accuracy: 0.8373 - val_loss: 0.6824 - val_categorical_accuracy: 0.8219\n",
            "Epoch 412/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5110 - categorical_accuracy: 0.8173 - val_loss: 0.6808 - val_categorical_accuracy: 0.8219\n",
            "Epoch 413/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5072 - categorical_accuracy: 0.8254 - val_loss: 0.6893 - val_categorical_accuracy: 0.7945\n",
            "Epoch 414/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5132 - categorical_accuracy: 0.8388 - val_loss: 0.6852 - val_categorical_accuracy: 0.8219\n",
            "Epoch 415/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5085 - categorical_accuracy: 0.8240 - val_loss: 0.6884 - val_categorical_accuracy: 0.8082\n",
            "Epoch 416/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4870 - categorical_accuracy: 0.8328 - val_loss: 0.6909 - val_categorical_accuracy: 0.8082\n",
            "Epoch 417/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4667 - categorical_accuracy: 0.8469 - val_loss: 0.6932 - val_categorical_accuracy: 0.8082\n",
            "Epoch 418/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5104 - categorical_accuracy: 0.8247 - val_loss: 0.6913 - val_categorical_accuracy: 0.8219\n",
            "Epoch 419/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4807 - categorical_accuracy: 0.8306 - val_loss: 0.6771 - val_categorical_accuracy: 0.7945\n",
            "Epoch 420/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4797 - categorical_accuracy: 0.8365 - val_loss: 0.6769 - val_categorical_accuracy: 0.7945\n",
            "Epoch 421/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4952 - categorical_accuracy: 0.8306 - val_loss: 0.6876 - val_categorical_accuracy: 0.8082\n",
            "Epoch 422/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4868 - categorical_accuracy: 0.8314 - val_loss: 0.6877 - val_categorical_accuracy: 0.8082\n",
            "Epoch 423/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4970 - categorical_accuracy: 0.8217 - val_loss: 0.6972 - val_categorical_accuracy: 0.8082\n",
            "Epoch 424/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4866 - categorical_accuracy: 0.8380 - val_loss: 0.6895 - val_categorical_accuracy: 0.8219\n",
            "Epoch 425/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5085 - categorical_accuracy: 0.8210 - val_loss: 0.6816 - val_categorical_accuracy: 0.8219\n",
            "Epoch 426/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4825 - categorical_accuracy: 0.8365 - val_loss: 0.6703 - val_categorical_accuracy: 0.7945\n",
            "Epoch 427/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5134 - categorical_accuracy: 0.8299 - val_loss: 0.6748 - val_categorical_accuracy: 0.7945\n",
            "Epoch 428/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5165 - categorical_accuracy: 0.8143 - val_loss: 0.6820 - val_categorical_accuracy: 0.8356\n",
            "Epoch 429/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4931 - categorical_accuracy: 0.8210 - val_loss: 0.6806 - val_categorical_accuracy: 0.8219\n",
            "Epoch 430/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.4868 - categorical_accuracy: 0.8306 - val_loss: 0.6808 - val_categorical_accuracy: 0.8219\n",
            "Epoch 431/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4846 - categorical_accuracy: 0.8291 - val_loss: 0.6774 - val_categorical_accuracy: 0.8219\n",
            "Epoch 432/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5016 - categorical_accuracy: 0.8210 - val_loss: 0.6870 - val_categorical_accuracy: 0.8082\n",
            "Epoch 433/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4779 - categorical_accuracy: 0.8402 - val_loss: 0.6840 - val_categorical_accuracy: 0.7945\n",
            "Epoch 434/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4992 - categorical_accuracy: 0.8232 - val_loss: 0.6750 - val_categorical_accuracy: 0.8082\n",
            "Epoch 435/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4757 - categorical_accuracy: 0.8410 - val_loss: 0.6839 - val_categorical_accuracy: 0.8082\n",
            "Epoch 436/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5081 - categorical_accuracy: 0.8188 - val_loss: 0.6788 - val_categorical_accuracy: 0.8082\n",
            "Epoch 437/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4794 - categorical_accuracy: 0.8343 - val_loss: 0.6756 - val_categorical_accuracy: 0.8082\n",
            "Epoch 438/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4919 - categorical_accuracy: 0.8254 - val_loss: 0.6854 - val_categorical_accuracy: 0.7945\n",
            "Epoch 439/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4864 - categorical_accuracy: 0.8336 - val_loss: 0.6841 - val_categorical_accuracy: 0.7808\n",
            "Epoch 440/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5117 - categorical_accuracy: 0.8240 - val_loss: 0.6886 - val_categorical_accuracy: 0.7671\n",
            "Epoch 441/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4969 - categorical_accuracy: 0.8284 - val_loss: 0.6902 - val_categorical_accuracy: 0.7671\n",
            "Epoch 442/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4684 - categorical_accuracy: 0.8469 - val_loss: 0.6907 - val_categorical_accuracy: 0.8082\n",
            "Epoch 443/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4901 - categorical_accuracy: 0.8321 - val_loss: 0.6866 - val_categorical_accuracy: 0.7945\n",
            "Epoch 444/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4855 - categorical_accuracy: 0.8336 - val_loss: 0.6829 - val_categorical_accuracy: 0.7808\n",
            "Epoch 445/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4602 - categorical_accuracy: 0.8432 - val_loss: 0.6814 - val_categorical_accuracy: 0.7945\n",
            "Epoch 446/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4783 - categorical_accuracy: 0.8299 - val_loss: 0.6789 - val_categorical_accuracy: 0.7945\n",
            "Epoch 447/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4740 - categorical_accuracy: 0.8365 - val_loss: 0.6821 - val_categorical_accuracy: 0.7945\n",
            "Epoch 448/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4586 - categorical_accuracy: 0.8417 - val_loss: 0.6786 - val_categorical_accuracy: 0.7945\n",
            "Epoch 449/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4922 - categorical_accuracy: 0.8180 - val_loss: 0.6783 - val_categorical_accuracy: 0.7808\n",
            "Epoch 450/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4617 - categorical_accuracy: 0.8410 - val_loss: 0.6740 - val_categorical_accuracy: 0.7945\n",
            "Epoch 451/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4759 - categorical_accuracy: 0.8328 - val_loss: 0.6767 - val_categorical_accuracy: 0.8082\n",
            "Epoch 452/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4809 - categorical_accuracy: 0.8425 - val_loss: 0.6764 - val_categorical_accuracy: 0.8082\n",
            "Epoch 453/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4686 - categorical_accuracy: 0.8395 - val_loss: 0.6728 - val_categorical_accuracy: 0.8219\n",
            "Epoch 454/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4466 - categorical_accuracy: 0.8499 - val_loss: 0.6707 - val_categorical_accuracy: 0.7945\n",
            "Epoch 455/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4780 - categorical_accuracy: 0.8336 - val_loss: 0.6739 - val_categorical_accuracy: 0.7945\n",
            "Epoch 456/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4766 - categorical_accuracy: 0.8314 - val_loss: 0.6768 - val_categorical_accuracy: 0.7945\n",
            "Epoch 457/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4826 - categorical_accuracy: 0.8373 - val_loss: 0.6833 - val_categorical_accuracy: 0.8082\n",
            "Epoch 458/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4717 - categorical_accuracy: 0.8336 - val_loss: 0.6773 - val_categorical_accuracy: 0.7945\n",
            "Epoch 459/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4652 - categorical_accuracy: 0.8417 - val_loss: 0.6856 - val_categorical_accuracy: 0.7945\n",
            "Epoch 460/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4804 - categorical_accuracy: 0.8395 - val_loss: 0.6945 - val_categorical_accuracy: 0.7808\n",
            "Epoch 461/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4762 - categorical_accuracy: 0.8351 - val_loss: 0.7019 - val_categorical_accuracy: 0.7945\n",
            "Epoch 462/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4812 - categorical_accuracy: 0.8417 - val_loss: 0.7009 - val_categorical_accuracy: 0.8082\n",
            "Epoch 463/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4690 - categorical_accuracy: 0.8314 - val_loss: 0.6973 - val_categorical_accuracy: 0.7945\n",
            "Epoch 464/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4514 - categorical_accuracy: 0.8484 - val_loss: 0.6901 - val_categorical_accuracy: 0.8082\n",
            "Epoch 465/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.4897 - categorical_accuracy: 0.8358 - val_loss: 0.6894 - val_categorical_accuracy: 0.8082\n",
            "Epoch 466/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4613 - categorical_accuracy: 0.8373 - val_loss: 0.6905 - val_categorical_accuracy: 0.8082\n",
            "Epoch 467/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4795 - categorical_accuracy: 0.8254 - val_loss: 0.6964 - val_categorical_accuracy: 0.8082\n",
            "Epoch 468/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4561 - categorical_accuracy: 0.8462 - val_loss: 0.7044 - val_categorical_accuracy: 0.7808\n",
            "Epoch 469/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4591 - categorical_accuracy: 0.8439 - val_loss: 0.7050 - val_categorical_accuracy: 0.7808\n",
            "Epoch 470/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4671 - categorical_accuracy: 0.8439 - val_loss: 0.7066 - val_categorical_accuracy: 0.7808\n",
            "Epoch 471/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4701 - categorical_accuracy: 0.8358 - val_loss: 0.7079 - val_categorical_accuracy: 0.7945\n",
            "Epoch 472/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4610 - categorical_accuracy: 0.8358 - val_loss: 0.7024 - val_categorical_accuracy: 0.7808\n",
            "Epoch 473/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4524 - categorical_accuracy: 0.8410 - val_loss: 0.6984 - val_categorical_accuracy: 0.7945\n",
            "Epoch 474/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4373 - categorical_accuracy: 0.8565 - val_loss: 0.6910 - val_categorical_accuracy: 0.8219\n",
            "Epoch 475/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4486 - categorical_accuracy: 0.8484 - val_loss: 0.6773 - val_categorical_accuracy: 0.7945\n",
            "Epoch 476/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4689 - categorical_accuracy: 0.8447 - val_loss: 0.6784 - val_categorical_accuracy: 0.7808\n",
            "Epoch 477/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4559 - categorical_accuracy: 0.8513 - val_loss: 0.6805 - val_categorical_accuracy: 0.7808\n",
            "Epoch 478/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4660 - categorical_accuracy: 0.8388 - val_loss: 0.6830 - val_categorical_accuracy: 0.7808\n",
            "Epoch 479/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4784 - categorical_accuracy: 0.8417 - val_loss: 0.6816 - val_categorical_accuracy: 0.7808\n",
            "Epoch 480/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4456 - categorical_accuracy: 0.8425 - val_loss: 0.6830 - val_categorical_accuracy: 0.7808\n",
            "Epoch 481/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4635 - categorical_accuracy: 0.8439 - val_loss: 0.6828 - val_categorical_accuracy: 0.7808\n",
            "Epoch 482/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4391 - categorical_accuracy: 0.8550 - val_loss: 0.6687 - val_categorical_accuracy: 0.7808\n",
            "Epoch 483/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4595 - categorical_accuracy: 0.8432 - val_loss: 0.6755 - val_categorical_accuracy: 0.7808\n",
            "Epoch 484/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4965 - categorical_accuracy: 0.8232 - val_loss: 0.6821 - val_categorical_accuracy: 0.7808\n",
            "Epoch 485/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4465 - categorical_accuracy: 0.8462 - val_loss: 0.6801 - val_categorical_accuracy: 0.7945\n",
            "Epoch 486/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4823 - categorical_accuracy: 0.8351 - val_loss: 0.6844 - val_categorical_accuracy: 0.8082\n",
            "Epoch 487/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4682 - categorical_accuracy: 0.8358 - val_loss: 0.6873 - val_categorical_accuracy: 0.8082\n",
            "Epoch 488/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4702 - categorical_accuracy: 0.8395 - val_loss: 0.6766 - val_categorical_accuracy: 0.8082\n",
            "Epoch 489/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4455 - categorical_accuracy: 0.8469 - val_loss: 0.6759 - val_categorical_accuracy: 0.7808\n",
            "Epoch 490/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4390 - categorical_accuracy: 0.8476 - val_loss: 0.6751 - val_categorical_accuracy: 0.8082\n",
            "Epoch 491/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4855 - categorical_accuracy: 0.8373 - val_loss: 0.6737 - val_categorical_accuracy: 0.8082\n",
            "Epoch 492/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4731 - categorical_accuracy: 0.8247 - val_loss: 0.6730 - val_categorical_accuracy: 0.8082\n",
            "Epoch 493/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4557 - categorical_accuracy: 0.8439 - val_loss: 0.6716 - val_categorical_accuracy: 0.8082\n",
            "Epoch 494/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4554 - categorical_accuracy: 0.8476 - val_loss: 0.6820 - val_categorical_accuracy: 0.7945\n",
            "Epoch 495/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4572 - categorical_accuracy: 0.8439 - val_loss: 0.6818 - val_categorical_accuracy: 0.8082\n",
            "Epoch 496/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4572 - categorical_accuracy: 0.8425 - val_loss: 0.6785 - val_categorical_accuracy: 0.7945\n",
            "Epoch 497/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4849 - categorical_accuracy: 0.8395 - val_loss: 0.6736 - val_categorical_accuracy: 0.8082\n",
            "Epoch 498/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4726 - categorical_accuracy: 0.8343 - val_loss: 0.6704 - val_categorical_accuracy: 0.7945\n",
            "Epoch 499/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4559 - categorical_accuracy: 0.8402 - val_loss: 0.6757 - val_categorical_accuracy: 0.7945\n",
            "Epoch 500/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4624 - categorical_accuracy: 0.8365 - val_loss: 0.6749 - val_categorical_accuracy: 0.7945\n",
            "Epoch 501/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4734 - categorical_accuracy: 0.8439 - val_loss: 0.6777 - val_categorical_accuracy: 0.8082\n",
            "Epoch 502/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4291 - categorical_accuracy: 0.8543 - val_loss: 0.6808 - val_categorical_accuracy: 0.8082\n",
            "Epoch 503/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4631 - categorical_accuracy: 0.8395 - val_loss: 0.6809 - val_categorical_accuracy: 0.7945\n",
            "Epoch 504/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4537 - categorical_accuracy: 0.8469 - val_loss: 0.6686 - val_categorical_accuracy: 0.8082\n",
            "Epoch 505/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4502 - categorical_accuracy: 0.8528 - val_loss: 0.6688 - val_categorical_accuracy: 0.8082\n",
            "Epoch 506/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4622 - categorical_accuracy: 0.8328 - val_loss: 0.6743 - val_categorical_accuracy: 0.8082\n",
            "Epoch 507/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4423 - categorical_accuracy: 0.8513 - val_loss: 0.6673 - val_categorical_accuracy: 0.8082\n",
            "Epoch 508/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4335 - categorical_accuracy: 0.8476 - val_loss: 0.6620 - val_categorical_accuracy: 0.8082\n",
            "Epoch 509/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4297 - categorical_accuracy: 0.8550 - val_loss: 0.6623 - val_categorical_accuracy: 0.7945\n",
            "Epoch 510/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.4562 - categorical_accuracy: 0.8454 - val_loss: 0.6664 - val_categorical_accuracy: 0.7945\n",
            "Epoch 511/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4575 - categorical_accuracy: 0.8425 - val_loss: 0.6687 - val_categorical_accuracy: 0.7945\n",
            "Epoch 512/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4455 - categorical_accuracy: 0.8454 - val_loss: 0.6662 - val_categorical_accuracy: 0.7945\n",
            "Epoch 513/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4720 - categorical_accuracy: 0.8299 - val_loss: 0.6730 - val_categorical_accuracy: 0.7808\n",
            "Epoch 514/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4567 - categorical_accuracy: 0.8484 - val_loss: 0.6779 - val_categorical_accuracy: 0.7945\n",
            "Epoch 515/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4218 - categorical_accuracy: 0.8558 - val_loss: 0.6743 - val_categorical_accuracy: 0.7671\n",
            "Epoch 516/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4610 - categorical_accuracy: 0.8328 - val_loss: 0.6725 - val_categorical_accuracy: 0.7945\n",
            "Epoch 517/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4455 - categorical_accuracy: 0.8380 - val_loss: 0.6771 - val_categorical_accuracy: 0.7945\n",
            "Epoch 518/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4376 - categorical_accuracy: 0.8558 - val_loss: 0.6760 - val_categorical_accuracy: 0.7808\n",
            "Epoch 519/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4465 - categorical_accuracy: 0.8417 - val_loss: 0.6772 - val_categorical_accuracy: 0.7945\n",
            "Epoch 520/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4457 - categorical_accuracy: 0.8476 - val_loss: 0.6789 - val_categorical_accuracy: 0.7671\n",
            "Epoch 521/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4461 - categorical_accuracy: 0.8491 - val_loss: 0.6729 - val_categorical_accuracy: 0.7808\n",
            "Epoch 522/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4345 - categorical_accuracy: 0.8425 - val_loss: 0.6686 - val_categorical_accuracy: 0.7945\n",
            "Epoch 523/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4440 - categorical_accuracy: 0.8476 - val_loss: 0.6674 - val_categorical_accuracy: 0.7808\n",
            "Epoch 524/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4562 - categorical_accuracy: 0.8417 - val_loss: 0.6630 - val_categorical_accuracy: 0.7945\n",
            "Epoch 525/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4215 - categorical_accuracy: 0.8639 - val_loss: 0.6584 - val_categorical_accuracy: 0.8082\n",
            "Epoch 526/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4480 - categorical_accuracy: 0.8402 - val_loss: 0.6575 - val_categorical_accuracy: 0.7945\n",
            "Epoch 527/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4481 - categorical_accuracy: 0.8432 - val_loss: 0.6590 - val_categorical_accuracy: 0.7945\n",
            "Epoch 528/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4566 - categorical_accuracy: 0.8402 - val_loss: 0.6597 - val_categorical_accuracy: 0.7945\n",
            "Epoch 529/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4310 - categorical_accuracy: 0.8536 - val_loss: 0.6655 - val_categorical_accuracy: 0.7945\n",
            "Epoch 530/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4400 - categorical_accuracy: 0.8439 - val_loss: 0.6671 - val_categorical_accuracy: 0.7945\n",
            "Epoch 531/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4569 - categorical_accuracy: 0.8314 - val_loss: 0.6693 - val_categorical_accuracy: 0.7945\n",
            "Epoch 532/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4321 - categorical_accuracy: 0.8565 - val_loss: 0.6660 - val_categorical_accuracy: 0.7945\n",
            "Epoch 533/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4464 - categorical_accuracy: 0.8402 - val_loss: 0.6597 - val_categorical_accuracy: 0.7945\n",
            "Epoch 534/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4381 - categorical_accuracy: 0.8491 - val_loss: 0.6533 - val_categorical_accuracy: 0.7945\n",
            "Epoch 535/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4580 - categorical_accuracy: 0.8454 - val_loss: 0.6501 - val_categorical_accuracy: 0.7945\n",
            "Epoch 536/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4852 - categorical_accuracy: 0.8254 - val_loss: 0.6535 - val_categorical_accuracy: 0.7945\n",
            "Epoch 537/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4411 - categorical_accuracy: 0.8528 - val_loss: 0.6543 - val_categorical_accuracy: 0.8082\n",
            "Epoch 538/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4256 - categorical_accuracy: 0.8565 - val_loss: 0.6470 - val_categorical_accuracy: 0.7945\n",
            "Epoch 539/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4492 - categorical_accuracy: 0.8425 - val_loss: 0.6535 - val_categorical_accuracy: 0.7945\n",
            "Epoch 540/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4120 - categorical_accuracy: 0.8572 - val_loss: 0.6555 - val_categorical_accuracy: 0.7945\n",
            "Epoch 541/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4323 - categorical_accuracy: 0.8580 - val_loss: 0.6640 - val_categorical_accuracy: 0.8082\n",
            "Epoch 542/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4407 - categorical_accuracy: 0.8521 - val_loss: 0.6566 - val_categorical_accuracy: 0.8082\n",
            "Epoch 543/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4339 - categorical_accuracy: 0.8595 - val_loss: 0.6543 - val_categorical_accuracy: 0.8082\n",
            "Epoch 544/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4308 - categorical_accuracy: 0.8543 - val_loss: 0.6540 - val_categorical_accuracy: 0.7945\n",
            "Epoch 545/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4426 - categorical_accuracy: 0.8380 - val_loss: 0.6587 - val_categorical_accuracy: 0.7945\n",
            "Epoch 546/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4219 - categorical_accuracy: 0.8587 - val_loss: 0.6680 - val_categorical_accuracy: 0.8082\n",
            "Epoch 547/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4436 - categorical_accuracy: 0.8462 - val_loss: 0.6589 - val_categorical_accuracy: 0.8082\n",
            "Epoch 548/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4418 - categorical_accuracy: 0.8417 - val_loss: 0.6581 - val_categorical_accuracy: 0.8219\n",
            "Epoch 549/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4415 - categorical_accuracy: 0.8528 - val_loss: 0.6579 - val_categorical_accuracy: 0.8082\n",
            "Epoch 550/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4305 - categorical_accuracy: 0.8565 - val_loss: 0.6555 - val_categorical_accuracy: 0.8219\n",
            "Epoch 551/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4417 - categorical_accuracy: 0.8388 - val_loss: 0.6592 - val_categorical_accuracy: 0.8219\n",
            "Epoch 552/1100\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.4415 - categorical_accuracy: 0.8491 - val_loss: 0.6559 - val_categorical_accuracy: 0.7945\n",
            "Epoch 553/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4284 - categorical_accuracy: 0.8565 - val_loss: 0.6581 - val_categorical_accuracy: 0.7945\n",
            "Epoch 554/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4180 - categorical_accuracy: 0.8676 - val_loss: 0.6522 - val_categorical_accuracy: 0.7808\n",
            "Epoch 555/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4685 - categorical_accuracy: 0.8284 - val_loss: 0.6622 - val_categorical_accuracy: 0.7945\n",
            "Epoch 556/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4153 - categorical_accuracy: 0.8654 - val_loss: 0.6578 - val_categorical_accuracy: 0.7945\n",
            "Epoch 557/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4400 - categorical_accuracy: 0.8506 - val_loss: 0.6559 - val_categorical_accuracy: 0.7808\n",
            "Epoch 558/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4306 - categorical_accuracy: 0.8351 - val_loss: 0.6620 - val_categorical_accuracy: 0.7945\n",
            "Epoch 559/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4423 - categorical_accuracy: 0.8402 - val_loss: 0.6632 - val_categorical_accuracy: 0.7671\n",
            "Epoch 560/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4107 - categorical_accuracy: 0.8632 - val_loss: 0.6584 - val_categorical_accuracy: 0.7808\n",
            "Epoch 561/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4142 - categorical_accuracy: 0.8543 - val_loss: 0.6645 - val_categorical_accuracy: 0.7945\n",
            "Epoch 562/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4245 - categorical_accuracy: 0.8499 - val_loss: 0.6773 - val_categorical_accuracy: 0.7808\n",
            "Epoch 563/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4399 - categorical_accuracy: 0.8506 - val_loss: 0.6836 - val_categorical_accuracy: 0.7808\n",
            "Epoch 564/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4213 - categorical_accuracy: 0.8602 - val_loss: 0.6756 - val_categorical_accuracy: 0.7808\n",
            "Epoch 565/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4114 - categorical_accuracy: 0.8521 - val_loss: 0.6742 - val_categorical_accuracy: 0.7945\n",
            "Epoch 566/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4031 - categorical_accuracy: 0.8587 - val_loss: 0.6619 - val_categorical_accuracy: 0.7945\n",
            "Epoch 567/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4276 - categorical_accuracy: 0.8506 - val_loss: 0.6640 - val_categorical_accuracy: 0.7808\n",
            "Epoch 568/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4509 - categorical_accuracy: 0.8402 - val_loss: 0.6601 - val_categorical_accuracy: 0.7945\n",
            "Epoch 569/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4147 - categorical_accuracy: 0.8639 - val_loss: 0.6614 - val_categorical_accuracy: 0.7945\n",
            "Epoch 570/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4039 - categorical_accuracy: 0.8639 - val_loss: 0.6584 - val_categorical_accuracy: 0.7945\n",
            "Epoch 571/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4123 - categorical_accuracy: 0.8595 - val_loss: 0.6560 - val_categorical_accuracy: 0.7945\n",
            "Epoch 572/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4025 - categorical_accuracy: 0.8646 - val_loss: 0.6593 - val_categorical_accuracy: 0.7945\n",
            "Epoch 573/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4081 - categorical_accuracy: 0.8624 - val_loss: 0.6585 - val_categorical_accuracy: 0.7945\n",
            "Epoch 574/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4330 - categorical_accuracy: 0.8528 - val_loss: 0.6612 - val_categorical_accuracy: 0.7945\n",
            "Epoch 575/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4322 - categorical_accuracy: 0.8521 - val_loss: 0.6627 - val_categorical_accuracy: 0.7945\n",
            "Epoch 576/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4172 - categorical_accuracy: 0.8565 - val_loss: 0.6609 - val_categorical_accuracy: 0.8082\n",
            "Epoch 577/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4078 - categorical_accuracy: 0.8536 - val_loss: 0.6646 - val_categorical_accuracy: 0.7945\n",
            "Epoch 578/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4113 - categorical_accuracy: 0.8572 - val_loss: 0.6671 - val_categorical_accuracy: 0.7945\n",
            "Epoch 579/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.4239 - categorical_accuracy: 0.8543 - val_loss: 0.6731 - val_categorical_accuracy: 0.8082\n",
            "Epoch 580/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4478 - categorical_accuracy: 0.8528 - val_loss: 0.6770 - val_categorical_accuracy: 0.8082\n",
            "Epoch 581/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4470 - categorical_accuracy: 0.8506 - val_loss: 0.6737 - val_categorical_accuracy: 0.8082\n",
            "Epoch 582/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4142 - categorical_accuracy: 0.8550 - val_loss: 0.6722 - val_categorical_accuracy: 0.8082\n",
            "Epoch 583/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4305 - categorical_accuracy: 0.8565 - val_loss: 0.6728 - val_categorical_accuracy: 0.8219\n",
            "Epoch 584/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4340 - categorical_accuracy: 0.8513 - val_loss: 0.6689 - val_categorical_accuracy: 0.8219\n",
            "Epoch 585/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4176 - categorical_accuracy: 0.8476 - val_loss: 0.6597 - val_categorical_accuracy: 0.8219\n",
            "Epoch 586/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4203 - categorical_accuracy: 0.8528 - val_loss: 0.6574 - val_categorical_accuracy: 0.8082\n",
            "Epoch 587/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4335 - categorical_accuracy: 0.8513 - val_loss: 0.6549 - val_categorical_accuracy: 0.8082\n",
            "Epoch 588/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4480 - categorical_accuracy: 0.8439 - val_loss: 0.6560 - val_categorical_accuracy: 0.8082\n",
            "Epoch 589/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4738 - categorical_accuracy: 0.8284 - val_loss: 0.6555 - val_categorical_accuracy: 0.7945\n",
            "Epoch 590/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4195 - categorical_accuracy: 0.8572 - val_loss: 0.6521 - val_categorical_accuracy: 0.8219\n",
            "Epoch 591/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.4244 - categorical_accuracy: 0.8698 - val_loss: 0.6486 - val_categorical_accuracy: 0.8082\n",
            "Epoch 592/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4281 - categorical_accuracy: 0.8528 - val_loss: 0.6547 - val_categorical_accuracy: 0.8219\n",
            "Epoch 593/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4103 - categorical_accuracy: 0.8713 - val_loss: 0.6532 - val_categorical_accuracy: 0.8219\n",
            "Epoch 594/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4279 - categorical_accuracy: 0.8565 - val_loss: 0.6501 - val_categorical_accuracy: 0.8082\n",
            "Epoch 595/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4332 - categorical_accuracy: 0.8506 - val_loss: 0.6459 - val_categorical_accuracy: 0.8082\n",
            "Epoch 596/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4101 - categorical_accuracy: 0.8543 - val_loss: 0.6450 - val_categorical_accuracy: 0.8082\n",
            "Epoch 597/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3936 - categorical_accuracy: 0.8572 - val_loss: 0.6496 - val_categorical_accuracy: 0.8082\n",
            "Epoch 598/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4356 - categorical_accuracy: 0.8565 - val_loss: 0.6479 - val_categorical_accuracy: 0.8082\n",
            "Epoch 599/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4479 - categorical_accuracy: 0.8558 - val_loss: 0.6442 - val_categorical_accuracy: 0.8082\n",
            "Epoch 600/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4267 - categorical_accuracy: 0.8536 - val_loss: 0.6409 - val_categorical_accuracy: 0.8082\n",
            "Epoch 601/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3853 - categorical_accuracy: 0.8735 - val_loss: 0.6404 - val_categorical_accuracy: 0.8082\n",
            "Epoch 602/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4224 - categorical_accuracy: 0.8602 - val_loss: 0.6450 - val_categorical_accuracy: 0.8082\n",
            "Epoch 603/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4089 - categorical_accuracy: 0.8536 - val_loss: 0.6490 - val_categorical_accuracy: 0.8082\n",
            "Epoch 604/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4318 - categorical_accuracy: 0.8484 - val_loss: 0.6543 - val_categorical_accuracy: 0.8082\n",
            "Epoch 605/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4255 - categorical_accuracy: 0.8602 - val_loss: 0.6509 - val_categorical_accuracy: 0.8082\n",
            "Epoch 606/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4192 - categorical_accuracy: 0.8713 - val_loss: 0.6530 - val_categorical_accuracy: 0.8082\n",
            "Epoch 607/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3918 - categorical_accuracy: 0.8720 - val_loss: 0.6566 - val_categorical_accuracy: 0.8082\n",
            "Epoch 608/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4082 - categorical_accuracy: 0.8639 - val_loss: 0.6592 - val_categorical_accuracy: 0.8082\n",
            "Epoch 609/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4118 - categorical_accuracy: 0.8550 - val_loss: 0.6579 - val_categorical_accuracy: 0.8082\n",
            "Epoch 610/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3959 - categorical_accuracy: 0.8536 - val_loss: 0.6525 - val_categorical_accuracy: 0.8082\n",
            "Epoch 611/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4150 - categorical_accuracy: 0.8550 - val_loss: 0.6493 - val_categorical_accuracy: 0.8082\n",
            "Epoch 612/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4128 - categorical_accuracy: 0.8609 - val_loss: 0.6543 - val_categorical_accuracy: 0.8219\n",
            "Epoch 613/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3912 - categorical_accuracy: 0.8646 - val_loss: 0.6533 - val_categorical_accuracy: 0.8219\n",
            "Epoch 614/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3929 - categorical_accuracy: 0.8735 - val_loss: 0.6551 - val_categorical_accuracy: 0.8219\n",
            "Epoch 615/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4041 - categorical_accuracy: 0.8587 - val_loss: 0.6545 - val_categorical_accuracy: 0.8219\n",
            "Epoch 616/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4235 - categorical_accuracy: 0.8476 - val_loss: 0.6495 - val_categorical_accuracy: 0.8082\n",
            "Epoch 617/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4394 - categorical_accuracy: 0.8425 - val_loss: 0.6516 - val_categorical_accuracy: 0.8082\n",
            "Epoch 618/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.4086 - categorical_accuracy: 0.8609 - val_loss: 0.6519 - val_categorical_accuracy: 0.8082\n",
            "Epoch 619/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4268 - categorical_accuracy: 0.8484 - val_loss: 0.6530 - val_categorical_accuracy: 0.8082\n",
            "Epoch 620/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4198 - categorical_accuracy: 0.8617 - val_loss: 0.6507 - val_categorical_accuracy: 0.7945\n",
            "Epoch 621/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4094 - categorical_accuracy: 0.8602 - val_loss: 0.6497 - val_categorical_accuracy: 0.8219\n",
            "Epoch 622/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4104 - categorical_accuracy: 0.8491 - val_loss: 0.6521 - val_categorical_accuracy: 0.8082\n",
            "Epoch 623/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4242 - categorical_accuracy: 0.8521 - val_loss: 0.6417 - val_categorical_accuracy: 0.8082\n",
            "Epoch 624/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4133 - categorical_accuracy: 0.8491 - val_loss: 0.6353 - val_categorical_accuracy: 0.8082\n",
            "Epoch 625/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3995 - categorical_accuracy: 0.8587 - val_loss: 0.6383 - val_categorical_accuracy: 0.8082\n",
            "Epoch 626/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4298 - categorical_accuracy: 0.8491 - val_loss: 0.6435 - val_categorical_accuracy: 0.7945\n",
            "Epoch 627/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4085 - categorical_accuracy: 0.8661 - val_loss: 0.6462 - val_categorical_accuracy: 0.7945\n",
            "Epoch 628/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3896 - categorical_accuracy: 0.8772 - val_loss: 0.6445 - val_categorical_accuracy: 0.7945\n",
            "Epoch 629/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3898 - categorical_accuracy: 0.8706 - val_loss: 0.6440 - val_categorical_accuracy: 0.8082\n",
            "Epoch 630/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4020 - categorical_accuracy: 0.8602 - val_loss: 0.6504 - val_categorical_accuracy: 0.8219\n",
            "Epoch 631/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4231 - categorical_accuracy: 0.8506 - val_loss: 0.6565 - val_categorical_accuracy: 0.8082\n",
            "Epoch 632/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4117 - categorical_accuracy: 0.8550 - val_loss: 0.6592 - val_categorical_accuracy: 0.8082\n",
            "Epoch 633/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3855 - categorical_accuracy: 0.8735 - val_loss: 0.6565 - val_categorical_accuracy: 0.8082\n",
            "Epoch 634/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4097 - categorical_accuracy: 0.8558 - val_loss: 0.6572 - val_categorical_accuracy: 0.7808\n",
            "Epoch 635/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4166 - categorical_accuracy: 0.8513 - val_loss: 0.6515 - val_categorical_accuracy: 0.7808\n",
            "Epoch 636/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4121 - categorical_accuracy: 0.8572 - val_loss: 0.6417 - val_categorical_accuracy: 0.7945\n",
            "Epoch 637/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4094 - categorical_accuracy: 0.8632 - val_loss: 0.6366 - val_categorical_accuracy: 0.7945\n",
            "Epoch 638/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4245 - categorical_accuracy: 0.8536 - val_loss: 0.6454 - val_categorical_accuracy: 0.7945\n",
            "Epoch 639/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4027 - categorical_accuracy: 0.8757 - val_loss: 0.6428 - val_categorical_accuracy: 0.7945\n",
            "Epoch 640/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3971 - categorical_accuracy: 0.8676 - val_loss: 0.6447 - val_categorical_accuracy: 0.7945\n",
            "Epoch 641/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3986 - categorical_accuracy: 0.8624 - val_loss: 0.6525 - val_categorical_accuracy: 0.7945\n",
            "Epoch 642/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3905 - categorical_accuracy: 0.8661 - val_loss: 0.6551 - val_categorical_accuracy: 0.8082\n",
            "Epoch 643/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.4123 - categorical_accuracy: 0.8521 - val_loss: 0.6559 - val_categorical_accuracy: 0.7945\n",
            "Epoch 644/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4069 - categorical_accuracy: 0.8654 - val_loss: 0.6590 - val_categorical_accuracy: 0.8082\n",
            "Epoch 645/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4128 - categorical_accuracy: 0.8617 - val_loss: 0.6570 - val_categorical_accuracy: 0.7945\n",
            "Epoch 646/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4043 - categorical_accuracy: 0.8661 - val_loss: 0.6542 - val_categorical_accuracy: 0.7945\n",
            "Epoch 647/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4024 - categorical_accuracy: 0.8757 - val_loss: 0.6502 - val_categorical_accuracy: 0.7808\n",
            "Epoch 648/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3894 - categorical_accuracy: 0.8602 - val_loss: 0.6401 - val_categorical_accuracy: 0.7945\n",
            "Epoch 649/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3950 - categorical_accuracy: 0.8558 - val_loss: 0.6444 - val_categorical_accuracy: 0.7945\n",
            "Epoch 650/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3737 - categorical_accuracy: 0.8780 - val_loss: 0.6472 - val_categorical_accuracy: 0.7945\n",
            "Epoch 651/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3949 - categorical_accuracy: 0.8698 - val_loss: 0.6443 - val_categorical_accuracy: 0.7808\n",
            "Epoch 652/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.4034 - categorical_accuracy: 0.8661 - val_loss: 0.6561 - val_categorical_accuracy: 0.7808\n",
            "Epoch 653/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3901 - categorical_accuracy: 0.8624 - val_loss: 0.6589 - val_categorical_accuracy: 0.7808\n",
            "Epoch 654/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4088 - categorical_accuracy: 0.8565 - val_loss: 0.6466 - val_categorical_accuracy: 0.7808\n",
            "Epoch 655/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3841 - categorical_accuracy: 0.8802 - val_loss: 0.6470 - val_categorical_accuracy: 0.7808\n",
            "Epoch 656/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3998 - categorical_accuracy: 0.8765 - val_loss: 0.6473 - val_categorical_accuracy: 0.7808\n",
            "Epoch 657/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3930 - categorical_accuracy: 0.8558 - val_loss: 0.6524 - val_categorical_accuracy: 0.7671\n",
            "Epoch 658/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4042 - categorical_accuracy: 0.8676 - val_loss: 0.6545 - val_categorical_accuracy: 0.7945\n",
            "Epoch 659/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4157 - categorical_accuracy: 0.8476 - val_loss: 0.6501 - val_categorical_accuracy: 0.7808\n",
            "Epoch 660/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3757 - categorical_accuracy: 0.8728 - val_loss: 0.6472 - val_categorical_accuracy: 0.7945\n",
            "Epoch 661/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3840 - categorical_accuracy: 0.8765 - val_loss: 0.6563 - val_categorical_accuracy: 0.7945\n",
            "Epoch 662/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3982 - categorical_accuracy: 0.8683 - val_loss: 0.6500 - val_categorical_accuracy: 0.7945\n",
            "Epoch 663/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3919 - categorical_accuracy: 0.8654 - val_loss: 0.6504 - val_categorical_accuracy: 0.7945\n",
            "Epoch 664/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3961 - categorical_accuracy: 0.8720 - val_loss: 0.6519 - val_categorical_accuracy: 0.7945\n",
            "Epoch 665/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3909 - categorical_accuracy: 0.8609 - val_loss: 0.6440 - val_categorical_accuracy: 0.7945\n",
            "Epoch 666/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4021 - categorical_accuracy: 0.8632 - val_loss: 0.6445 - val_categorical_accuracy: 0.7945\n",
            "Epoch 667/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4120 - categorical_accuracy: 0.8528 - val_loss: 0.6514 - val_categorical_accuracy: 0.7945\n",
            "Epoch 668/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3972 - categorical_accuracy: 0.8661 - val_loss: 0.6489 - val_categorical_accuracy: 0.7945\n",
            "Epoch 669/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3823 - categorical_accuracy: 0.8698 - val_loss: 0.6443 - val_categorical_accuracy: 0.7945\n",
            "Epoch 670/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4106 - categorical_accuracy: 0.8661 - val_loss: 0.6486 - val_categorical_accuracy: 0.7945\n",
            "Epoch 671/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4129 - categorical_accuracy: 0.8572 - val_loss: 0.6510 - val_categorical_accuracy: 0.7945\n",
            "Epoch 672/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3911 - categorical_accuracy: 0.8713 - val_loss: 0.6565 - val_categorical_accuracy: 0.7945\n",
            "Epoch 673/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3967 - categorical_accuracy: 0.8676 - val_loss: 0.6554 - val_categorical_accuracy: 0.7945\n",
            "Epoch 674/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3940 - categorical_accuracy: 0.8609 - val_loss: 0.6505 - val_categorical_accuracy: 0.8082\n",
            "Epoch 675/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4079 - categorical_accuracy: 0.8513 - val_loss: 0.6593 - val_categorical_accuracy: 0.8082\n",
            "Epoch 676/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3978 - categorical_accuracy: 0.8757 - val_loss: 0.6560 - val_categorical_accuracy: 0.7945\n",
            "Epoch 677/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3790 - categorical_accuracy: 0.8698 - val_loss: 0.6513 - val_categorical_accuracy: 0.7808\n",
            "Epoch 678/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3957 - categorical_accuracy: 0.8720 - val_loss: 0.6451 - val_categorical_accuracy: 0.7945\n",
            "Epoch 679/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3929 - categorical_accuracy: 0.8691 - val_loss: 0.6463 - val_categorical_accuracy: 0.8082\n",
            "Epoch 680/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3920 - categorical_accuracy: 0.8757 - val_loss: 0.6542 - val_categorical_accuracy: 0.8219\n",
            "Epoch 681/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3944 - categorical_accuracy: 0.8624 - val_loss: 0.6515 - val_categorical_accuracy: 0.8082\n",
            "Epoch 682/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3976 - categorical_accuracy: 0.8654 - val_loss: 0.6533 - val_categorical_accuracy: 0.8082\n",
            "Epoch 683/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3631 - categorical_accuracy: 0.8891 - val_loss: 0.6588 - val_categorical_accuracy: 0.7945\n",
            "Epoch 684/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3790 - categorical_accuracy: 0.8617 - val_loss: 0.6537 - val_categorical_accuracy: 0.7945\n",
            "Epoch 685/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3952 - categorical_accuracy: 0.8521 - val_loss: 0.6483 - val_categorical_accuracy: 0.8082\n",
            "Epoch 686/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.4093 - categorical_accuracy: 0.8587 - val_loss: 0.6454 - val_categorical_accuracy: 0.8082\n",
            "Epoch 687/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3817 - categorical_accuracy: 0.8728 - val_loss: 0.6520 - val_categorical_accuracy: 0.7945\n",
            "Epoch 688/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4123 - categorical_accuracy: 0.8528 - val_loss: 0.6520 - val_categorical_accuracy: 0.7945\n",
            "Epoch 689/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3929 - categorical_accuracy: 0.8646 - val_loss: 0.6480 - val_categorical_accuracy: 0.8082\n",
            "Epoch 690/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3774 - categorical_accuracy: 0.8743 - val_loss: 0.6443 - val_categorical_accuracy: 0.8082\n",
            "Epoch 691/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.4040 - categorical_accuracy: 0.8558 - val_loss: 0.6411 - val_categorical_accuracy: 0.7945\n",
            "Epoch 692/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3967 - categorical_accuracy: 0.8595 - val_loss: 0.6402 - val_categorical_accuracy: 0.7945\n",
            "Epoch 693/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3839 - categorical_accuracy: 0.8787 - val_loss: 0.6398 - val_categorical_accuracy: 0.7945\n",
            "Epoch 694/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.4045 - categorical_accuracy: 0.8720 - val_loss: 0.6369 - val_categorical_accuracy: 0.7945\n",
            "Epoch 695/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3728 - categorical_accuracy: 0.8765 - val_loss: 0.6394 - val_categorical_accuracy: 0.7945\n",
            "Epoch 696/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3937 - categorical_accuracy: 0.8661 - val_loss: 0.6414 - val_categorical_accuracy: 0.8082\n",
            "Epoch 697/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3956 - categorical_accuracy: 0.8654 - val_loss: 0.6409 - val_categorical_accuracy: 0.7945\n",
            "Epoch 698/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4168 - categorical_accuracy: 0.8558 - val_loss: 0.6417 - val_categorical_accuracy: 0.8082\n",
            "Epoch 699/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3711 - categorical_accuracy: 0.8728 - val_loss: 0.6456 - val_categorical_accuracy: 0.7945\n",
            "Epoch 700/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3767 - categorical_accuracy: 0.8780 - val_loss: 0.6473 - val_categorical_accuracy: 0.7945\n",
            "Epoch 701/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3792 - categorical_accuracy: 0.8772 - val_loss: 0.6437 - val_categorical_accuracy: 0.7945\n",
            "Epoch 702/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4013 - categorical_accuracy: 0.8499 - val_loss: 0.6356 - val_categorical_accuracy: 0.7945\n",
            "Epoch 703/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3981 - categorical_accuracy: 0.8587 - val_loss: 0.6412 - val_categorical_accuracy: 0.7945\n",
            "Epoch 704/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3703 - categorical_accuracy: 0.8698 - val_loss: 0.6449 - val_categorical_accuracy: 0.7945\n",
            "Epoch 705/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3622 - categorical_accuracy: 0.8772 - val_loss: 0.6482 - val_categorical_accuracy: 0.8082\n",
            "Epoch 706/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3684 - categorical_accuracy: 0.8854 - val_loss: 0.6447 - val_categorical_accuracy: 0.7945\n",
            "Epoch 707/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3706 - categorical_accuracy: 0.8802 - val_loss: 0.6392 - val_categorical_accuracy: 0.7945\n",
            "Epoch 708/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3811 - categorical_accuracy: 0.8728 - val_loss: 0.6374 - val_categorical_accuracy: 0.7945\n",
            "Epoch 709/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3857 - categorical_accuracy: 0.8543 - val_loss: 0.6290 - val_categorical_accuracy: 0.8082\n",
            "Epoch 710/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4042 - categorical_accuracy: 0.8676 - val_loss: 0.6309 - val_categorical_accuracy: 0.8082\n",
            "Epoch 711/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3919 - categorical_accuracy: 0.8587 - val_loss: 0.6307 - val_categorical_accuracy: 0.8082\n",
            "Epoch 712/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3936 - categorical_accuracy: 0.8587 - val_loss: 0.6341 - val_categorical_accuracy: 0.8082\n",
            "Epoch 713/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3949 - categorical_accuracy: 0.8676 - val_loss: 0.6338 - val_categorical_accuracy: 0.8082\n",
            "Epoch 714/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3843 - categorical_accuracy: 0.8735 - val_loss: 0.6337 - val_categorical_accuracy: 0.8082\n",
            "Epoch 715/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3915 - categorical_accuracy: 0.8698 - val_loss: 0.6339 - val_categorical_accuracy: 0.8219\n",
            "Epoch 716/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4034 - categorical_accuracy: 0.8602 - val_loss: 0.6325 - val_categorical_accuracy: 0.8219\n",
            "Epoch 717/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3900 - categorical_accuracy: 0.8720 - val_loss: 0.6388 - val_categorical_accuracy: 0.8082\n",
            "Epoch 718/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3481 - categorical_accuracy: 0.8868 - val_loss: 0.6349 - val_categorical_accuracy: 0.8082\n",
            "Epoch 719/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3614 - categorical_accuracy: 0.8743 - val_loss: 0.6409 - val_categorical_accuracy: 0.7945\n",
            "Epoch 720/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3703 - categorical_accuracy: 0.8765 - val_loss: 0.6357 - val_categorical_accuracy: 0.8082\n",
            "Epoch 721/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3661 - categorical_accuracy: 0.8839 - val_loss: 0.6362 - val_categorical_accuracy: 0.8082\n",
            "Epoch 722/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3845 - categorical_accuracy: 0.8728 - val_loss: 0.6359 - val_categorical_accuracy: 0.8082\n",
            "Epoch 723/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4037 - categorical_accuracy: 0.8646 - val_loss: 0.6309 - val_categorical_accuracy: 0.8219\n",
            "Epoch 724/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3640 - categorical_accuracy: 0.8743 - val_loss: 0.6321 - val_categorical_accuracy: 0.8356\n",
            "Epoch 725/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3704 - categorical_accuracy: 0.8728 - val_loss: 0.6333 - val_categorical_accuracy: 0.8219\n",
            "Epoch 726/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.4315 - categorical_accuracy: 0.8484 - val_loss: 0.6321 - val_categorical_accuracy: 0.8219\n",
            "Epoch 727/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3659 - categorical_accuracy: 0.8743 - val_loss: 0.6314 - val_categorical_accuracy: 0.7945\n",
            "Epoch 728/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3731 - categorical_accuracy: 0.8669 - val_loss: 0.6347 - val_categorical_accuracy: 0.8082\n",
            "Epoch 729/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3869 - categorical_accuracy: 0.8706 - val_loss: 0.6360 - val_categorical_accuracy: 0.8219\n",
            "Epoch 730/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3726 - categorical_accuracy: 0.8757 - val_loss: 0.6356 - val_categorical_accuracy: 0.8082\n",
            "Epoch 731/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3818 - categorical_accuracy: 0.8661 - val_loss: 0.6363 - val_categorical_accuracy: 0.8219\n",
            "Epoch 732/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3909 - categorical_accuracy: 0.8587 - val_loss: 0.6382 - val_categorical_accuracy: 0.8219\n",
            "Epoch 733/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3653 - categorical_accuracy: 0.8787 - val_loss: 0.6388 - val_categorical_accuracy: 0.8356\n",
            "Epoch 734/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3788 - categorical_accuracy: 0.8632 - val_loss: 0.6364 - val_categorical_accuracy: 0.8356\n",
            "Epoch 735/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3955 - categorical_accuracy: 0.8743 - val_loss: 0.6322 - val_categorical_accuracy: 0.8219\n",
            "Epoch 736/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3673 - categorical_accuracy: 0.8757 - val_loss: 0.6357 - val_categorical_accuracy: 0.8219\n",
            "Epoch 737/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4351 - categorical_accuracy: 0.8543 - val_loss: 0.6328 - val_categorical_accuracy: 0.7945\n",
            "Epoch 738/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3892 - categorical_accuracy: 0.8499 - val_loss: 0.6293 - val_categorical_accuracy: 0.8082\n",
            "Epoch 739/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3806 - categorical_accuracy: 0.8706 - val_loss: 0.6372 - val_categorical_accuracy: 0.8082\n",
            "Epoch 740/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3674 - categorical_accuracy: 0.8750 - val_loss: 0.6364 - val_categorical_accuracy: 0.8082\n",
            "Epoch 741/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4009 - categorical_accuracy: 0.8572 - val_loss: 0.6305 - val_categorical_accuracy: 0.8219\n",
            "Epoch 742/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.4051 - categorical_accuracy: 0.8543 - val_loss: 0.6199 - val_categorical_accuracy: 0.8219\n",
            "Epoch 743/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3726 - categorical_accuracy: 0.8580 - val_loss: 0.6210 - val_categorical_accuracy: 0.8082\n",
            "Epoch 744/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3502 - categorical_accuracy: 0.8876 - val_loss: 0.6225 - val_categorical_accuracy: 0.8082\n",
            "Epoch 745/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3789 - categorical_accuracy: 0.8713 - val_loss: 0.6252 - val_categorical_accuracy: 0.8082\n",
            "Epoch 746/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3931 - categorical_accuracy: 0.8735 - val_loss: 0.6274 - val_categorical_accuracy: 0.7945\n",
            "Epoch 747/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3842 - categorical_accuracy: 0.8787 - val_loss: 0.6302 - val_categorical_accuracy: 0.8082\n",
            "Epoch 748/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.4056 - categorical_accuracy: 0.8550 - val_loss: 0.6289 - val_categorical_accuracy: 0.8219\n",
            "Epoch 749/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3759 - categorical_accuracy: 0.8750 - val_loss: 0.6268 - val_categorical_accuracy: 0.8082\n",
            "Epoch 750/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3987 - categorical_accuracy: 0.8469 - val_loss: 0.6341 - val_categorical_accuracy: 0.8082\n",
            "Epoch 751/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3754 - categorical_accuracy: 0.8765 - val_loss: 0.6358 - val_categorical_accuracy: 0.8082\n",
            "Epoch 752/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3712 - categorical_accuracy: 0.8683 - val_loss: 0.6346 - val_categorical_accuracy: 0.8082\n",
            "Epoch 753/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3866 - categorical_accuracy: 0.8802 - val_loss: 0.6325 - val_categorical_accuracy: 0.7945\n",
            "Epoch 754/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3835 - categorical_accuracy: 0.8646 - val_loss: 0.6383 - val_categorical_accuracy: 0.8082\n",
            "Epoch 755/1100\n",
            "22/22 [==============================] - 1s 33ms/step - loss: 0.3659 - categorical_accuracy: 0.8794 - val_loss: 0.6399 - val_categorical_accuracy: 0.8219\n",
            "Epoch 756/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3960 - categorical_accuracy: 0.8587 - val_loss: 0.6365 - val_categorical_accuracy: 0.8356\n",
            "Epoch 757/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.4041 - categorical_accuracy: 0.8609 - val_loss: 0.6433 - val_categorical_accuracy: 0.8219\n",
            "Epoch 758/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3844 - categorical_accuracy: 0.8639 - val_loss: 0.6420 - val_categorical_accuracy: 0.8219\n",
            "Epoch 759/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3695 - categorical_accuracy: 0.8846 - val_loss: 0.6416 - val_categorical_accuracy: 0.8219\n",
            "Epoch 760/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3695 - categorical_accuracy: 0.8757 - val_loss: 0.6400 - val_categorical_accuracy: 0.8219\n",
            "Epoch 761/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3895 - categorical_accuracy: 0.8676 - val_loss: 0.6456 - val_categorical_accuracy: 0.8082\n",
            "Epoch 762/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3759 - categorical_accuracy: 0.8713 - val_loss: 0.6449 - val_categorical_accuracy: 0.8082\n",
            "Epoch 763/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3694 - categorical_accuracy: 0.8824 - val_loss: 0.6456 - val_categorical_accuracy: 0.8082\n",
            "Epoch 764/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3623 - categorical_accuracy: 0.8831 - val_loss: 0.6447 - val_categorical_accuracy: 0.8082\n",
            "Epoch 765/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3844 - categorical_accuracy: 0.8646 - val_loss: 0.6488 - val_categorical_accuracy: 0.8219\n",
            "Epoch 766/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3629 - categorical_accuracy: 0.8839 - val_loss: 0.6435 - val_categorical_accuracy: 0.8219\n",
            "Epoch 767/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3833 - categorical_accuracy: 0.8787 - val_loss: 0.6445 - val_categorical_accuracy: 0.8082\n",
            "Epoch 768/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3536 - categorical_accuracy: 0.8824 - val_loss: 0.6424 - val_categorical_accuracy: 0.8082\n",
            "Epoch 769/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3605 - categorical_accuracy: 0.8780 - val_loss: 0.6426 - val_categorical_accuracy: 0.7945\n",
            "Epoch 770/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3825 - categorical_accuracy: 0.8698 - val_loss: 0.6452 - val_categorical_accuracy: 0.8082\n",
            "Epoch 771/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3774 - categorical_accuracy: 0.8765 - val_loss: 0.6438 - val_categorical_accuracy: 0.8082\n",
            "Epoch 772/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3660 - categorical_accuracy: 0.8839 - val_loss: 0.6420 - val_categorical_accuracy: 0.8082\n",
            "Epoch 773/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3614 - categorical_accuracy: 0.8750 - val_loss: 0.6441 - val_categorical_accuracy: 0.8219\n",
            "Epoch 774/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3842 - categorical_accuracy: 0.8661 - val_loss: 0.6454 - val_categorical_accuracy: 0.8219\n",
            "Epoch 775/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3801 - categorical_accuracy: 0.8757 - val_loss: 0.6462 - val_categorical_accuracy: 0.8219\n",
            "Epoch 776/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.4049 - categorical_accuracy: 0.8595 - val_loss: 0.6471 - val_categorical_accuracy: 0.8082\n",
            "Epoch 777/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3723 - categorical_accuracy: 0.8676 - val_loss: 0.6435 - val_categorical_accuracy: 0.8082\n",
            "Epoch 778/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3562 - categorical_accuracy: 0.8817 - val_loss: 0.6383 - val_categorical_accuracy: 0.8082\n",
            "Epoch 779/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3702 - categorical_accuracy: 0.8780 - val_loss: 0.6352 - val_categorical_accuracy: 0.8082\n",
            "Epoch 780/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3835 - categorical_accuracy: 0.8676 - val_loss: 0.6334 - val_categorical_accuracy: 0.8082\n",
            "Epoch 781/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3640 - categorical_accuracy: 0.8728 - val_loss: 0.6319 - val_categorical_accuracy: 0.8082\n",
            "Epoch 782/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3785 - categorical_accuracy: 0.8757 - val_loss: 0.6328 - val_categorical_accuracy: 0.8082\n",
            "Epoch 783/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3643 - categorical_accuracy: 0.8706 - val_loss: 0.6295 - val_categorical_accuracy: 0.8082\n",
            "Epoch 784/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3622 - categorical_accuracy: 0.8824 - val_loss: 0.6347 - val_categorical_accuracy: 0.7945\n",
            "Epoch 785/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3755 - categorical_accuracy: 0.8809 - val_loss: 0.6354 - val_categorical_accuracy: 0.7945\n",
            "Epoch 786/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3816 - categorical_accuracy: 0.8691 - val_loss: 0.6374 - val_categorical_accuracy: 0.8082\n",
            "Epoch 787/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3641 - categorical_accuracy: 0.8831 - val_loss: 0.6413 - val_categorical_accuracy: 0.8082\n",
            "Epoch 788/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3681 - categorical_accuracy: 0.8743 - val_loss: 0.6414 - val_categorical_accuracy: 0.8082\n",
            "Epoch 789/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3707 - categorical_accuracy: 0.8735 - val_loss: 0.6369 - val_categorical_accuracy: 0.8082\n",
            "Epoch 790/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3933 - categorical_accuracy: 0.8683 - val_loss: 0.6352 - val_categorical_accuracy: 0.8082\n",
            "Epoch 791/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3507 - categorical_accuracy: 0.8750 - val_loss: 0.6412 - val_categorical_accuracy: 0.8082\n",
            "Epoch 792/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3429 - categorical_accuracy: 0.8839 - val_loss: 0.6398 - val_categorical_accuracy: 0.8082\n",
            "Epoch 793/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3533 - categorical_accuracy: 0.8905 - val_loss: 0.6363 - val_categorical_accuracy: 0.8082\n",
            "Epoch 794/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3504 - categorical_accuracy: 0.8868 - val_loss: 0.6375 - val_categorical_accuracy: 0.8082\n",
            "Epoch 795/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3480 - categorical_accuracy: 0.8905 - val_loss: 0.6372 - val_categorical_accuracy: 0.8082\n",
            "Epoch 796/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3665 - categorical_accuracy: 0.8728 - val_loss: 0.6368 - val_categorical_accuracy: 0.8082\n",
            "Epoch 797/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3644 - categorical_accuracy: 0.8617 - val_loss: 0.6385 - val_categorical_accuracy: 0.8219\n",
            "Epoch 798/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3851 - categorical_accuracy: 0.8654 - val_loss: 0.6338 - val_categorical_accuracy: 0.8082\n",
            "Epoch 799/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3875 - categorical_accuracy: 0.8609 - val_loss: 0.6335 - val_categorical_accuracy: 0.8082\n",
            "Epoch 800/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3490 - categorical_accuracy: 0.8802 - val_loss: 0.6353 - val_categorical_accuracy: 0.8219\n",
            "Epoch 801/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3425 - categorical_accuracy: 0.8861 - val_loss: 0.6362 - val_categorical_accuracy: 0.8219\n",
            "Epoch 802/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3703 - categorical_accuracy: 0.8676 - val_loss: 0.6350 - val_categorical_accuracy: 0.8219\n",
            "Epoch 803/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3799 - categorical_accuracy: 0.8683 - val_loss: 0.6317 - val_categorical_accuracy: 0.8082\n",
            "Epoch 804/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3585 - categorical_accuracy: 0.8802 - val_loss: 0.6292 - val_categorical_accuracy: 0.8082\n",
            "Epoch 805/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3449 - categorical_accuracy: 0.8757 - val_loss: 0.6241 - val_categorical_accuracy: 0.8219\n",
            "Epoch 806/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3908 - categorical_accuracy: 0.8654 - val_loss: 0.6272 - val_categorical_accuracy: 0.8219\n",
            "Epoch 807/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3850 - categorical_accuracy: 0.8720 - val_loss: 0.6348 - val_categorical_accuracy: 0.8219\n",
            "Epoch 808/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3525 - categorical_accuracy: 0.8757 - val_loss: 0.6330 - val_categorical_accuracy: 0.8082\n",
            "Epoch 809/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3555 - categorical_accuracy: 0.8831 - val_loss: 0.6343 - val_categorical_accuracy: 0.8082\n",
            "Epoch 810/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3407 - categorical_accuracy: 0.8802 - val_loss: 0.6318 - val_categorical_accuracy: 0.8219\n",
            "Epoch 811/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3557 - categorical_accuracy: 0.8824 - val_loss: 0.6289 - val_categorical_accuracy: 0.8219\n",
            "Epoch 812/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3670 - categorical_accuracy: 0.8802 - val_loss: 0.6339 - val_categorical_accuracy: 0.8219\n",
            "Epoch 813/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3630 - categorical_accuracy: 0.8735 - val_loss: 0.6288 - val_categorical_accuracy: 0.8219\n",
            "Epoch 814/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3347 - categorical_accuracy: 0.8928 - val_loss: 0.6265 - val_categorical_accuracy: 0.8219\n",
            "Epoch 815/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3362 - categorical_accuracy: 0.8876 - val_loss: 0.6256 - val_categorical_accuracy: 0.8219\n",
            "Epoch 816/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3805 - categorical_accuracy: 0.8609 - val_loss: 0.6323 - val_categorical_accuracy: 0.8082\n",
            "Epoch 817/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3512 - categorical_accuracy: 0.8802 - val_loss: 0.6314 - val_categorical_accuracy: 0.8082\n",
            "Epoch 818/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3629 - categorical_accuracy: 0.8706 - val_loss: 0.6299 - val_categorical_accuracy: 0.7945\n",
            "Epoch 819/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3556 - categorical_accuracy: 0.8787 - val_loss: 0.6293 - val_categorical_accuracy: 0.8082\n",
            "Epoch 820/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3446 - categorical_accuracy: 0.8824 - val_loss: 0.6300 - val_categorical_accuracy: 0.8082\n",
            "Epoch 821/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3647 - categorical_accuracy: 0.8757 - val_loss: 0.6392 - val_categorical_accuracy: 0.8219\n",
            "Epoch 822/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3767 - categorical_accuracy: 0.8624 - val_loss: 0.6331 - val_categorical_accuracy: 0.8219\n",
            "Epoch 823/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3427 - categorical_accuracy: 0.8824 - val_loss: 0.6279 - val_categorical_accuracy: 0.8082\n",
            "Epoch 824/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3539 - categorical_accuracy: 0.8809 - val_loss: 0.6264 - val_categorical_accuracy: 0.8082\n",
            "Epoch 825/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3360 - categorical_accuracy: 0.8846 - val_loss: 0.6255 - val_categorical_accuracy: 0.7945\n",
            "Epoch 826/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3769 - categorical_accuracy: 0.8654 - val_loss: 0.6226 - val_categorical_accuracy: 0.7945\n",
            "Epoch 827/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3501 - categorical_accuracy: 0.8868 - val_loss: 0.6195 - val_categorical_accuracy: 0.8219\n",
            "Epoch 828/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3599 - categorical_accuracy: 0.8824 - val_loss: 0.6263 - val_categorical_accuracy: 0.8219\n",
            "Epoch 829/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3489 - categorical_accuracy: 0.8846 - val_loss: 0.6318 - val_categorical_accuracy: 0.8082\n",
            "Epoch 830/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3698 - categorical_accuracy: 0.8728 - val_loss: 0.6376 - val_categorical_accuracy: 0.8082\n",
            "Epoch 831/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3619 - categorical_accuracy: 0.8802 - val_loss: 0.6360 - val_categorical_accuracy: 0.8082\n",
            "Epoch 832/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3428 - categorical_accuracy: 0.8876 - val_loss: 0.6281 - val_categorical_accuracy: 0.8219\n",
            "Epoch 833/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3292 - categorical_accuracy: 0.8905 - val_loss: 0.6262 - val_categorical_accuracy: 0.8219\n",
            "Epoch 834/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3559 - categorical_accuracy: 0.8802 - val_loss: 0.6259 - val_categorical_accuracy: 0.8219\n",
            "Epoch 835/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3575 - categorical_accuracy: 0.8846 - val_loss: 0.6298 - val_categorical_accuracy: 0.8219\n",
            "Epoch 836/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3926 - categorical_accuracy: 0.8713 - val_loss: 0.6267 - val_categorical_accuracy: 0.8219\n",
            "Epoch 837/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3353 - categorical_accuracy: 0.8861 - val_loss: 0.6209 - val_categorical_accuracy: 0.8082\n",
            "Epoch 838/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3630 - categorical_accuracy: 0.8698 - val_loss: 0.6182 - val_categorical_accuracy: 0.8219\n",
            "Epoch 839/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3467 - categorical_accuracy: 0.8802 - val_loss: 0.6211 - val_categorical_accuracy: 0.8219\n",
            "Epoch 840/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3588 - categorical_accuracy: 0.8831 - val_loss: 0.6243 - val_categorical_accuracy: 0.8082\n",
            "Epoch 841/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3629 - categorical_accuracy: 0.8743 - val_loss: 0.6190 - val_categorical_accuracy: 0.8082\n",
            "Epoch 842/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3721 - categorical_accuracy: 0.8772 - val_loss: 0.6209 - val_categorical_accuracy: 0.8219\n",
            "Epoch 843/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3554 - categorical_accuracy: 0.8809 - val_loss: 0.6192 - val_categorical_accuracy: 0.8219\n",
            "Epoch 844/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3676 - categorical_accuracy: 0.8765 - val_loss: 0.6208 - val_categorical_accuracy: 0.8219\n",
            "Epoch 845/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3423 - categorical_accuracy: 0.8964 - val_loss: 0.6221 - val_categorical_accuracy: 0.8082\n",
            "Epoch 846/1100\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3415 - categorical_accuracy: 0.8928 - val_loss: 0.6205 - val_categorical_accuracy: 0.8082\n",
            "Epoch 847/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3762 - categorical_accuracy: 0.8683 - val_loss: 0.6233 - val_categorical_accuracy: 0.8219\n",
            "Epoch 848/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3671 - categorical_accuracy: 0.8698 - val_loss: 0.6298 - val_categorical_accuracy: 0.8219\n",
            "Epoch 849/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3662 - categorical_accuracy: 0.8817 - val_loss: 0.6240 - val_categorical_accuracy: 0.8082\n",
            "Epoch 850/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3438 - categorical_accuracy: 0.8876 - val_loss: 0.6202 - val_categorical_accuracy: 0.8082\n",
            "Epoch 851/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3718 - categorical_accuracy: 0.8713 - val_loss: 0.6197 - val_categorical_accuracy: 0.8082\n",
            "Epoch 852/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3611 - categorical_accuracy: 0.8720 - val_loss: 0.6203 - val_categorical_accuracy: 0.8082\n",
            "Epoch 853/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3725 - categorical_accuracy: 0.8772 - val_loss: 0.6216 - val_categorical_accuracy: 0.8082\n",
            "Epoch 854/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3378 - categorical_accuracy: 0.8846 - val_loss: 0.6267 - val_categorical_accuracy: 0.7945\n",
            "Epoch 855/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3488 - categorical_accuracy: 0.8839 - val_loss: 0.6322 - val_categorical_accuracy: 0.8082\n",
            "Epoch 856/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3554 - categorical_accuracy: 0.8780 - val_loss: 0.6329 - val_categorical_accuracy: 0.8082\n",
            "Epoch 857/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3518 - categorical_accuracy: 0.8802 - val_loss: 0.6280 - val_categorical_accuracy: 0.8082\n",
            "Epoch 858/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3566 - categorical_accuracy: 0.8787 - val_loss: 0.6397 - val_categorical_accuracy: 0.8082\n",
            "Epoch 859/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3587 - categorical_accuracy: 0.8720 - val_loss: 0.6364 - val_categorical_accuracy: 0.8082\n",
            "Epoch 860/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3697 - categorical_accuracy: 0.8669 - val_loss: 0.6360 - val_categorical_accuracy: 0.8082\n",
            "Epoch 861/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3616 - categorical_accuracy: 0.8683 - val_loss: 0.6335 - val_categorical_accuracy: 0.8082\n",
            "Epoch 862/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3661 - categorical_accuracy: 0.8728 - val_loss: 0.6351 - val_categorical_accuracy: 0.8219\n",
            "Epoch 863/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3337 - categorical_accuracy: 0.8920 - val_loss: 0.6336 - val_categorical_accuracy: 0.8219\n",
            "Epoch 864/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3392 - categorical_accuracy: 0.8846 - val_loss: 0.6296 - val_categorical_accuracy: 0.8082\n",
            "Epoch 865/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3652 - categorical_accuracy: 0.8750 - val_loss: 0.6322 - val_categorical_accuracy: 0.8219\n",
            "Epoch 866/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3566 - categorical_accuracy: 0.8883 - val_loss: 0.6311 - val_categorical_accuracy: 0.8082\n",
            "Epoch 867/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3667 - categorical_accuracy: 0.8809 - val_loss: 0.6332 - val_categorical_accuracy: 0.8219\n",
            "Epoch 868/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3743 - categorical_accuracy: 0.8676 - val_loss: 0.6336 - val_categorical_accuracy: 0.8219\n",
            "Epoch 869/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3702 - categorical_accuracy: 0.8706 - val_loss: 0.6311 - val_categorical_accuracy: 0.8219\n",
            "Epoch 870/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3560 - categorical_accuracy: 0.8787 - val_loss: 0.6329 - val_categorical_accuracy: 0.8219\n",
            "Epoch 871/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3792 - categorical_accuracy: 0.8595 - val_loss: 0.6387 - val_categorical_accuracy: 0.8082\n",
            "Epoch 872/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3562 - categorical_accuracy: 0.8802 - val_loss: 0.6407 - val_categorical_accuracy: 0.8082\n",
            "Epoch 873/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3655 - categorical_accuracy: 0.8876 - val_loss: 0.6403 - val_categorical_accuracy: 0.8082\n",
            "Epoch 874/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3742 - categorical_accuracy: 0.8654 - val_loss: 0.6331 - val_categorical_accuracy: 0.8219\n",
            "Epoch 875/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3279 - categorical_accuracy: 0.8972 - val_loss: 0.6322 - val_categorical_accuracy: 0.8219\n",
            "Epoch 876/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3448 - categorical_accuracy: 0.8913 - val_loss: 0.6363 - val_categorical_accuracy: 0.8219\n",
            "Epoch 877/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3632 - categorical_accuracy: 0.8757 - val_loss: 0.6406 - val_categorical_accuracy: 0.8082\n",
            "Epoch 878/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3488 - categorical_accuracy: 0.8780 - val_loss: 0.6404 - val_categorical_accuracy: 0.8082\n",
            "Epoch 879/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3672 - categorical_accuracy: 0.8698 - val_loss: 0.6404 - val_categorical_accuracy: 0.8082\n",
            "Epoch 880/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3457 - categorical_accuracy: 0.8809 - val_loss: 0.6358 - val_categorical_accuracy: 0.8082\n",
            "Epoch 881/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3533 - categorical_accuracy: 0.8765 - val_loss: 0.6382 - val_categorical_accuracy: 0.8219\n",
            "Epoch 882/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3626 - categorical_accuracy: 0.8787 - val_loss: 0.6325 - val_categorical_accuracy: 0.8082\n",
            "Epoch 883/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3727 - categorical_accuracy: 0.8706 - val_loss: 0.6286 - val_categorical_accuracy: 0.7945\n",
            "Epoch 884/1100\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3184 - categorical_accuracy: 0.8928 - val_loss: 0.6285 - val_categorical_accuracy: 0.8082\n",
            "Epoch 885/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3775 - categorical_accuracy: 0.8706 - val_loss: 0.6337 - val_categorical_accuracy: 0.8082\n",
            "Epoch 886/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3603 - categorical_accuracy: 0.8780 - val_loss: 0.6323 - val_categorical_accuracy: 0.8082\n",
            "Epoch 887/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3102 - categorical_accuracy: 0.9001 - val_loss: 0.6369 - val_categorical_accuracy: 0.8082\n",
            "Epoch 888/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3433 - categorical_accuracy: 0.8780 - val_loss: 0.6357 - val_categorical_accuracy: 0.8082\n",
            "Epoch 889/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3409 - categorical_accuracy: 0.8839 - val_loss: 0.6290 - val_categorical_accuracy: 0.8082\n",
            "Epoch 890/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3148 - categorical_accuracy: 0.9031 - val_loss: 0.6279 - val_categorical_accuracy: 0.7945\n",
            "Epoch 891/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3488 - categorical_accuracy: 0.8883 - val_loss: 0.6269 - val_categorical_accuracy: 0.8082\n",
            "Epoch 892/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3596 - categorical_accuracy: 0.8898 - val_loss: 0.6308 - val_categorical_accuracy: 0.7945\n",
            "Epoch 893/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3767 - categorical_accuracy: 0.8706 - val_loss: 0.6322 - val_categorical_accuracy: 0.7945\n",
            "Epoch 894/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3316 - categorical_accuracy: 0.8876 - val_loss: 0.6276 - val_categorical_accuracy: 0.7945\n",
            "Epoch 895/1100\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.3471 - categorical_accuracy: 0.8817 - val_loss: 0.6237 - val_categorical_accuracy: 0.7945\n",
            "Epoch 896/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3488 - categorical_accuracy: 0.8831 - val_loss: 0.6250 - val_categorical_accuracy: 0.7945\n",
            "Epoch 897/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3452 - categorical_accuracy: 0.8794 - val_loss: 0.6234 - val_categorical_accuracy: 0.7945\n",
            "Epoch 898/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3653 - categorical_accuracy: 0.8698 - val_loss: 0.6243 - val_categorical_accuracy: 0.8082\n",
            "Epoch 899/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3305 - categorical_accuracy: 0.9016 - val_loss: 0.6289 - val_categorical_accuracy: 0.8219\n",
            "Epoch 900/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3900 - categorical_accuracy: 0.8617 - val_loss: 0.6296 - val_categorical_accuracy: 0.8082\n",
            "Epoch 901/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3496 - categorical_accuracy: 0.8831 - val_loss: 0.6310 - val_categorical_accuracy: 0.8219\n",
            "Epoch 902/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3591 - categorical_accuracy: 0.8720 - val_loss: 0.6312 - val_categorical_accuracy: 0.8082\n",
            "Epoch 903/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3283 - categorical_accuracy: 0.8817 - val_loss: 0.6346 - val_categorical_accuracy: 0.8082\n",
            "Epoch 904/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3396 - categorical_accuracy: 0.8780 - val_loss: 0.6367 - val_categorical_accuracy: 0.8082\n",
            "Epoch 905/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3519 - categorical_accuracy: 0.8809 - val_loss: 0.6383 - val_categorical_accuracy: 0.8082\n",
            "Epoch 906/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3447 - categorical_accuracy: 0.8957 - val_loss: 0.6389 - val_categorical_accuracy: 0.8082\n",
            "Epoch 907/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3373 - categorical_accuracy: 0.8750 - val_loss: 0.6400 - val_categorical_accuracy: 0.8082\n",
            "Epoch 908/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3343 - categorical_accuracy: 0.8876 - val_loss: 0.6394 - val_categorical_accuracy: 0.8082\n",
            "Epoch 909/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3450 - categorical_accuracy: 0.8780 - val_loss: 0.6382 - val_categorical_accuracy: 0.8082\n",
            "Epoch 910/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3482 - categorical_accuracy: 0.8780 - val_loss: 0.6319 - val_categorical_accuracy: 0.8219\n",
            "Epoch 911/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3524 - categorical_accuracy: 0.8794 - val_loss: 0.6359 - val_categorical_accuracy: 0.8082\n",
            "Epoch 912/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3133 - categorical_accuracy: 0.8950 - val_loss: 0.6376 - val_categorical_accuracy: 0.8082\n",
            "Epoch 913/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3212 - categorical_accuracy: 0.8928 - val_loss: 0.6334 - val_categorical_accuracy: 0.8219\n",
            "Epoch 914/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3394 - categorical_accuracy: 0.8817 - val_loss: 0.6320 - val_categorical_accuracy: 0.8219\n",
            "Epoch 915/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3616 - categorical_accuracy: 0.8802 - val_loss: 0.6301 - val_categorical_accuracy: 0.8082\n",
            "Epoch 916/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3476 - categorical_accuracy: 0.8824 - val_loss: 0.6288 - val_categorical_accuracy: 0.8082\n",
            "Epoch 917/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3476 - categorical_accuracy: 0.8824 - val_loss: 0.6259 - val_categorical_accuracy: 0.8082\n",
            "Epoch 918/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3476 - categorical_accuracy: 0.8787 - val_loss: 0.6257 - val_categorical_accuracy: 0.8082\n",
            "Epoch 919/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3245 - categorical_accuracy: 0.8935 - val_loss: 0.6270 - val_categorical_accuracy: 0.8219\n",
            "Epoch 920/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3559 - categorical_accuracy: 0.8839 - val_loss: 0.6271 - val_categorical_accuracy: 0.8219\n",
            "Epoch 921/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3438 - categorical_accuracy: 0.8787 - val_loss: 0.6287 - val_categorical_accuracy: 0.8219\n",
            "Epoch 922/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3416 - categorical_accuracy: 0.8846 - val_loss: 0.6241 - val_categorical_accuracy: 0.8356\n",
            "Epoch 923/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3328 - categorical_accuracy: 0.8891 - val_loss: 0.6189 - val_categorical_accuracy: 0.8356\n",
            "Epoch 924/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3207 - categorical_accuracy: 0.8883 - val_loss: 0.6186 - val_categorical_accuracy: 0.8219\n",
            "Epoch 925/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3395 - categorical_accuracy: 0.8876 - val_loss: 0.6211 - val_categorical_accuracy: 0.8219\n",
            "Epoch 926/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3586 - categorical_accuracy: 0.8831 - val_loss: 0.6202 - val_categorical_accuracy: 0.8082\n",
            "Epoch 927/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3392 - categorical_accuracy: 0.8846 - val_loss: 0.6236 - val_categorical_accuracy: 0.8082\n",
            "Epoch 928/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3239 - categorical_accuracy: 0.8972 - val_loss: 0.6242 - val_categorical_accuracy: 0.8082\n",
            "Epoch 929/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3486 - categorical_accuracy: 0.8765 - val_loss: 0.6226 - val_categorical_accuracy: 0.8082\n",
            "Epoch 930/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3651 - categorical_accuracy: 0.8787 - val_loss: 0.6217 - val_categorical_accuracy: 0.8082\n",
            "Epoch 931/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3443 - categorical_accuracy: 0.8780 - val_loss: 0.6184 - val_categorical_accuracy: 0.8082\n",
            "Epoch 932/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3559 - categorical_accuracy: 0.8780 - val_loss: 0.6209 - val_categorical_accuracy: 0.8082\n",
            "Epoch 933/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3472 - categorical_accuracy: 0.8824 - val_loss: 0.6166 - val_categorical_accuracy: 0.8082\n",
            "Epoch 934/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3541 - categorical_accuracy: 0.8824 - val_loss: 0.6204 - val_categorical_accuracy: 0.8082\n",
            "Epoch 935/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3485 - categorical_accuracy: 0.8780 - val_loss: 0.6150 - val_categorical_accuracy: 0.7945\n",
            "Epoch 936/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3234 - categorical_accuracy: 0.8861 - val_loss: 0.6173 - val_categorical_accuracy: 0.8082\n",
            "Epoch 937/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3568 - categorical_accuracy: 0.8743 - val_loss: 0.6108 - val_categorical_accuracy: 0.8082\n",
            "Epoch 938/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3281 - categorical_accuracy: 0.8987 - val_loss: 0.6141 - val_categorical_accuracy: 0.8082\n",
            "Epoch 939/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3333 - categorical_accuracy: 0.8935 - val_loss: 0.6175 - val_categorical_accuracy: 0.8219\n",
            "Epoch 940/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3467 - categorical_accuracy: 0.8809 - val_loss: 0.6202 - val_categorical_accuracy: 0.8219\n",
            "Epoch 941/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3499 - categorical_accuracy: 0.8735 - val_loss: 0.6207 - val_categorical_accuracy: 0.8219\n",
            "Epoch 942/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3360 - categorical_accuracy: 0.8802 - val_loss: 0.6159 - val_categorical_accuracy: 0.8219\n",
            "Epoch 943/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3127 - categorical_accuracy: 0.8935 - val_loss: 0.6186 - val_categorical_accuracy: 0.8082\n",
            "Epoch 944/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3603 - categorical_accuracy: 0.8846 - val_loss: 0.6186 - val_categorical_accuracy: 0.8219\n",
            "Epoch 945/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3756 - categorical_accuracy: 0.8676 - val_loss: 0.6198 - val_categorical_accuracy: 0.8219\n",
            "Epoch 946/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3189 - categorical_accuracy: 0.8950 - val_loss: 0.6245 - val_categorical_accuracy: 0.8219\n",
            "Epoch 947/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3542 - categorical_accuracy: 0.8765 - val_loss: 0.6282 - val_categorical_accuracy: 0.8082\n",
            "Epoch 948/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3571 - categorical_accuracy: 0.8831 - val_loss: 0.6313 - val_categorical_accuracy: 0.8219\n",
            "Epoch 949/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3386 - categorical_accuracy: 0.8846 - val_loss: 0.6243 - val_categorical_accuracy: 0.8219\n",
            "Epoch 950/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3342 - categorical_accuracy: 0.8854 - val_loss: 0.6221 - val_categorical_accuracy: 0.8356\n",
            "Epoch 951/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3249 - categorical_accuracy: 0.8891 - val_loss: 0.6212 - val_categorical_accuracy: 0.8356\n",
            "Epoch 952/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3487 - categorical_accuracy: 0.8772 - val_loss: 0.6206 - val_categorical_accuracy: 0.8219\n",
            "Epoch 953/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3293 - categorical_accuracy: 0.8854 - val_loss: 0.6204 - val_categorical_accuracy: 0.8219\n",
            "Epoch 954/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3335 - categorical_accuracy: 0.8809 - val_loss: 0.6217 - val_categorical_accuracy: 0.8356\n",
            "Epoch 955/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3354 - categorical_accuracy: 0.8839 - val_loss: 0.6228 - val_categorical_accuracy: 0.8082\n",
            "Epoch 956/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3260 - categorical_accuracy: 0.8942 - val_loss: 0.6260 - val_categorical_accuracy: 0.7945\n",
            "Epoch 957/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3379 - categorical_accuracy: 0.8876 - val_loss: 0.6271 - val_categorical_accuracy: 0.7945\n",
            "Epoch 958/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3383 - categorical_accuracy: 0.8794 - val_loss: 0.6276 - val_categorical_accuracy: 0.7945\n",
            "Epoch 959/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3183 - categorical_accuracy: 0.8972 - val_loss: 0.6269 - val_categorical_accuracy: 0.7945\n",
            "Epoch 960/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3334 - categorical_accuracy: 0.8883 - val_loss: 0.6270 - val_categorical_accuracy: 0.7945\n",
            "Epoch 961/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3489 - categorical_accuracy: 0.8735 - val_loss: 0.6256 - val_categorical_accuracy: 0.8082\n",
            "Epoch 962/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3348 - categorical_accuracy: 0.8905 - val_loss: 0.6256 - val_categorical_accuracy: 0.8082\n",
            "Epoch 963/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3436 - categorical_accuracy: 0.8891 - val_loss: 0.6266 - val_categorical_accuracy: 0.8219\n",
            "Epoch 964/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3356 - categorical_accuracy: 0.8824 - val_loss: 0.6239 - val_categorical_accuracy: 0.8219\n",
            "Epoch 965/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3388 - categorical_accuracy: 0.8876 - val_loss: 0.6266 - val_categorical_accuracy: 0.8219\n",
            "Epoch 966/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3311 - categorical_accuracy: 0.8861 - val_loss: 0.6297 - val_categorical_accuracy: 0.8219\n",
            "Epoch 967/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3545 - categorical_accuracy: 0.8794 - val_loss: 0.6356 - val_categorical_accuracy: 0.8082\n",
            "Epoch 968/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3356 - categorical_accuracy: 0.8787 - val_loss: 0.6286 - val_categorical_accuracy: 0.8219\n",
            "Epoch 969/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3636 - categorical_accuracy: 0.8743 - val_loss: 0.6270 - val_categorical_accuracy: 0.8219\n",
            "Epoch 970/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3327 - categorical_accuracy: 0.8950 - val_loss: 0.6249 - val_categorical_accuracy: 0.8082\n",
            "Epoch 971/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3422 - categorical_accuracy: 0.8846 - val_loss: 0.6277 - val_categorical_accuracy: 0.8082\n",
            "Epoch 972/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3272 - categorical_accuracy: 0.8854 - val_loss: 0.6266 - val_categorical_accuracy: 0.7945\n",
            "Epoch 973/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3678 - categorical_accuracy: 0.8787 - val_loss: 0.6209 - val_categorical_accuracy: 0.8219\n",
            "Epoch 974/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3397 - categorical_accuracy: 0.8831 - val_loss: 0.6233 - val_categorical_accuracy: 0.8082\n",
            "Epoch 975/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3315 - categorical_accuracy: 0.8928 - val_loss: 0.6268 - val_categorical_accuracy: 0.8082\n",
            "Epoch 976/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3201 - categorical_accuracy: 0.8913 - val_loss: 0.6274 - val_categorical_accuracy: 0.8082\n",
            "Epoch 977/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3348 - categorical_accuracy: 0.8846 - val_loss: 0.6345 - val_categorical_accuracy: 0.8082\n",
            "Epoch 978/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3403 - categorical_accuracy: 0.8839 - val_loss: 0.6365 - val_categorical_accuracy: 0.8082\n",
            "Epoch 979/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3353 - categorical_accuracy: 0.8854 - val_loss: 0.6346 - val_categorical_accuracy: 0.8082\n",
            "Epoch 980/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3507 - categorical_accuracy: 0.8780 - val_loss: 0.6377 - val_categorical_accuracy: 0.8082\n",
            "Epoch 981/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3400 - categorical_accuracy: 0.8809 - val_loss: 0.6386 - val_categorical_accuracy: 0.8082\n",
            "Epoch 982/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3537 - categorical_accuracy: 0.8750 - val_loss: 0.6354 - val_categorical_accuracy: 0.8082\n",
            "Epoch 983/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3391 - categorical_accuracy: 0.8831 - val_loss: 0.6376 - val_categorical_accuracy: 0.7945\n",
            "Epoch 984/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3570 - categorical_accuracy: 0.8839 - val_loss: 0.6381 - val_categorical_accuracy: 0.8082\n",
            "Epoch 985/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3492 - categorical_accuracy: 0.8787 - val_loss: 0.6346 - val_categorical_accuracy: 0.8082\n",
            "Epoch 986/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3471 - categorical_accuracy: 0.8913 - val_loss: 0.6332 - val_categorical_accuracy: 0.8082\n",
            "Epoch 987/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3484 - categorical_accuracy: 0.8817 - val_loss: 0.6320 - val_categorical_accuracy: 0.8082\n",
            "Epoch 988/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3201 - categorical_accuracy: 0.8891 - val_loss: 0.6280 - val_categorical_accuracy: 0.8082\n",
            "Epoch 989/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3515 - categorical_accuracy: 0.8802 - val_loss: 0.6242 - val_categorical_accuracy: 0.8219\n",
            "Epoch 990/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3490 - categorical_accuracy: 0.8757 - val_loss: 0.6231 - val_categorical_accuracy: 0.8219\n",
            "Epoch 991/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3332 - categorical_accuracy: 0.8883 - val_loss: 0.6170 - val_categorical_accuracy: 0.8219\n",
            "Epoch 992/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3246 - categorical_accuracy: 0.8868 - val_loss: 0.6202 - val_categorical_accuracy: 0.8082\n",
            "Epoch 993/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3363 - categorical_accuracy: 0.8854 - val_loss: 0.6221 - val_categorical_accuracy: 0.8219\n",
            "Epoch 994/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3297 - categorical_accuracy: 0.8920 - val_loss: 0.6206 - val_categorical_accuracy: 0.8219\n",
            "Epoch 995/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3215 - categorical_accuracy: 0.8950 - val_loss: 0.6221 - val_categorical_accuracy: 0.8219\n",
            "Epoch 996/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3266 - categorical_accuracy: 0.8972 - val_loss: 0.6218 - val_categorical_accuracy: 0.8219\n",
            "Epoch 997/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3387 - categorical_accuracy: 0.8854 - val_loss: 0.6233 - val_categorical_accuracy: 0.8082\n",
            "Epoch 998/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3308 - categorical_accuracy: 0.8935 - val_loss: 0.6221 - val_categorical_accuracy: 0.8219\n",
            "Epoch 999/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3422 - categorical_accuracy: 0.8854 - val_loss: 0.6239 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1000/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3278 - categorical_accuracy: 0.8964 - val_loss: 0.6209 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1001/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3346 - categorical_accuracy: 0.8794 - val_loss: 0.6223 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1002/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3396 - categorical_accuracy: 0.8846 - val_loss: 0.6200 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1003/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3089 - categorical_accuracy: 0.8846 - val_loss: 0.6156 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1004/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3462 - categorical_accuracy: 0.8809 - val_loss: 0.6161 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1005/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3356 - categorical_accuracy: 0.8905 - val_loss: 0.6152 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1006/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3433 - categorical_accuracy: 0.8780 - val_loss: 0.6176 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1007/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3438 - categorical_accuracy: 0.8817 - val_loss: 0.6188 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1008/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3082 - categorical_accuracy: 0.8994 - val_loss: 0.6218 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1009/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3409 - categorical_accuracy: 0.8802 - val_loss: 0.6215 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1010/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3186 - categorical_accuracy: 0.8935 - val_loss: 0.6205 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1011/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3173 - categorical_accuracy: 0.8891 - val_loss: 0.6174 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1012/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3118 - categorical_accuracy: 0.8920 - val_loss: 0.6191 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1013/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3380 - categorical_accuracy: 0.8876 - val_loss: 0.6183 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1014/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3208 - categorical_accuracy: 0.8935 - val_loss: 0.6177 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1015/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3452 - categorical_accuracy: 0.8765 - val_loss: 0.6208 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1016/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3330 - categorical_accuracy: 0.8898 - val_loss: 0.6235 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1017/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3551 - categorical_accuracy: 0.8772 - val_loss: 0.6230 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1018/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3377 - categorical_accuracy: 0.8876 - val_loss: 0.6192 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1019/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3302 - categorical_accuracy: 0.8942 - val_loss: 0.6195 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1020/1100\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.3498 - categorical_accuracy: 0.8876 - val_loss: 0.6187 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1021/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3143 - categorical_accuracy: 0.8972 - val_loss: 0.6185 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1022/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3424 - categorical_accuracy: 0.8831 - val_loss: 0.6215 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1023/1100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.3248 - categorical_accuracy: 0.8935 - val_loss: 0.6184 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1024/1100\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.3322 - categorical_accuracy: 0.8839 - val_loss: 0.6173 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1025/1100\n",
            "22/22 [==============================] - 1s 34ms/step - loss: 0.3487 - categorical_accuracy: 0.8809 - val_loss: 0.6176 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1026/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3371 - categorical_accuracy: 0.8794 - val_loss: 0.6175 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1027/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3326 - categorical_accuracy: 0.8802 - val_loss: 0.6166 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1028/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3263 - categorical_accuracy: 0.8928 - val_loss: 0.6164 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1029/1100\n",
            "22/22 [==============================] - 1s 35ms/step - loss: 0.3153 - categorical_accuracy: 0.8928 - val_loss: 0.6225 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1030/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3611 - categorical_accuracy: 0.8735 - val_loss: 0.6238 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1031/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3227 - categorical_accuracy: 0.8868 - val_loss: 0.6267 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1032/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3498 - categorical_accuracy: 0.8839 - val_loss: 0.6288 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1033/1100\n",
            "22/22 [==============================] - 1s 33ms/step - loss: 0.3361 - categorical_accuracy: 0.8802 - val_loss: 0.6311 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1034/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3406 - categorical_accuracy: 0.8831 - val_loss: 0.6359 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1035/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3232 - categorical_accuracy: 0.8868 - val_loss: 0.6402 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1036/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3137 - categorical_accuracy: 0.8950 - val_loss: 0.6381 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1037/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3610 - categorical_accuracy: 0.8765 - val_loss: 0.6372 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1038/1100\n",
            "22/22 [==============================] - 1s 33ms/step - loss: 0.3299 - categorical_accuracy: 0.8846 - val_loss: 0.6413 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1039/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3504 - categorical_accuracy: 0.8898 - val_loss: 0.6398 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1040/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3190 - categorical_accuracy: 0.8979 - val_loss: 0.6411 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1041/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3273 - categorical_accuracy: 0.8846 - val_loss: 0.6413 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1042/1100\n",
            "22/22 [==============================] - 1s 33ms/step - loss: 0.3183 - categorical_accuracy: 0.8935 - val_loss: 0.6382 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1043/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3112 - categorical_accuracy: 0.8964 - val_loss: 0.6392 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1044/1100\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.2879 - categorical_accuracy: 0.9068 - val_loss: 0.6402 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1045/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3230 - categorical_accuracy: 0.8957 - val_loss: 0.6403 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1046/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3285 - categorical_accuracy: 0.8883 - val_loss: 0.6395 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1047/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3330 - categorical_accuracy: 0.8809 - val_loss: 0.6402 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1048/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3286 - categorical_accuracy: 0.8883 - val_loss: 0.6414 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1049/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3324 - categorical_accuracy: 0.8891 - val_loss: 0.6365 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1050/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3078 - categorical_accuracy: 0.8972 - val_loss: 0.6338 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1051/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3244 - categorical_accuracy: 0.8765 - val_loss: 0.6337 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1052/1100\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.3290 - categorical_accuracy: 0.8898 - val_loss: 0.6378 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1053/1100\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3310 - categorical_accuracy: 0.8854 - val_loss: 0.6410 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1054/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.2967 - categorical_accuracy: 0.9038 - val_loss: 0.6421 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1055/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3381 - categorical_accuracy: 0.8854 - val_loss: 0.6409 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1056/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3239 - categorical_accuracy: 0.8928 - val_loss: 0.6431 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1057/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3430 - categorical_accuracy: 0.8765 - val_loss: 0.6444 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1058/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3423 - categorical_accuracy: 0.8861 - val_loss: 0.6459 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1059/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3378 - categorical_accuracy: 0.8817 - val_loss: 0.6397 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1060/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3174 - categorical_accuracy: 0.8935 - val_loss: 0.6392 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1061/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3277 - categorical_accuracy: 0.8817 - val_loss: 0.6343 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1062/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3293 - categorical_accuracy: 0.8846 - val_loss: 0.6316 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1063/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3355 - categorical_accuracy: 0.8891 - val_loss: 0.6311 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1064/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3155 - categorical_accuracy: 0.9024 - val_loss: 0.6344 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1065/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3293 - categorical_accuracy: 0.8920 - val_loss: 0.6314 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1066/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3464 - categorical_accuracy: 0.8743 - val_loss: 0.6272 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1067/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3320 - categorical_accuracy: 0.8802 - val_loss: 0.6237 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1068/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3150 - categorical_accuracy: 0.8942 - val_loss: 0.6276 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1069/1100\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3341 - categorical_accuracy: 0.8876 - val_loss: 0.6320 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1070/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3318 - categorical_accuracy: 0.8957 - val_loss: 0.6312 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1071/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3329 - categorical_accuracy: 0.8846 - val_loss: 0.6319 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1072/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3212 - categorical_accuracy: 0.8898 - val_loss: 0.6307 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1073/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3079 - categorical_accuracy: 0.8964 - val_loss: 0.6297 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1074/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3355 - categorical_accuracy: 0.8868 - val_loss: 0.6282 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1075/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.2904 - categorical_accuracy: 0.9038 - val_loss: 0.6311 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1076/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3220 - categorical_accuracy: 0.8883 - val_loss: 0.6336 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1077/1100\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.3301 - categorical_accuracy: 0.8846 - val_loss: 0.6316 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1078/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3103 - categorical_accuracy: 0.8994 - val_loss: 0.6232 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1079/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3415 - categorical_accuracy: 0.8817 - val_loss: 0.6252 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1080/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3436 - categorical_accuracy: 0.8846 - val_loss: 0.6259 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1081/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3100 - categorical_accuracy: 0.8935 - val_loss: 0.6247 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1082/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3240 - categorical_accuracy: 0.8972 - val_loss: 0.6256 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1083/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3357 - categorical_accuracy: 0.8824 - val_loss: 0.6301 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1084/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3402 - categorical_accuracy: 0.8824 - val_loss: 0.6327 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1085/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3014 - categorical_accuracy: 0.9053 - val_loss: 0.6329 - val_categorical_accuracy: 0.7945\n",
            "Epoch 1086/1100\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.3353 - categorical_accuracy: 0.8794 - val_loss: 0.6303 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1087/1100\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3191 - categorical_accuracy: 0.9031 - val_loss: 0.6263 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1088/1100\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3517 - categorical_accuracy: 0.8824 - val_loss: 0.6277 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1089/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3187 - categorical_accuracy: 0.8898 - val_loss: 0.6264 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1090/1100\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 0.3430 - categorical_accuracy: 0.8802 - val_loss: 0.6305 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1091/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3188 - categorical_accuracy: 0.8905 - val_loss: 0.6303 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1092/1100\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.3084 - categorical_accuracy: 0.8964 - val_loss: 0.6315 - val_categorical_accuracy: 0.7808\n",
            "Epoch 1093/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3120 - categorical_accuracy: 0.8950 - val_loss: 0.6269 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1094/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3167 - categorical_accuracy: 0.8898 - val_loss: 0.6233 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1095/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3369 - categorical_accuracy: 0.8854 - val_loss: 0.6264 - val_categorical_accuracy: 0.8082\n",
            "Epoch 1096/1100\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 0.3304 - categorical_accuracy: 0.8883 - val_loss: 0.6252 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1097/1100\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3416 - categorical_accuracy: 0.8876 - val_loss: 0.6259 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1098/1100\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3399 - categorical_accuracy: 0.8809 - val_loss: 0.6262 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1099/1100\n",
            "22/22 [==============================] - 1s 31ms/step - loss: 0.3071 - categorical_accuracy: 0.8957 - val_loss: 0.6248 - val_categorical_accuracy: 0.8219\n",
            "Epoch 1100/1100\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3373 - categorical_accuracy: 0.8802 - val_loss: 0.6247 - val_categorical_accuracy: 0.8219\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "history = model.fit(X_train, Y_train, batch_size=64, epochs=1100, validation_data=(X_val, Y_val))\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "_nlY2imOcDAw",
        "outputId": "ac109aea-5e1c-45e2-ed8a-2d54498108f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-126-d10d0569b26a>:2: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"414.46375pt\" height=\"325.986375pt\" viewBox=\"0 0 414.46375 325.986375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-12-17T18:42:22.884272</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 325.986375 \nL 414.46375 325.986375 \nL 414.46375 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 288.430125 \nL 407.26375 288.430125 \nL 407.26375 22.318125 \nL 50.14375 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"mfae1c69675\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mfae1c69675\" x=\"66.376477\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(63.195227 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#mfae1c69675\" x=\"125.458287\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(115.914537 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#mfae1c69675\" x=\"184.540097\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(174.996347 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mfae1c69675\" x=\"243.621907\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(234.078157 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#mfae1c69675\" x=\"302.703717\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(293.159967 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mfae1c69675\" x=\"361.785527\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <g transform=\"translate(349.060527 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(213.475625 316.706687) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m167f3f4662\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"281.43588\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 285.235099) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"247.74783\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.50 -->\n      <g transform=\"translate(20.878125 251.547049) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"214.05978\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.75 -->\n      <g transform=\"translate(20.878125 217.858999) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"180.37173\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.00 -->\n      <g transform=\"translate(20.878125 184.170949) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"146.68368\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.25 -->\n      <g transform=\"translate(20.878125 150.482899) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"112.99563\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.50 -->\n      <g transform=\"translate(20.878125 116.794848) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"79.30758\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.75 -->\n      <g transform=\"translate(20.878125 83.106798) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m167f3f4662\" x=\"50.14375\" y=\"45.619529\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 2.00 -->\n      <g transform=\"translate(20.878125 49.418748) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 165.031937) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 66.376477 34.414125 \nL 66.671886 44.479327 \nL 67.558113 60.074639 \nL 68.73975 75.207268 \nL 69.035159 81.536838 \nL 69.330568 82.235031 \nL 69.625977 88.606639 \nL 69.921386 90.333054 \nL 70.216795 92.937239 \nL 70.807613 101.161331 \nL 71.103022 103.788069 \nL 71.398431 104.37711 \nL 71.69384 107.769075 \nL 71.989249 109.107279 \nL 72.284658 112.41997 \nL 73.170885 118.480745 \nL 73.466294 115.545535 \nL 74.057113 126.11162 \nL 74.352522 124.402569 \nL 74.94334 128.825295 \nL 75.238749 129.852988 \nL 75.534158 131.836615 \nL 75.829567 136.92817 \nL 76.124976 130.415057 \nL 76.420385 134.900688 \nL 76.715794 136.045855 \nL 77.011203 137.983588 \nL 77.306612 136.264658 \nL 77.602021 141.561459 \nL 77.89743 137.881793 \nL 78.488248 143.78328 \nL 78.783657 143.698945 \nL 79.079066 145.245945 \nL 79.669885 153.099238 \nL 79.965294 151.685021 \nL 80.260703 148.180191 \nL 80.851521 154.625853 \nL 81.14693 151.702835 \nL 81.442339 155.011189 \nL 81.737748 154.732404 \nL 82.033157 157.377808 \nL 82.919384 154.26103 \nL 83.214793 155.89985 \nL 83.510202 161.105377 \nL 83.805611 158.945836 \nL 84.10102 159.158118 \nL 84.396429 164.748386 \nL 84.691838 167.186119 \nL 84.987247 161.089378 \nL 85.282656 167.483009 \nL 85.578065 161.928642 \nL 85.873475 163.152922 \nL 86.168884 170.244683 \nL 86.464293 166.76435 \nL 86.759702 169.925481 \nL 87.055111 167.388329 \nL 87.35052 170.614518 \nL 87.645929 166.636435 \nL 87.941338 170.656428 \nL 88.236747 170.187014 \nL 88.532156 174.532586 \nL 88.827565 170.596863 \nL 89.122974 171.240745 \nL 89.418383 179.831459 \nL 89.713792 173.929972 \nL 90.009201 176.051972 \nL 90.30461 175.168259 \nL 90.600019 173.727922 \nL 90.895428 175.070769 \nL 91.190837 177.869919 \nL 91.486246 176.516422 \nL 91.781656 178.139468 \nL 92.077065 178.710051 \nL 92.372474 180.510914 \nL 92.667883 184.100198 \nL 92.963292 181.43661 \nL 93.258701 183.591476 \nL 93.55411 181.056847 \nL 93.849519 180.118421 \nL 94.144928 185.111449 \nL 94.440337 179.420999 \nL 94.735746 188.198879 \nL 95.031155 180.660419 \nL 95.326564 185.412074 \nL 95.621973 187.286886 \nL 95.917382 185.63291 \nL 96.212791 186.01933 \nL 96.5082 185.418555 \nL 96.803609 188.2667 \nL 97.099018 186.340653 \nL 97.394427 190.364284 \nL 97.689837 186.51096 \nL 97.985246 190.376131 \nL 98.280655 185.185752 \nL 98.576064 192.786379 \nL 98.871473 189.039774 \nL 99.166882 190.367055 \nL 99.462291 190.764792 \nL 99.7577 190.276994 \nL 100.053109 193.334513 \nL 100.348518 192.364553 \nL 100.643927 195.946938 \nL 101.234745 192.159123 \nL 101.530154 195.415752 \nL 101.825563 196.315464 \nL 102.120972 196.226294 \nL 102.416381 195.704384 \nL 102.71179 196.786203 \nL 103.007199 193.798561 \nL 103.302608 198.104608 \nL 103.598018 197.804008 \nL 103.893427 195.901823 \nL 104.188836 199.04734 \nL 104.484245 198.892614 \nL 104.779654 199.623625 \nL 105.075063 197.162335 \nL 105.370472 203.891931 \nL 105.665881 198.595957 \nL 105.96129 198.384976 \nL 106.256699 204.587546 \nL 106.552108 201.955555 \nL 106.847517 201.083737 \nL 107.142926 203.84925 \nL 107.438335 200.459051 \nL 108.029153 203.383651 \nL 108.324562 199.302182 \nL 108.619971 202.202117 \nL 108.91538 203.468684 \nL 109.210789 207.625604 \nL 109.506199 207.976829 \nL 109.801608 203.00017 \nL 110.097017 208.129474 \nL 110.392426 205.738005 \nL 110.687835 200.3574 \nL 110.983244 206.602771 \nL 111.574062 208.231664 \nL 111.869471 206.594924 \nL 112.16488 207.885852 \nL 112.460289 207.968331 \nL 112.755698 211.386473 \nL 113.051107 207.439691 \nL 113.346516 207.28629 \nL 113.641925 205.948962 \nL 113.937334 213.938081 \nL 114.232743 204.386444 \nL 114.528152 207.34775 \nL 114.823561 207.325261 \nL 115.11897 207.60387 \nL 115.414379 215.663107 \nL 115.709789 210.995418 \nL 116.005198 214.175447 \nL 116.300607 213.540545 \nL 116.596016 210.173084 \nL 116.891425 216.728196 \nL 117.186834 213.79309 \nL 117.482243 214.597376 \nL 117.777652 209.319466 \nL 118.073061 210.05545 \nL 118.36847 213.663954 \nL 118.959288 212.136945 \nL 119.254697 215.378314 \nL 119.550106 216.666013 \nL 119.845515 213.724514 \nL 120.140924 217.418044 \nL 120.436333 217.152262 \nL 120.731742 214.118726 \nL 121.027151 218.354077 \nL 121.32256 214.040504 \nL 121.61797 213.181496 \nL 121.913379 215.316356 \nL 122.208788 213.980634 \nL 122.504197 218.775886 \nL 122.799606 214.881825 \nL 123.095015 214.308832 \nL 123.390424 221.508155 \nL 123.685833 216.578434 \nL 123.981242 217.322441 \nL 124.276651 220.63016 \nL 124.57206 218.090262 \nL 124.867469 219.192354 \nL 125.162878 221.421748 \nL 125.753696 219.566847 \nL 126.049105 224.308341 \nL 126.344514 219.384982 \nL 126.639923 219.106774 \nL 126.935332 220.879309 \nL 127.230741 221.0214 \nL 127.526151 224.449268 \nL 127.82156 220.543264 \nL 128.116969 220.989763 \nL 128.412378 217.343067 \nL 128.707787 222.184823 \nL 129.003196 224.162828 \nL 129.298605 222.297253 \nL 130.184832 220.747787 \nL 130.480241 224.823986 \nL 130.77565 223.954088 \nL 131.071059 223.887986 \nL 131.366468 221.194527 \nL 131.661877 222.339356 \nL 131.957286 225.573833 \nL 132.252695 224.572035 \nL 132.548104 224.6478 \nL 133.138922 222.644116 \nL 133.434332 223.594124 \nL 133.729741 224.077457 \nL 134.320559 225.753152 \nL 134.615968 223.204563 \nL 134.911377 225.264148 \nL 135.206786 225.367534 \nL 135.502195 227.755815 \nL 135.797604 222.711729 \nL 136.093013 228.065556 \nL 136.388422 224.997683 \nL 136.683831 225.590611 \nL 136.97924 227.432428 \nL 137.274649 228.24857 \nL 137.570058 230.161091 \nL 137.865467 224.008705 \nL 138.160876 231.372191 \nL 138.456285 226.099172 \nL 138.751694 229.539377 \nL 139.047103 228.392998 \nL 139.342513 232.886124 \nL 139.637922 228.168018 \nL 139.933331 232.90344 \nL 140.22874 232.007679 \nL 140.524149 224.251323 \nL 140.819558 230.744726 \nL 141.114967 228.723485 \nL 141.410376 232.125716 \nL 141.705785 229.742415 \nL 142.001194 230.24659 \nL 142.296603 230.9912 \nL 142.592012 232.589812 \nL 142.887421 232.782392 \nL 143.18283 227.688404 \nL 143.478239 231.019898 \nL 143.773648 229.529105 \nL 144.069057 231.253592 \nL 144.364466 228.730007 \nL 144.659875 237.584238 \nL 144.955284 229.844467 \nL 145.250694 229.187903 \nL 145.546103 237.02163 \nL 145.841512 234.034189 \nL 146.136921 232.617908 \nL 146.43233 232.731599 \nL 146.727739 235.095037 \nL 147.023148 232.219953 \nL 147.318557 231.55 \nL 147.613966 235.224631 \nL 147.909375 235.636995 \nL 148.204784 232.519646 \nL 148.500193 233.634363 \nL 148.795602 237.77028 \nL 149.091011 234.919236 \nL 149.38642 238.119577 \nL 150.272647 231.716468 \nL 150.568056 236.072112 \nL 150.863465 235.771496 \nL 151.158874 236.525527 \nL 151.454284 229.517169 \nL 151.749693 236.613813 \nL 152.045102 229.323634 \nL 152.340511 237.064713 \nL 152.63592 235.901676 \nL 152.931329 237.412798 \nL 153.226738 237.53028 \nL 153.522147 235.476357 \nL 153.817556 238.966231 \nL 154.112965 232.886493 \nL 154.408374 239.976478 \nL 154.703783 239.231475 \nL 154.999192 237.044698 \nL 155.294601 236.073534 \nL 155.59001 240.080234 \nL 155.885419 239.643799 \nL 156.180828 235.039786 \nL 156.476237 234.214994 \nL 156.771646 235.610441 \nL 157.067055 234.515105 \nL 157.362465 238.879174 \nL 157.657874 237.323508 \nL 157.953283 238.395761 \nL 158.248692 238.763242 \nL 158.544101 236.392656 \nL 158.83951 235.225338 \nL 159.134919 239.317577 \nL 159.430328 238.907221 \nL 160.021146 241.804401 \nL 160.316555 239.056084 \nL 160.611964 237.952892 \nL 160.907373 238.341931 \nL 161.202782 237.434869 \nL 161.498191 243.352854 \nL 161.7936 242.02461 \nL 162.089009 239.772148 \nL 162.384418 239.064999 \nL 162.679827 239.857992 \nL 162.975236 239.89469 \nL 163.270646 239.729394 \nL 163.566055 242.552014 \nL 163.861464 241.555558 \nL 164.156873 241.27184 \nL 164.452282 238.147094 \nL 165.0431 241.987671 \nL 165.338509 240.018276 \nL 165.633918 244.13376 \nL 165.929327 240.34 \nL 166.224736 240.859838 \nL 166.520145 240.803736 \nL 166.815554 243.500777 \nL 167.110963 240.248309 \nL 167.406372 241.114135 \nL 167.701781 241.292289 \nL 167.99719 236.758523 \nL 168.292599 244.361551 \nL 168.588008 241.484684 \nL 168.883417 242.814993 \nL 169.178827 243.123802 \nL 169.474236 242.269566 \nL 169.769645 242.77689 \nL 170.065054 239.410305 \nL 170.360463 242.634244 \nL 170.655872 244.618145 \nL 170.951281 243.468039 \nL 171.24669 244.895806 \nL 171.542099 244.529738 \nL 171.837508 241.709898 \nL 172.132917 243.287885 \nL 172.428326 242.351563 \nL 172.723735 243.970432 \nL 173.019144 242.717831 \nL 173.314553 242.940402 \nL 173.609962 241.542474 \nL 173.905371 242.126229 \nL 174.20078 245.816201 \nL 174.496189 247.755713 \nL 174.791598 244.921436 \nL 175.087008 243.351746 \nL 175.382417 243.85503 \nL 175.677826 241.392278 \nL 175.973235 246.852559 \nL 176.268644 244.549818 \nL 176.564053 244.144145 \nL 176.859462 248.378034 \nL 177.154871 247.169488 \nL 177.45028 240.456719 \nL 177.745689 242.815097 \nL 178.041098 242.668564 \nL 178.336507 248.825966 \nL 178.631916 241.741937 \nL 178.927325 243.701236 \nL 179.222734 249.768312 \nL 179.518143 243.788968 \nL 179.813552 243.847496 \nL 180.108961 247.437487 \nL 180.40437 248.967082 \nL 180.699779 244.203348 \nL 180.995188 242.666998 \nL 181.290598 244.851783 \nL 181.586007 248.059016 \nL 181.881416 247.744288 \nL 182.472234 246.932998 \nL 182.767643 247.414026 \nL 183.063052 248.600295 \nL 183.358461 247.497678 \nL 183.65387 245.186889 \nL 183.949279 251.217074 \nL 184.244688 247.145136 \nL 184.540097 246.754113 \nL 184.835506 245.338305 \nL 185.130915 248.521165 \nL 186.60796 245.290355 \nL 186.903369 246.461793 \nL 187.494188 251.078011 \nL 187.789597 246.264876 \nL 188.085006 246.779485 \nL 188.380415 245.969248 \nL 188.675824 246.595757 \nL 189.266642 252.232092 \nL 189.562051 246.352825 \nL 189.85746 250.345216 \nL 190.152869 250.480188 \nL 190.448278 248.398403 \nL 190.743687 249.528758 \nL 191.039096 248.152033 \nL 191.334505 249.554472 \nL 191.629914 246.600182 \nL 191.925323 250.111927 \nL 192.220732 245.94561 \nL 192.516141 245.521592 \nL 192.81155 248.671088 \nL 193.10696 249.520212 \nL 193.402369 249.824254 \nL 193.697778 247.532199 \nL 193.993187 250.728866 \nL 194.288596 247.861959 \nL 194.584005 251.02839 \nL 194.879414 246.65145 \nL 195.174823 250.517154 \nL 195.470232 248.832998 \nL 195.765641 249.579122 \nL 196.06105 246.16892 \nL 196.356459 248.163009 \nL 196.651868 252.001333 \nL 196.947277 249.079123 \nL 197.242686 249.70066 \nL 197.538095 253.10948 \nL 197.833504 250.668563 \nL 198.128913 251.24684 \nL 198.424322 253.326168 \nL 198.719731 248.803232 \nL 199.015141 252.902604 \nL 199.31055 250.997929 \nL 199.605959 250.323341 \nL 199.901368 251.973952 \nL 200.196777 254.944731 \nL 200.492186 250.718629 \nL 200.787595 250.905065 \nL 201.083004 250.096807 \nL 201.378413 251.559998 \nL 201.673822 252.434704 \nL 201.969231 250.39213 \nL 202.26464 250.952264 \nL 202.560049 250.280174 \nL 202.855458 251.920677 \nL 203.150867 254.293083 \nL 203.446276 249.139639 \nL 203.741685 252.968015 \nL 204.037094 250.507596 \nL 204.332503 253.661699 \nL 204.627912 253.258748 \nL 204.923322 252.183214 \nL 205.218731 251.772907 \nL 205.809549 254.159621 \nL 206.104958 256.202504 \nL 206.400367 254.678066 \nL 206.695776 251.940825 \nL 206.991185 253.686726 \nL 207.582003 250.654194 \nL 207.877412 255.083518 \nL 208.172821 252.663174 \nL 208.46823 255.958693 \nL 208.763639 253.201935 \nL 209.059048 248.225991 \nL 209.354457 254.961663 \nL 209.649866 250.126529 \nL 209.945275 252.030742 \nL 210.240684 251.757345 \nL 210.536093 255.089152 \nL 210.831503 255.964749 \nL 211.126912 249.697917 \nL 211.422321 251.377547 \nL 211.71773 253.715163 \nL 212.013139 253.763362 \nL 212.308548 253.518523 \nL 212.603957 253.520699 \nL 212.899366 249.777978 \nL 213.490184 253.690212 \nL 213.785593 252.80978 \nL 214.081002 251.333496 \nL 214.376411 257.298708 \nL 214.67182 252.723694 \nL 214.967229 253.989434 \nL 215.262638 254.462298 \nL 215.558047 252.847116 \nL 215.853456 255.518005 \nL 216.148865 256.70566 \nL 216.444274 257.221116 \nL 216.739683 253.644177 \nL 217.035093 253.468737 \nL 217.330502 255.090719 \nL 217.625911 251.516229 \nL 217.92132 253.586789 \nL 218.216729 258.278972 \nL 218.512138 253.006909 \nL 218.807547 255.087325 \nL 219.102956 256.154614 \nL 219.398365 254.953498 \nL 219.693774 255.065426 \nL 219.989183 255.009247 \nL 220.284592 256.570612 \nL 220.87541 253.645791 \nL 221.170819 258.330882 \nL 221.466228 254.755457 \nL 221.761637 254.74067 \nL 222.057046 253.598347 \nL 222.352455 257.042717 \nL 222.647864 255.832232 \nL 222.943274 253.5531 \nL 223.238683 256.897252 \nL 223.534092 254.966245 \nL 223.829501 256.085516 \nL 224.420319 249.743361 \nL 224.715728 255.690176 \nL 225.011137 257.77356 \nL 225.306546 254.59641 \nL 225.601955 259.601546 \nL 225.897364 256.863928 \nL 226.192773 255.739223 \nL 226.488182 256.654742 \nL 226.783591 257.078306 \nL 227.079 255.477288 \nL 227.374409 258.278241 \nL 227.669818 255.353561 \nL 227.965227 255.594577 \nL 228.260636 255.633327 \nL 228.556045 257.112566 \nL 228.851455 255.606018 \nL 229.146864 255.636319 \nL 229.737682 258.799726 \nL 230.033091 251.990374 \nL 230.3285 259.154947 \nL 230.623909 255.831039 \nL 230.919318 257.095703 \nL 231.214727 255.518005 \nL 231.510136 259.778504 \nL 231.805545 259.304146 \nL 232.100954 257.920386 \nL 232.396363 255.83985 \nL 232.691772 258.35303 \nL 233.28259 260.800481 \nL 233.873408 254.370124 \nL 234.168817 259.247634 \nL 234.464226 260.691826 \nL 234.759636 259.561704 \nL 235.055045 260.88178 \nL 235.350454 260.136592 \nL 235.645863 256.775015 \nL 235.941272 256.878224 \nL 236.53209 260.17392 \nL 236.827499 259.705498 \nL 237.122908 258.001251 \nL 237.418317 254.776111 \nL 237.713726 254.888504 \nL 238.009135 259.313881 \nL 238.304544 257.108847 \nL 238.599953 256.634899 \nL 238.895362 258.857884 \nL 239.190771 258.482572 \nL 239.781589 254.753585 \nL 240.076998 251.280887 \nL 240.372407 258.595961 \nL 240.963226 257.434876 \nL 241.258635 259.836807 \nL 241.554044 257.467417 \nL 241.849453 256.755225 \nL 242.440271 262.081976 \nL 242.73568 256.428609 \nL 243.031089 254.770533 \nL 243.326498 257.620344 \nL 243.621907 263.210557 \nL 243.917316 258.19985 \nL 244.212725 260.019933 \nL 244.508134 256.944306 \nL 245.098952 258.630386 \nL 245.394361 262.330072 \nL 245.68977 260.11738 \nL 245.985179 259.630706 \nL 246.280588 261.782251 \nL 246.575997 259.200997 \nL 246.871407 259.502353 \nL 247.166816 262.407435 \nL 247.462225 262.186511 \nL 247.757634 260.669919 \nL 248.348452 255.915189 \nL 248.643861 260.061141 \nL 248.93927 257.61391 \nL 249.234679 258.560902 \nL 249.530088 259.960458 \nL 249.825497 259.819651 \nL 250.120906 257.961276 \nL 250.711724 261.287541 \nL 251.007133 257.212747 \nL 251.597951 262.62299 \nL 251.89336 262.603232 \nL 252.188769 260.947529 \nL 252.484178 258.116869 \nL 252.779588 259.652946 \nL 253.074997 263.178457 \nL 253.370406 259.916881 \nL 253.665815 258.982334 \nL 254.256633 259.959172 \nL 254.552042 257.919157 \nL 254.847451 260.865523 \nL 255.14286 261.609803 \nL 255.438269 261.41122 \nL 255.733678 262.507922 \nL 256.029087 259.568824 \nL 256.324496 260.29792 \nL 256.619905 259.494538 \nL 256.915314 260.650005 \nL 257.210723 260.901036 \nL 257.506132 262.652371 \nL 257.801541 261.901059 \nL 258.09695 264.771588 \nL 258.392359 261.913367 \nL 258.687769 260.768306 \nL 258.983178 262.561329 \nL 259.278587 260.036005 \nL 259.573996 263.368535 \nL 259.869405 261.244225 \nL 260.164814 262.159921 \nL 260.755632 259.110402 \nL 261.051041 264.499051 \nL 261.34645 263.373097 \nL 261.641859 261.47021 \nL 261.937268 262.317004 \nL 262.232677 261.751558 \nL 262.528086 262.450928 \nL 263.118904 259.603791 \nL 263.709722 263.604178 \nL 264.005131 259.791026 \nL 264.30054 259.483442 \nL 264.59595 262.42376 \nL 264.891359 261.672508 \nL 265.186768 262.032781 \nL 265.482177 260.15682 \nL 265.777586 261.518987 \nL 266.072995 264.055857 \nL 266.368404 261.80815 \nL 266.663813 262.17421 \nL 266.959222 262.307197 \nL 267.55004 261.543665 \nL 267.845449 266.194223 \nL 268.731676 259.97573 \nL 269.027085 263.688765 \nL 269.322494 259.570037 \nL 269.913312 264.263637 \nL 270.208721 260.690332 \nL 270.504131 261.665757 \nL 270.79954 263.392763 \nL 271.094949 260.613749 \nL 271.390358 264.883271 \nL 271.685767 262.076896 \nL 271.981176 261.810588 \nL 272.276585 258.960933 \nL 272.571994 265.113408 \nL 272.867403 264.367766 \nL 273.162812 264.03146 \nL 273.458221 261.049919 \nL 273.75363 261.477575 \nL 274.049039 265.228842 \nL 274.344448 266.316351 \nL 274.639857 265.47548 \nL 274.935266 265.182393 \nL 275.230675 263.769184 \nL 275.526084 263.154522 \nL 275.821493 260.656583 \nL 276.116902 262.317579 \nL 276.707721 261.905805 \nL 277.00313 263.337327 \nL 277.298539 262.36693 \nL 277.593948 260.76229 \nL 277.889357 262.568847 \nL 278.184766 268.217307 \nL 278.775584 265.231625 \nL 279.070993 265.787341 \nL 279.661811 260.722681 \nL 279.95722 266.080074 \nL 280.252629 265.209224 \nL 280.548038 256.98059 \nL 280.843447 265.817356 \nL 281.138856 264.848919 \nL 281.434265 262.993263 \nL 281.729674 264.910728 \nL 282.320492 262.449835 \nL 282.615902 265.904943 \nL 283.20672 261.831788 \nL 283.502129 265.62464 \nL 283.797538 256.489583 \nL 284.092947 262.681024 \nL 284.388356 263.841692 \nL 284.683765 265.617668 \nL 284.979174 261.097893 \nL 285.274583 260.530286 \nL 285.865401 267.9301 \nL 286.16081 264.065765 \nL 286.456219 262.150677 \nL 286.751628 263.350235 \nL 287.047037 260.463228 \nL 287.342446 264.472361 \nL 287.637855 261.399284 \nL 287.933264 264.533508 \nL 288.228673 265.103918 \nL 288.524083 263.030667 \nL 288.819492 263.450087 \nL 289.114901 265.822545 \nL 289.41031 261.767409 \nL 289.705719 260.669666 \nL 290.296537 265.326449 \nL 290.591946 265.329453 \nL 290.887355 262.633367 \nL 291.182764 264.473691 \nL 291.773582 266.306094 \nL 292.068991 263.325669 \nL 292.3644 266.225796 \nL 292.659809 263.476427 \nL 292.955218 267.471886 \nL 293.250627 266.552291 \nL 293.546036 263.58654 \nL 293.841445 264.272468 \nL 294.136854 265.803449 \nL 294.432264 266.424323 \nL 294.727673 263.349998 \nL 295.023082 263.898079 \nL 295.318491 260.565811 \nL 295.6139 264.960734 \nL 295.909309 267.128344 \nL 296.500127 263.442488 \nL 296.795536 266.076749 \nL 297.090945 264.126686 \nL 297.386354 266.033553 \nL 297.681763 266.313042 \nL 297.977172 264.519344 \nL 298.272581 263.69762 \nL 298.56799 266.054131 \nL 299.158808 265.170081 \nL 299.454217 262.12179 \nL 299.749626 267.862853 \nL 300.045035 268.916408 \nL 300.340445 267.515977 \nL 300.931263 268.233246 \nL 301.226672 265.732483 \nL 301.522081 266.025943 \nL 301.81749 263.236636 \nL 302.112899 262.902037 \nL 302.408308 268.099929 \nL 302.703717 268.973679 \nL 302.999126 265.222557 \nL 303.294535 263.926994 \nL 303.885353 268.643297 \nL 304.180762 262.46626 \nL 304.476171 263.247057 \nL 304.77158 267.618821 \nL 305.066989 267.221995 \nL 305.362398 269.217028 \nL 305.953216 265.673674 \nL 306.248626 266.213652 \nL 306.544035 270.02837 \nL 306.839444 269.814871 \nL 307.134853 263.849451 \nL 307.430262 267.800277 \nL 307.725671 266.228499 \nL 308.02108 267.203948 \nL 308.316489 268.683392 \nL 308.611898 265.981607 \nL 308.907307 264.359478 \nL 309.202716 268.941744 \nL 309.498125 267.431486 \nL 309.793534 269.846139 \nL 310.088943 264.337659 \nL 310.384352 267.951392 \nL 310.679761 266.633216 \nL 310.97517 268.113547 \nL 311.270579 265.286173 \nL 311.565988 266.356972 \nL 312.156806 270.76965 \nL 312.452216 267.15972 \nL 312.747625 266.950446 \nL 313.043034 262.224959 \nL 313.338443 269.948168 \nL 313.633852 266.212459 \nL 313.929261 268.40775 \nL 314.22467 266.773103 \nL 314.520079 266.221073 \nL 314.815488 264.979613 \nL 315.110897 267.227826 \nL 315.406306 265.595078 \nL 315.701715 269.001405 \nL 315.997124 269.104895 \nL 316.292533 264.430343 \nL 316.587942 265.659225 \nL 316.883351 265.772442 \nL 317.17876 268.790617 \nL 317.474169 265.025202 \nL 317.769578 266.459154 \nL 318.064987 264.929274 \nL 318.360397 269.610919 \nL 318.655806 268.119692 \nL 318.951215 267.237236 \nL 319.246624 267.719396 \nL 319.542033 267.069065 \nL 319.837442 266.790897 \nL 320.132851 265.311461 \nL 320.42826 266.402983 \nL 320.723669 265.791501 \nL 321.019078 270.154197 \nL 321.314487 269.409676 \nL 321.609896 265.919108 \nL 321.905305 267.07139 \nL 322.496123 264.687234 \nL 322.791532 265.24356 \nL 323.086941 267.151969 \nL 323.38235 264.02322 \nL 323.677759 267.130705 \nL 324.268578 264.6948 \nL 324.563987 270.944351 \nL 325.154805 266.178476 \nL 325.450214 268.127623 \nL 325.745623 265.642113 \nL 326.041032 268.537975 \nL 326.63185 266.261686 \nL 326.927259 264.90456 \nL 327.222668 272.222216 \nL 327.518077 264.256875 \nL 327.813486 266.5777 \nL 328.108895 273.324656 \nL 328.404304 268.867422 \nL 328.699713 269.181049 \nL 328.995122 272.704541 \nL 329.290531 268.121483 \nL 329.58594 266.670271 \nL 329.881349 264.360445 \nL 330.176759 270.444268 \nL 330.472168 268.35374 \nL 330.767577 268.126856 \nL 331.062986 268.604511 \nL 331.358395 265.896795 \nL 331.653804 270.593805 \nL 331.949213 262.564542 \nL 332.244622 268.017575 \nL 332.540031 266.735389 \nL 332.83544 270.87947 \nL 333.426258 267.709244 \nL 334.017076 269.666848 \nL 334.312485 270.076626 \nL 334.607894 268.639815 \nL 335.198712 267.633985 \nL 335.494121 272.906329 \nL 335.78953 271.835225 \nL 336.380349 266.397597 \nL 336.675758 268.286032 \nL 337.266576 268.284979 \nL 337.561985 271.394143 \nL 337.857394 267.161415 \nL 338.152803 268.793328 \nL 338.448212 269.092016 \nL 339.03903 271.911933 \nL 339.629848 266.80164 \nL 340.220666 271.473635 \nL 340.811484 265.931898 \nL 341.106893 268.732093 \nL 341.402302 267.164363 \nL 341.697711 268.339323 \nL 341.993121 267.401945 \nL 342.28853 268.165224 \nL 342.583939 271.544584 \nL 342.879348 267.046756 \nL 343.174757 270.910601 \nL 343.470166 270.205437 \nL 343.765575 268.401843 \nL 344.060984 267.976745 \nL 344.356393 269.843818 \nL 344.651802 272.989865 \nL 344.947211 266.571298 \nL 345.24262 264.510416 \nL 345.538029 272.152772 \nL 345.833438 267.390817 \nL 346.128847 267.001264 \nL 346.424256 269.490236 \nL 346.719665 270.093669 \nL 347.015074 271.339254 \nL 347.310483 268.134442 \nL 347.605892 270.755635 \nL 347.901301 270.186457 \nL 348.196711 269.931932 \nL 348.49212 271.190632 \nL 348.787529 269.593775 \nL 349.082938 269.542885 \nL 349.378347 272.235782 \nL 349.969165 268.106592 \nL 350.264574 270.00303 \nL 350.559983 268.829748 \nL 350.855392 269.898146 \nL 351.150801 269.465148 \nL 351.44621 270.505077 \nL 351.741619 267.348955 \nL 352.037028 269.897242 \nL 352.332437 266.126599 \nL 352.627846 270.293972 \nL 352.923255 269.015296 \nL 353.218664 271.035107 \nL 353.514073 265.564128 \nL 353.809482 269.348964 \nL 354.400301 271.987019 \nL 354.69571 270.012604 \nL 354.991119 269.27336 \nL 355.286528 269.944702 \nL 355.581937 267.863632 \nL 355.877346 269.304041 \nL 356.172755 267.460465 \nL 356.468164 269.42682 \nL 356.763573 267.022536 \nL 357.058982 268.065465 \nL 357.354391 268.357672 \nL 357.6498 268.176517 \nL 357.945209 271.986497 \nL 358.240618 267.761503 \nL 358.536027 268.099122 \nL 358.831436 270.228789 \nL 359.126845 271.381823 \nL 359.422254 269.802069 \nL 360.013073 271.79664 \nL 360.308482 271.110245 \nL 360.603891 269.481071 \nL 360.8993 270.549991 \nL 361.194709 269.011762 \nL 361.490118 270.955893 \nL 362.080936 269.356184 \nL 362.376345 273.494739 \nL 362.671754 268.478547 \nL 362.967163 269.900925 \nL 363.262572 268.857109 \nL 363.557981 268.790569 \nL 363.85339 273.588631 \nL 364.148799 269.183519 \nL 364.444208 272.198429 \nL 364.739617 272.365078 \nL 365.035026 273.10972 \nL 365.330435 269.579028 \nL 365.625844 271.901925 \nL 365.921254 268.606017 \nL 366.216663 270.252816 \nL 366.512072 267.274331 \nL 366.807481 269.622926 \nL 367.10289 270.627153 \nL 367.398299 267.984275 \nL 367.693708 272.766691 \nL 367.989117 268.982972 \nL 368.284526 271.35284 \nL 368.579935 270.357158 \nL 368.875344 268.135619 \nL 369.170753 269.697373 \nL 369.761571 271.155838 \nL 370.05698 272.632419 \nL 370.352389 266.459844 \nL 370.647798 271.640031 \nL 370.943207 267.981612 \nL 371.238616 269.832811 \nL 371.534025 269.229004 \nL 371.829435 271.569005 \nL 372.124844 272.852584 \nL 372.420253 266.479639 \nL 372.715662 270.671654 \nL 373.011071 267.901205 \nL 373.30648 272.131733 \nL 373.601889 271.025722 \nL 374.192707 273.195396 \nL 374.488116 276.334125 \nL 374.783525 271.595325 \nL 375.374343 270.257065 \nL 375.669752 270.850768 \nL 375.965161 270.327163 \nL 376.26057 273.641071 \nL 376.555979 271.405332 \nL 376.851388 270.785766 \nL 377.146797 270.524402 \nL 377.442206 275.142033 \nL 377.737615 269.557663 \nL 378.033025 271.482879 \nL 378.328434 268.90128 \nL 378.623843 268.992566 \nL 378.919252 269.598682 \nL 379.214661 272.354601 \nL 379.51007 270.963142 \nL 379.805479 270.75237 \nL 380.100888 269.912973 \nL 380.396297 272.607492 \nL 380.987115 268.444769 \nL 381.577933 272.678144 \nL 381.873342 270.1003 \nL 382.168751 270.409016 \nL 382.46416 270.265121 \nL 383.054978 273.635975 \nL 383.350387 269.918768 \nL 383.645796 275.990229 \nL 383.941206 271.738746 \nL 384.236615 270.648418 \nL 384.532024 273.316131 \nL 384.827433 269.103474 \nL 385.122842 268.823286 \nL 385.418251 273.348628 \nL 386.009069 269.886728 \nL 386.304478 269.281934 \nL 386.599887 274.506829 \nL 386.895296 269.939164 \nL 387.190705 272.121472 \nL 387.486114 267.733026 \nL 387.781523 272.182639 \nL 388.076932 268.897971 \nL 388.372341 272.169125 \nL 388.66775 273.56211 \nL 389.258568 272.443409 \nL 389.553977 269.726911 \nL 389.849387 270.604463 \nL 390.144796 269.090004 \nL 390.440205 269.324077 \nL 390.735614 273.736891 \nL 391.031023 269.667507 \nL 391.031023 269.667507 \n\" clip-path=\"url(#p0e73b5e31f)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 66.376477 38.76556 \nL 67.558113 57.560074 \nL 69.330568 90.790501 \nL 69.921386 104.381688 \nL 70.216795 108.859866 \nL 70.512204 115.126674 \nL 70.807613 115.885042 \nL 71.989249 132.758351 \nL 72.284658 133.322718 \nL 72.580067 134.729128 \nL 73.170885 142.781852 \nL 73.466294 142.435197 \nL 73.761704 142.771796 \nL 74.057113 145.588616 \nL 74.352522 143.962374 \nL 74.647931 143.833238 \nL 74.94334 150.680509 \nL 75.238749 147.059217 \nL 75.534158 146.400011 \nL 75.829567 152.869222 \nL 76.124976 151.68735 \nL 76.420385 149.87215 \nL 76.715794 151.228651 \nL 77.011203 153.767103 \nL 77.306612 152.611399 \nL 77.602021 149.257601 \nL 78.192839 158.295095 \nL 78.783657 154.928927 \nL 79.079066 152.244118 \nL 79.374475 157.006704 \nL 79.669885 159.405627 \nL 79.965294 160.325259 \nL 80.260703 159.3766 \nL 80.556112 162.318862 \nL 80.851521 161.361449 \nL 81.14693 158.036244 \nL 81.442339 158.408649 \nL 81.737748 166.025371 \nL 82.033157 160.034795 \nL 82.328566 162.753434 \nL 82.623975 163.817173 \nL 82.919384 163.115381 \nL 83.214793 165.132663 \nL 83.510202 163.057134 \nL 83.805611 162.568444 \nL 84.10102 166.728287 \nL 84.396429 167.553464 \nL 84.691838 166.537065 \nL 84.987247 163.950566 \nL 85.282656 164.772546 \nL 85.873475 170.451101 \nL 86.168884 169.495873 \nL 86.464293 167.414031 \nL 86.759702 169.765245 \nL 87.055111 169.826914 \nL 87.35052 171.775024 \nL 87.645929 170.323443 \nL 87.941338 171.128412 \nL 88.236747 168.302226 \nL 88.532156 166.763579 \nL 88.827565 174.339195 \nL 89.122974 173.688422 \nL 89.418383 173.951674 \nL 90.009201 172.141052 \nL 90.30461 174.782859 \nL 90.600019 174.21438 \nL 90.895428 171.487516 \nL 91.190837 176.193381 \nL 91.486246 176.017114 \nL 91.781656 176.867109 \nL 92.077065 176.890659 \nL 92.372474 179.502538 \nL 92.667883 177.403975 \nL 92.963292 176.724656 \nL 93.258701 176.696802 \nL 93.55411 173.748227 \nL 93.849519 176.398483 \nL 94.144928 177.120595 \nL 94.440337 181.46897 \nL 94.735746 181.068469 \nL 95.031155 178.462509 \nL 95.326564 180.737388 \nL 95.621973 181.009419 \nL 95.917382 180.74444 \nL 96.212791 182.312612 \nL 96.5082 181.352717 \nL 97.099018 182.108169 \nL 97.394427 181.178401 \nL 97.689837 181.476199 \nL 97.985246 180.773708 \nL 98.280655 182.999288 \nL 98.576064 179.679239 \nL 98.871473 183.877619 \nL 99.166882 183.722066 \nL 99.462291 182.784684 \nL 99.7577 184.31028 \nL 100.053109 182.051328 \nL 100.348518 185.459445 \nL 100.643927 186.41249 \nL 100.939336 185.523259 \nL 101.234745 185.874998 \nL 101.530154 183.307679 \nL 102.120972 185.78525 \nL 102.416381 187.454478 \nL 102.71179 182.317696 \nL 103.007199 184.429119 \nL 103.302608 184.307509 \nL 103.598018 183.218324 \nL 103.893427 186.168723 \nL 104.188836 187.868232 \nL 104.484245 187.007072 \nL 104.779654 186.888289 \nL 105.075063 189.108238 \nL 105.370472 188.555341 \nL 105.665881 187.077841 \nL 105.96129 184.745935 \nL 106.256699 186.745739 \nL 106.552108 187.563286 \nL 106.847517 187.895387 \nL 107.142926 186.955315 \nL 107.438335 186.822709 \nL 107.733744 187.54605 \nL 108.029153 187.524348 \nL 108.324562 188.659884 \nL 108.619971 187.545038 \nL 108.91538 185.162516 \nL 109.506199 190.589047 \nL 109.801608 193.087148 \nL 110.097017 190.296174 \nL 110.392426 193.116568 \nL 110.687835 190.7779 \nL 110.983244 190.09557 \nL 111.278653 191.037474 \nL 111.574062 193.287189 \nL 111.869471 193.261808 \nL 112.16488 193.687352 \nL 112.460289 192.330402 \nL 112.755698 194.379176 \nL 113.051107 191.498631 \nL 113.346516 193.451794 \nL 113.641925 192.655467 \nL 113.937334 192.773487 \nL 114.232743 195.143801 \nL 114.528152 192.500444 \nL 114.823561 193.433714 \nL 115.11897 196.402642 \nL 115.709789 198.55473 \nL 116.005198 199.422588 \nL 116.300607 195.870539 \nL 116.596016 196.593246 \nL 116.891425 199.027292 \nL 117.186834 197.733513 \nL 117.777652 200.156451 \nL 118.073061 201.652111 \nL 118.36847 201.733875 \nL 118.663879 201.453017 \nL 118.959288 199.362309 \nL 119.254697 200.648771 \nL 119.550106 200.269563 \nL 120.140924 201.030172 \nL 120.436333 200.804742 \nL 121.027151 202.487191 \nL 121.61797 204.129642 \nL 121.913379 201.813013 \nL 122.208788 202.94063 \nL 122.799606 203.955511 \nL 123.095015 205.584998 \nL 123.390424 205.479524 \nL 123.685833 202.963103 \nL 123.981242 203.348945 \nL 124.276651 203.251358 \nL 124.57206 204.122437 \nL 124.867469 207.246388 \nL 125.162878 204.100735 \nL 125.458287 205.231878 \nL 125.753696 207.394736 \nL 126.049105 206.808732 \nL 126.344514 208.319356 \nL 126.639923 208.2449 \nL 127.526151 206.534781 \nL 127.82156 206.576418 \nL 128.707787 208.870188 \nL 129.003196 207.602785 \nL 129.298605 208.42532 \nL 129.594014 209.602148 \nL 130.184832 209.184306 \nL 130.480241 207.545189 \nL 130.77565 208.65671 \nL 131.071059 208.397369 \nL 131.366468 210.520647 \nL 131.661877 210.629053 \nL 131.957286 211.249168 \nL 132.252695 210.327858 \nL 132.548104 211.064492 \nL 132.843513 211.130955 \nL 133.138922 210.953989 \nL 133.434332 211.229145 \nL 133.729741 210.784959 \nL 134.02515 211.271874 \nL 134.320559 210.75202 \nL 134.615968 211.5709 \nL 134.911377 211.48773 \nL 135.206786 211.060315 \nL 135.502195 211.647813 \nL 135.797604 210.797039 \nL 136.093013 211.750477 \nL 136.683831 214.060615 \nL 136.97924 213.260401 \nL 137.274649 213.075773 \nL 137.865467 216.08086 \nL 138.160876 215.196987 \nL 138.456285 215.937347 \nL 138.751694 214.356003 \nL 139.047103 214.753692 \nL 139.342513 212.744684 \nL 139.933331 212.007239 \nL 140.22874 211.43525 \nL 140.819558 212.383925 \nL 141.114967 211.391806 \nL 141.410376 211.893508 \nL 141.705785 212.982676 \nL 142.001194 216.526227 \nL 142.296603 215.550918 \nL 142.592012 215.347046 \nL 142.887421 216.765134 \nL 143.18283 215.833535 \nL 143.478239 213.9467 \nL 143.773648 214.258809 \nL 144.069057 216.329045 \nL 144.659875 212.479576 \nL 144.955284 213.309613 \nL 145.250694 214.479348 \nL 145.841512 215.70358 \nL 146.136921 214.643375 \nL 146.43233 212.596231 \nL 146.727739 214.284206 \nL 147.023148 214.332839 \nL 147.318557 214.164556 \nL 147.613966 214.751443 \nL 147.909375 216.912543 \nL 148.204784 214.656097 \nL 148.500193 214.950939 \nL 148.795602 214.472634 \nL 149.091011 214.908884 \nL 149.38642 213.77693 \nL 149.681829 214.733733 \nL 149.977238 212.937216 \nL 150.272647 212.286137 \nL 150.568056 214.646933 \nL 150.863465 214.754327 \nL 151.158874 215.915436 \nL 151.454284 215.715981 \nL 152.045102 214.171061 \nL 152.63592 216.071238 \nL 152.931329 215.13478 \nL 153.522147 214.541306 \nL 153.817556 214.046985 \nL 154.408374 216.331294 \nL 154.999192 215.49929 \nL 155.294601 213.733799 \nL 155.59001 213.352736 \nL 156.180828 213.093644 \nL 156.476237 213.395248 \nL 156.771646 214.262343 \nL 157.067055 213.984128 \nL 157.362465 216.223827 \nL 157.657874 215.834082 \nL 157.953283 215.053281 \nL 158.248692 213.956956 \nL 158.544101 214.685976 \nL 159.134919 214.167664 \nL 159.430328 214.149207 \nL 159.725737 216.276966 \nL 160.316555 216.780933 \nL 160.611964 216.564499 \nL 161.202782 220.015201 \nL 161.498191 219.039531 \nL 161.7936 216.712204 \nL 162.089009 215.929251 \nL 162.384418 216.990701 \nL 162.679827 216.306098 \nL 162.975236 216.3167 \nL 163.270646 216.497609 \nL 163.566055 218.040497 \nL 163.861464 218.970096 \nL 164.156873 219.462216 \nL 164.452282 218.897223 \nL 164.747691 219.099489 \nL 165.0431 218.460675 \nL 165.338509 218.710057 \nL 165.633918 217.884173 \nL 165.929327 218.128518 \nL 166.224736 218.620437 \nL 166.520145 221.930293 \nL 167.110963 220.281369 \nL 167.406372 218.506505 \nL 167.701781 217.647803 \nL 168.292599 220.072179 \nL 168.588008 219.719838 \nL 169.178827 221.163195 \nL 169.474236 219.568662 \nL 170.065054 218.081082 \nL 170.655872 220.36784 \nL 170.951281 219.463148 \nL 171.24669 219.67645 \nL 171.542099 219.477733 \nL 171.837508 220.735562 \nL 172.132917 220.145968 \nL 172.428326 219.812879 \nL 172.723735 217.318465 \nL 173.019144 217.351259 \nL 173.314553 216.970581 \nL 173.609962 218.832237 \nL 174.20078 220.349857 \nL 174.496189 219.740447 \nL 174.791598 219.419374 \nL 175.087008 219.511644 \nL 175.382417 219.082486 \nL 175.677826 218.096085 \nL 175.973235 219.429968 \nL 176.268644 219.669831 \nL 176.564053 220.194681 \nL 176.859462 221.8686 \nL 177.45028 220.262823 \nL 177.745689 220.529385 \nL 178.041098 220.260542 \nL 178.336507 221.164528 \nL 178.631916 221.57914 \nL 178.927325 221.56784 \nL 179.518143 219.942224 \nL 179.813552 220.647445 \nL 180.108961 220.726607 \nL 180.40437 221.010123 \nL 180.699779 219.346397 \nL 180.995188 220.429838 \nL 181.290598 220.438568 \nL 181.586007 220.57919 \nL 181.881416 220.852787 \nL 182.176825 221.315085 \nL 182.767643 221.187587 \nL 183.063052 220.310525 \nL 183.358461 219.920722 \nL 183.65387 220.002423 \nL 183.949279 220.285562 \nL 184.540097 222.545702 \nL 184.835506 222.139475 \nL 185.130915 222.99845 \nL 185.426324 221.79392 \nL 185.721733 221.535568 \nL 186.017142 220.920689 \nL 186.312551 219.993467 \nL 186.60796 220.561022 \nL 186.903369 221.831196 \nL 187.494188 223.170749 \nL 187.789597 223.381907 \nL 188.085006 222.24315 \nL 188.380415 222.7982 \nL 188.971233 222.028884 \nL 189.266642 221.711786 \nL 189.562051 221.964356 \nL 189.85746 223.877263 \nL 190.152869 223.907279 \nL 190.448278 222.46813 \nL 190.743687 222.457079 \nL 191.039096 221.175218 \nL 191.629914 223.2764 \nL 191.925323 224.798236 \nL 192.220732 224.195092 \nL 192.516141 223.221213 \nL 192.81155 223.4134 \nL 193.10696 223.379088 \nL 193.402369 223.847811 \nL 193.697778 222.550561 \nL 193.993187 222.954331 \nL 194.288596 224.16686 \nL 194.584005 222.968258 \nL 195.174823 224.091987 \nL 195.470232 222.761647 \nL 195.765641 222.938934 \nL 196.06105 222.337621 \nL 196.356459 222.117138 \nL 196.651868 222.046843 \nL 197.242686 223.097812 \nL 197.833504 223.638741 \nL 198.128913 223.214386 \nL 198.424322 223.683905 \nL 198.719731 223.718401 \nL 199.015141 224.297731 \nL 199.31055 223.934193 \nL 199.605959 223.970827 \nL 200.196777 224.741475 \nL 200.787595 223.928097 \nL 201.083004 223.049034 \nL 201.378413 223.860091 \nL 202.26464 220.542461 \nL 202.560049 220.680641 \nL 202.855458 221.162913 \nL 203.150867 222.126182 \nL 203.446276 222.226974 \nL 203.741685 222.080754 \nL 204.332503 220.201372 \nL 204.627912 220.128764 \nL 205.218731 219.736672 \nL 206.104958 222.013945 \nL 206.400367 223.855489 \nL 206.695776 223.710723 \nL 207.286594 223.094414 \nL 207.582003 223.276047 \nL 207.877412 223.088864 \nL 208.172821 223.109378 \nL 208.46823 225.012478 \nL 209.059048 223.207013 \nL 209.354457 223.479405 \nL 209.945275 222.515309 \nL 210.240684 223.951912 \nL 211.126912 224.337898 \nL 211.71773 224.619576 \nL 212.013139 223.217583 \nL 212.308548 223.245132 \nL 212.603957 223.69184 \nL 213.194775 224.788767 \nL 213.490184 224.068221 \nL 213.785593 224.173165 \nL 214.376411 223.386662 \nL 214.67182 223.37622 \nL 214.967229 225.032919 \nL 215.262638 224.995539 \nL 215.558047 224.258078 \nL 216.148865 225.922319 \nL 216.444274 225.875742 \nL 217.035093 225.013064 \nL 217.330502 225.354555 \nL 217.92132 223.779532 \nL 218.216729 224.265274 \nL 218.512138 224.496664 \nL 218.807547 223.880428 \nL 219.102956 224.029475 \nL 219.693774 223.638155 \nL 220.284592 225.02508 \nL 220.580001 225.185701 \nL 221.170819 226.402303 \nL 221.466228 226.51866 \nL 222.057046 226.227738 \nL 222.352455 225.450455 \nL 222.943274 224.933846 \nL 223.238683 225.379687 \nL 223.829501 227.086087 \nL 224.12491 227.524 \nL 224.420319 227.066481 \nL 224.715728 226.955939 \nL 225.011137 227.945576 \nL 225.306546 227.057911 \nL 225.601955 226.794787 \nL 225.897364 225.652962 \nL 226.192773 226.640752 \nL 226.488182 226.961312 \nL 226.783591 226.992483 \nL 227.079 226.360585 \nL 227.374409 225.108403 \nL 227.669818 226.335815 \nL 228.556045 226.793518 \nL 228.851455 226.292049 \nL 229.146864 226.738251 \nL 229.442273 226.450076 \nL 229.737682 227.243375 \nL 230.033091 225.895557 \nL 230.3285 226.48124 \nL 230.623909 226.744516 \nL 230.919318 225.921965 \nL 231.214727 225.759883 \nL 231.510136 226.402431 \nL 231.805545 225.584025 \nL 232.100954 223.854124 \nL 232.396363 223.0071 \nL 232.691772 224.086742 \nL 232.987181 224.277603 \nL 233.28259 225.929821 \nL 233.577999 225.653444 \nL 233.873408 226.167869 \nL 234.168817 226.003738 \nL 234.759636 226.725023 \nL 235.055045 226.276572 \nL 235.350454 226.386785 \nL 235.941272 225.82013 \nL 236.236681 226.06274 \nL 237.418317 223.899102 \nL 237.713726 224.345079 \nL 238.009135 224.544229 \nL 238.304544 224.467669 \nL 238.599953 224.989025 \nL 238.895362 226.228855 \nL 239.48618 226.876728 \nL 239.781589 226.724541 \nL 240.076998 226.794731 \nL 240.667817 227.717913 \nL 240.963226 226.905619 \nL 241.258635 227.101492 \nL 242.144862 228.203736 \nL 242.440271 227.590608 \nL 242.73568 227.812223 \nL 243.326498 228.756207 \nL 243.621907 228.826413 \nL 244.508134 226.951352 \nL 244.803543 227.417232 \nL 245.098952 227.131547 \nL 245.68977 226.291977 \nL 245.985179 226.473409 \nL 246.575997 227.627699 \nL 246.871407 226.958019 \nL 247.166816 227.09138 \nL 247.462225 226.85187 \nL 247.757634 226.92602 \nL 248.053043 227.602134 \nL 248.348452 227.324737 \nL 248.643861 227.283791 \nL 248.93927 227.130342 \nL 249.234679 227.436444 \nL 249.530088 227.574914 \nL 249.825497 227.248804 \nL 250.416315 229.51827 \nL 250.711724 229.113327 \nL 251.007133 228.409142 \nL 251.302542 228.041155 \nL 251.597951 228.276689 \nL 251.89336 228.345177 \nL 252.484178 226.653322 \nL 252.779588 226.291857 \nL 253.074997 226.661812 \nL 253.370406 226.560932 \nL 253.665815 227.331653 \nL 253.961224 228.650684 \nL 254.256633 229.340043 \nL 254.552042 228.159328 \nL 254.847451 228.505894 \nL 255.14286 228.242546 \nL 255.438269 227.19491 \nL 255.733678 226.851934 \nL 256.029087 226.738235 \nL 256.324496 226.316771 \nL 256.915314 226.968468 \nL 257.210723 227.50637 \nL 257.506132 228.864195 \nL 258.09695 227.917874 \nL 258.392359 228.306343 \nL 258.687769 226.709867 \nL 258.983178 226.332152 \nL 259.278587 227.988289 \nL 259.869405 227.893618 \nL 260.164814 227.209609 \nL 260.460223 226.925875 \nL 261.051041 227.905673 \nL 261.34645 226.685297 \nL 261.641859 227.54061 \nL 261.937268 227.482467 \nL 262.232677 227.281598 \nL 262.528086 228.34553 \nL 262.823495 228.274424 \nL 263.118904 227.343476 \nL 263.414313 227.680412 \nL 263.709722 228.308407 \nL 264.005131 227.723117 \nL 264.30054 227.398879 \nL 264.59595 226.653539 \nL 264.891359 226.803638 \nL 265.186768 227.472363 \nL 265.482177 226.287158 \nL 266.072995 227.361877 \nL 266.368404 228.191953 \nL 266.663813 228.026882 \nL 266.959222 226.968396 \nL 267.254631 227.329147 \nL 267.55004 227.084898 \nL 267.845449 226.350794 \nL 268.731676 228.159866 \nL 269.027085 227.262041 \nL 269.322494 227.269253 \nL 270.208721 228.728883 \nL 270.79954 228.908957 \nL 271.094949 229.301763 \nL 271.685767 228.691976 \nL 271.981176 228.758094 \nL 272.276585 228.655415 \nL 272.571994 228.123923 \nL 272.867403 227.89765 \nL 273.162812 228.385344 \nL 273.458221 229.481395 \nL 274.049039 228.21596 \nL 274.344448 227.779099 \nL 274.639857 228.253212 \nL 274.935266 228.990071 \nL 275.230675 229.233436 \nL 275.526084 230.370988 \nL 275.821493 230.106234 \nL 276.116902 230.13 \nL 276.412312 229.676023 \nL 277.593948 229.896883 \nL 277.889357 229.048382 \nL 278.184766 229.564043 \nL 278.480175 228.764785 \nL 278.775584 229.461002 \nL 279.070993 229.390226 \nL 279.366402 229.440272 \nL 279.661811 230.102892 \nL 280.252629 229.791819 \nL 280.843447 230.042766 \nL 281.138856 229.597319 \nL 281.434265 229.415558 \nL 281.729674 229.476745 \nL 282.025083 229.378588 \nL 282.320492 229.119456 \nL 282.615902 229.045747 \nL 282.911311 229.373407 \nL 283.20672 229.93036 \nL 283.502129 229.464296 \nL 284.092947 230.319704 \nL 284.388356 229.259411 \nL 284.683765 229.36866 \nL 284.979174 230.162119 \nL 285.274583 231.58791 \nL 285.865401 231.239031 \nL 286.751628 230.203829 \nL 287.342446 230.660665 \nL 287.637855 229.683678 \nL 287.933264 229.449115 \nL 288.524083 229.89763 \nL 288.819492 229.107344 \nL 289.114901 228.901519 \nL 289.41031 229.351898 \nL 289.705719 228.434475 \nL 290.591946 228.876637 \nL 290.887355 228.134244 \nL 291.182764 228.221936 \nL 291.478173 228.1252 \nL 291.773582 228.253509 \nL 292.068991 227.694347 \nL 292.3644 228.411086 \nL 292.659809 228.271549 \nL 292.955218 228.554358 \nL 293.250627 228.53517 \nL 293.546036 228.175945 \nL 294.136854 228.606461 \nL 294.727673 228.15699 \nL 295.318491 227.922918 \nL 295.6139 228.407922 \nL 295.909309 229.115713 \nL 296.500127 229.767699 \nL 296.795536 229.974511 \nL 297.090945 229.851126 \nL 297.386354 230.295127 \nL 297.681763 229.602202 \nL 297.977172 229.50525 \nL 298.272581 229.238062 \nL 298.56799 228.706209 \nL 298.863399 228.694755 \nL 299.158808 229.293667 \nL 299.454217 229.535498 \nL 299.749626 228.727108 \nL 300.045035 228.905254 \nL 300.340445 229.384579 \nL 300.635854 229.217629 \nL 301.226672 229.314365 \nL 301.522081 229.082766 \nL 301.81749 229.718215 \nL 302.112899 229.758904 \nL 302.703717 229.396788 \nL 302.999126 229.5543 \nL 303.885353 231.028403 \nL 304.180762 230.606546 \nL 304.476171 229.581665 \nL 304.77158 229.831351 \nL 305.066989 229.649285 \nL 305.657807 230.381116 \nL 305.953216 229.701179 \nL 306.248626 230.396593 \nL 306.544035 230.69925 \nL 306.839444 230.81735 \nL 307.134853 229.921557 \nL 308.02108 230.327351 \nL 308.316489 230.227338 \nL 308.611898 228.989018 \nL 309.202716 230.511738 \nL 310.088943 231.233537 \nL 310.384352 231.641146 \nL 311.270579 229.208095 \nL 311.565988 229.427204 \nL 311.861397 230.486349 \nL 312.156806 230.742148 \nL 312.452216 230.778468 \nL 312.747625 230.253201 \nL 313.043034 230.669275 \nL 313.338443 231.455369 \nL 313.633852 231.824505 \nL 314.22467 231.002468 \nL 314.520079 231.708244 \nL 314.815488 231.452542 \nL 315.110897 231.680317 \nL 315.701715 231.296434 \nL 315.997124 231.510829 \nL 316.292533 231.1275 \nL 316.587942 230.255538 \nL 317.17876 231.544611 \nL 317.474169 231.6206 \nL 318.064987 231.356713 \nL 318.655806 229.92824 \nL 318.951215 229.833295 \nL 319.246624 230.497554 \nL 319.542033 228.916314 \nL 319.837442 229.366556 \nL 320.132851 229.426714 \nL 320.42826 229.755185 \nL 320.723669 229.548228 \nL 321.019078 229.74537 \nL 321.314487 230.280637 \nL 321.609896 229.930031 \nL 321.905305 230.077729 \nL 322.200714 229.801569 \nL 322.496123 229.74974 \nL 322.791532 230.078805 \nL 323.086941 229.834765 \nL 323.38235 229.053747 \nL 323.677759 228.791234 \nL 323.973168 228.845505 \nL 324.268578 229.80944 \nL 324.563987 229.93697 \nL 325.154805 228.801442 \nL 325.745623 228.824261 \nL 326.041032 229.454497 \nL 326.336441 229.123496 \nL 326.927259 230.419452 \nL 327.222668 230.432006 \nL 327.518077 229.734616 \nL 327.813486 229.914361 \nL 328.108895 229.298695 \nL 328.404304 229.457854 \nL 328.699713 230.360289 \nL 329.290531 230.641846 \nL 329.58594 230.11588 \nL 329.881349 229.928232 \nL 330.472168 231.074257 \nL 330.767577 230.908632 \nL 331.062986 231.12428 \nL 331.358395 230.997111 \nL 331.653804 230.371614 \nL 332.540031 230.063103 \nL 333.130849 229.323216 \nL 333.721667 229.028961 \nL 334.017076 228.881665 \nL 334.607894 229.123078 \nL 334.903303 229.9677 \nL 335.198712 229.441172 \nL 335.494121 229.209019 \nL 335.78953 229.766904 \nL 336.675758 230.38881 \nL 336.971167 230.77601 \nL 337.266576 230.80817 \nL 337.561985 230.639814 \nL 337.857394 230.617244 \nL 338.152803 230.408416 \nL 338.743621 231.720918 \nL 339.03903 231.766812 \nL 339.334439 231.43304 \nL 339.629848 231.554715 \nL 339.925257 231.094064 \nL 340.220666 231.005384 \nL 341.106893 231.79049 \nL 341.402302 231.457594 \nL 341.697711 232.036064 \nL 341.993121 231.518331 \nL 342.28853 232.250041 \nL 342.583939 231.938822 \nL 342.879348 232.811435 \nL 343.765575 231.545012 \nL 344.060984 231.488597 \nL 344.356393 232.125242 \nL 344.651802 231.768957 \nL 344.947211 231.771446 \nL 345.24262 231.602095 \nL 345.833438 230.47084 \nL 346.128847 230.055609 \nL 346.424256 230.993433 \nL 346.719665 231.291141 \nL 347.310483 231.496187 \nL 347.605892 231.528619 \nL 348.196711 231.202405 \nL 348.49212 230.772709 \nL 349.082938 230.553046 \nL 350.264574 230.822242 \nL 350.559983 230.689628 \nL 350.855392 231.049029 \nL 351.44621 230.267425 \nL 351.741619 229.472416 \nL 352.037028 230.419757 \nL 352.627846 230.917813 \nL 352.923255 230.543608 \nL 353.218664 230.691523 \nL 353.514073 231.45773 \nL 354.400301 230.584956 \nL 354.69571 229.617326 \nL 354.991119 229.351295 \nL 355.286528 229.606379 \nL 355.581937 229.193148 \nL 355.877346 229.06565 \nL 356.172755 229.498455 \nL 356.468164 229.209621 \nL 356.763573 229.135544 \nL 357.058982 229.611768 \nL 357.6498 229.955781 \nL 358.240618 231.004998 \nL 358.536027 231.16223 \nL 358.831436 231.988082 \nL 359.422254 231.291165 \nL 359.717663 231.490645 \nL 360.013073 231.289366 \nL 360.308482 231.338714 \nL 360.603891 231.134046 \nL 360.8993 231.292772 \nL 361.194709 231.048451 \nL 361.490118 231.453987 \nL 361.785527 231.273447 \nL 362.080936 231.571975 \nL 362.376345 232.174413 \nL 362.671754 232.106704 \nL 362.967163 232.2276 \nL 363.85339 231.331967 \nL 364.444208 231.511768 \nL 364.739617 231.931811 \nL 365.035026 231.698782 \nL 365.625844 231.891796 \nL 366.216663 231.105252 \nL 366.512072 231.176446 \nL 366.807481 231.679024 \nL 367.10289 231.651418 \nL 367.693708 231.775952 \nL 367.989117 231.380424 \nL 368.284526 231.797719 \nL 368.579935 231.940589 \nL 369.170753 231.917305 \nL 369.761571 232.061589 \nL 370.05698 231.235505 \nL 370.352389 231.062587 \nL 371.534025 229.437244 \nL 371.829435 228.858051 \nL 372.124844 229.13674 \nL 372.420253 229.25374 \nL 372.715662 228.700867 \nL 373.011071 228.914989 \nL 373.30648 228.735991 \nL 373.601889 228.71155 \nL 373.897298 229.124564 \nL 374.783525 228.837369 \nL 375.078934 228.949582 \nL 375.669752 228.699269 \nL 375.965161 229.358395 \nL 376.26057 229.721235 \nL 376.555979 229.729989 \nL 377.146797 228.741059 \nL 377.442206 228.60157 \nL 377.737615 228.765741 \nL 378.328434 228.290954 \nL 378.623843 228.089924 \nL 378.919252 228.921639 \nL 379.214661 228.989661 \nL 379.51007 229.643944 \nL 379.805479 230.015337 \nL 380.100888 230.085439 \nL 380.396297 229.635487 \nL 380.987115 230.611743 \nL 381.282524 231.082731 \nL 381.873342 229.967026 \nL 382.168751 230.068026 \nL 382.46416 229.97566 \nL 383.350387 230.470085 \nL 383.941206 229.743611 \nL 384.236615 230.012365 \nL 384.532024 231.140954 \nL 384.827433 230.871501 \nL 385.122842 230.779311 \nL 385.418251 230.942374 \nL 385.71366 230.827197 \nL 386.304478 229.866354 \nL 386.599887 229.843078 \nL 386.895296 230.183974 \nL 387.190705 230.726261 \nL 387.486114 230.54099 \nL 387.781523 230.719329 \nL 388.076932 230.164947 \nL 388.372341 230.19162 \nL 388.66775 230.029963 \nL 389.258568 231.133966 \nL 389.553977 230.710438 \nL 389.849387 230.879485 \nL 390.440205 230.737979 \nL 390.735614 230.924511 \nL 391.031023 230.945651 \nL 391.031023 230.945651 \n\" clip-path=\"url(#p0e73b5e31f)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 288.430125 \nL 50.14375 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 407.26375 288.430125 \nL 407.26375 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 288.430125 \nL 407.26375 288.430125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 407.26375 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- model loss -->\n    <g transform=\"translate(196.524063 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-6d\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"97.412109\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"158.59375\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"222.070312\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"283.59375\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"311.376953\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"343.164062\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"370.947266\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"432.128906\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"484.228516\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 57.14375 59.674375 \nL 138.378125 59.674375 \nQ 140.378125 59.674375 140.378125 57.674375 \nL 140.378125 29.318125 \nQ 140.378125 27.318125 138.378125 27.318125 \nL 57.14375 27.318125 \nQ 55.14375 27.318125 55.14375 29.318125 \nL 55.14375 57.674375 \nQ 55.14375 59.674375 57.14375 59.674375 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 59.14375 35.416562 \nL 69.14375 35.416562 \nL 79.14375 35.416562 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- train -->\n     <g transform=\"translate(87.14375 38.916562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 59.14375 50.094687 \nL 69.14375 50.094687 \nL 79.14375 50.094687 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- validation -->\n     <g transform=\"translate(87.14375 53.594687) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0e73b5e31f\">\n   <rect x=\"50.14375\" y=\"22.318125\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "7n3PeLwaeAAn",
        "outputId": "7de4a613-0f9e-46cd-af75-18adfe65ee59"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"408.10125pt\" height=\"325.986375pt\" viewBox=\"0 0 408.10125 325.986375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-12-17T18:42:36.065037</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 325.986375 \nL 408.10125 325.986375 \nL 408.10125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 288.430125 \nL 400.90125 288.430125 \nL 400.90125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m602586adf0\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m602586adf0\" x=\"60.013977\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(56.832727 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m602586adf0\" x=\"119.095787\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(109.552037 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m602586adf0\" x=\"178.177597\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(168.633847 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m602586adf0\" x=\"237.259407\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(227.715657 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m602586adf0\" x=\"296.341217\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(286.797467 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m602586adf0\" x=\"355.423027\" y=\"288.430125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <g transform=\"translate(342.698027 303.028562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(207.113125 316.706687) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m17c4960dde\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"286.695517\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.1 -->\n      <g transform=\"translate(20.878125 290.494735) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"255.426316\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 259.225535) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"224.157116\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.3 -->\n      <g transform=\"translate(20.878125 227.956334) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"192.887915\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 196.687134) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"161.618715\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 165.417933) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"130.349514\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 134.148733) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"99.080314\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.7 -->\n      <g transform=\"translate(20.878125 102.879533) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"67.811113\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 71.610332) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m17c4960dde\" x=\"43.78125\" y=\"36.541913\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.9 -->\n      <g transform=\"translate(20.878125 40.341132) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- acc -->\n     <g transform=\"translate(14.798438 163.936625) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 60.013977 276.334125 \nL 60.309386 247.42399 \nL 60.604795 246.498866 \nL 61.195613 222.676918 \nL 61.491023 219.438984 \nL 62.081841 208.800054 \nL 62.37725 207.874925 \nL 62.672659 203.018029 \nL 62.968068 200.011373 \nL 63.263477 194.229348 \nL 63.558886 193.072941 \nL 63.854295 193.304219 \nL 64.149704 184.978102 \nL 64.740522 181.277604 \nL 65.035931 181.740168 \nL 65.33134 182.434011 \nL 65.922158 175.958143 \nL 66.217567 176.883263 \nL 66.512976 175.958143 \nL 66.808385 169.944831 \nL 67.103794 171.79508 \nL 67.399204 170.176118 \nL 67.694613 164.394084 \nL 67.990022 171.332516 \nL 68.285431 164.394084 \nL 68.58084 164.394084 \nL 68.876249 161.618715 \nL 69.171658 163.931529 \nL 69.467067 153.292589 \nL 69.762476 161.618715 \nL 70.057885 156.530532 \nL 70.353294 159.768475 \nL 70.648703 157.224383 \nL 70.944112 157.686939 \nL 71.239521 148.435702 \nL 71.53493 156.530532 \nL 71.830339 153.755163 \nL 72.125748 147.047999 \nL 72.421157 153.755163 \nL 72.716566 148.204406 \nL 73.011975 150.51722 \nL 73.307385 148.898258 \nL 73.602794 143.578797 \nL 73.898203 152.367478 \nL 74.193612 144.966482 \nL 74.489021 143.116223 \nL 74.78443 147.510573 \nL 75.079839 141.959816 \nL 75.375248 144.735204 \nL 75.670657 139.878299 \nL 75.966066 147.741851 \nL 76.261475 144.735204 \nL 76.556884 142.884946 \nL 77.147702 132.70858 \nL 77.443111 140.803409 \nL 77.73852 138.02804 \nL 78.033929 137.334207 \nL 78.329338 130.85834 \nL 78.624747 138.259318 \nL 78.920156 128.776804 \nL 79.215565 138.02804 \nL 79.510975 136.640356 \nL 79.806384 129.933211 \nL 80.101793 132.939858 \nL 80.397202 124.61375 \nL 80.692611 132.477302 \nL 80.98802 128.314249 \nL 81.283429 130.164488 \nL 81.578838 127.157842 \nL 81.874247 125.307583 \nL 82.169656 126.001435 \nL 82.465065 131.089618 \nL 82.760474 127.157842 \nL 83.055883 117.444031 \nL 83.351292 126.001435 \nL 83.646701 120.450678 \nL 83.94211 126.926545 \nL 84.237519 124.61375 \nL 84.532928 124.151176 \nL 84.828337 124.845028 \nL 85.123746 125.076306 \nL 85.419156 120.913252 \nL 85.714565 119.062994 \nL 86.009974 120.2194 \nL 86.305383 116.287624 \nL 86.600792 119.062994 \nL 86.896201 115.593773 \nL 87.19161 119.062994 \nL 87.487019 117.444031 \nL 87.782428 117.675309 \nL 88.077837 121.375807 \nL 88.373246 113.280978 \nL 88.668655 117.444031 \nL 89.259473 108.424073 \nL 89.554882 112.818404 \nL 89.850291 111.893293 \nL 90.1457 112.355848 \nL 90.736518 115.362495 \nL 91.031927 105.417407 \nL 91.327337 111.430719 \nL 91.622746 107.267666 \nL 91.918155 120.2194 \nL 92.213564 107.730221 \nL 92.804382 114.437385 \nL 93.3952 110.043035 \nL 93.690609 106.80511 \nL 93.986018 108.424073 \nL 94.281427 104.954852 \nL 94.576836 110.043035 \nL 94.872245 110.50559 \nL 95.167654 103.104612 \nL 95.463063 108.192795 \nL 95.758472 105.648703 \nL 96.053881 106.573814 \nL 96.34929 101.716909 \nL 96.644699 107.961499 \nL 96.940108 103.104612 \nL 97.235518 103.104612 \nL 97.530927 104.029723 \nL 97.826336 101.948205 \nL 98.121745 101.948205 \nL 98.417154 103.798445 \nL 98.712563 107.036388 \nL 99.007972 95.703615 \nL 99.303381 102.873316 \nL 99.59879 97.0913 \nL 99.894199 96.166171 \nL 100.189608 96.628726 \nL 100.485017 99.635391 \nL 100.780426 97.785133 \nL 101.075835 102.642038 \nL 101.371244 99.172817 \nL 101.666653 92.928246 \nL 101.962062 102.41076 \nL 102.257471 102.179483 \nL 102.55288 101.485631 \nL 102.848289 92.465672 \nL 103.143699 93.390802 \nL 103.439108 99.866669 \nL 103.734517 93.390802 \nL 104.325335 103.567167 \nL 104.620744 94.084635 \nL 104.916153 96.628726 \nL 105.211562 93.853357 \nL 105.506971 97.322578 \nL 105.80238 91.771839 \nL 106.097789 91.309266 \nL 106.393198 88.302619 \nL 106.688607 97.553855 \nL 106.984016 98.016429 \nL 107.279425 96.628726 \nL 107.574834 88.302619 \nL 107.870243 101.948205 \nL 108.165652 95.472338 \nL 108.461061 95.703615 \nL 108.75647 98.016429 \nL 109.051879 85.295953 \nL 109.347289 90.615433 \nL 109.642698 91.077988 \nL 109.938107 88.302619 \nL 110.233516 96.166171 \nL 110.528925 87.146212 \nL 110.824334 91.771839 \nL 111.119743 86.914934 \nL 111.710561 96.860022 \nL 112.00597 88.071341 \nL 112.301379 91.771839 \nL 112.596788 91.309266 \nL 112.892197 85.52725 \nL 113.187606 86.914934 \nL 113.483015 91.540543 \nL 113.778424 89.921581 \nL 114.073833 83.214436 \nL 114.369242 88.533896 \nL 114.664651 82.289307 \nL 114.96006 92.003117 \nL 115.550879 90.384155 \nL 115.846288 91.540543 \nL 116.141697 84.370843 \nL 116.437106 89.459026 \nL 116.732515 89.690303 \nL 117.027924 83.676991 \nL 117.323333 86.914934 \nL 117.618742 85.52725 \nL 117.914151 80.670345 \nL 118.20956 84.370843 \nL 118.504969 79.976493 \nL 118.800378 83.214436 \nL 119.095787 79.976493 \nL 119.391196 86.221083 \nL 119.686605 76.275995 \nL 119.982014 86.683657 \nL 120.277423 85.758527 \nL 120.572832 84.370843 \nL 121.163651 75.813439 \nL 121.45906 84.370843 \nL 121.754469 84.833398 \nL 122.049878 86.45236 \nL 122.345287 82.058029 \nL 122.640696 83.445714 \nL 122.936105 80.670345 \nL 123.231514 80.901622 \nL 123.526923 86.683657 \nL 123.822332 80.670345 \nL 124.117741 79.28266 \nL 124.41315 79.513938 \nL 124.708559 80.670345 \nL 125.003968 80.901622 \nL 125.299377 84.139565 \nL 125.594786 80.207771 \nL 125.890195 80.439067 \nL 126.185604 79.051382 \nL 126.481013 79.745215 \nL 126.776422 81.595474 \nL 127.071832 79.051382 \nL 127.367241 81.364177 \nL 127.66265 72.112941 \nL 128.253468 81.364177 \nL 128.548877 76.044717 \nL 128.844286 81.826751 \nL 129.139695 75.350884 \nL 129.435104 82.058029 \nL 129.730513 71.650386 \nL 130.025922 80.439067 \nL 130.321331 78.820086 \nL 130.61674 74.657032 \nL 130.912149 77.432402 \nL 131.207558 72.806793 \nL 131.502967 77.201124 \nL 131.798376 73.731903 \nL 132.093785 76.969846 \nL 132.389194 74.425755 \nL 132.684603 76.738569 \nL 132.980013 69.56885 \nL 133.275422 76.275995 \nL 133.570831 72.112941 \nL 133.86624 71.187812 \nL 134.161649 81.595474 \nL 134.457058 70.956534 \nL 134.752467 77.663679 \nL 135.047876 72.112941 \nL 135.343285 77.663679 \nL 135.638694 74.425755 \nL 135.934103 75.582162 \nL 136.229512 74.425755 \nL 136.524921 70.956534 \nL 136.82033 76.969846 \nL 137.115739 75.119588 \nL 137.411148 75.350884 \nL 137.706557 73.269348 \nL 138.001966 76.044717 \nL 138.297375 63.092982 \nL 138.592784 76.969846 \nL 139.183603 70.956534 \nL 139.479012 70.725256 \nL 139.774421 70.262701 \nL 140.06983 71.650386 \nL 140.365239 69.106294 \nL 140.956057 73.269348 \nL 141.546875 68.412443 \nL 141.842284 69.800127 \nL 142.137693 70.031405 \nL 142.433102 67.949887 \nL 142.728511 72.806793 \nL 143.02392 66.562203 \nL 143.319329 72.806793 \nL 143.614738 68.874998 \nL 143.910147 73.500626 \nL 144.205556 66.793481 \nL 144.500965 70.262701 \nL 144.796374 70.031405 \nL 145.091784 73.731903 \nL 145.387193 71.419089 \nL 145.682602 74.88831 \nL 145.978011 67.949887 \nL 146.27342 70.493979 \nL 146.568829 67.71861 \nL 146.864238 70.031405 \nL 147.159647 70.956534 \nL 147.455056 64.018111 \nL 147.750465 70.725256 \nL 148.045874 64.480667 \nL 148.341283 69.800127 \nL 148.636692 67.949887 \nL 148.932101 68.874998 \nL 149.22751 66.793481 \nL 149.522919 67.256036 \nL 149.818328 69.56885 \nL 150.113737 74.657032 \nL 150.409146 69.56885 \nL 150.704555 70.493979 \nL 150.999965 68.412443 \nL 151.295374 67.487313 \nL 151.590783 64.711944 \nL 151.886192 68.874998 \nL 152.181601 70.031405 \nL 152.47701 72.344219 \nL 152.772419 66.793481 \nL 153.067828 65.637074 \nL 153.363237 65.174518 \nL 153.658646 67.487313 \nL 153.954055 66.562203 \nL 154.249464 67.024758 \nL 154.544873 66.562203 \nL 154.840282 69.106294 \nL 155.135691 61.242724 \nL 155.4311 64.249389 \nL 155.726509 65.174518 \nL 156.021918 69.337572 \nL 156.317327 67.487313 \nL 156.612736 63.092982 \nL 156.908146 67.256036 \nL 157.203555 66.562203 \nL 157.498964 64.480667 \nL 157.794373 63.555538 \nL 158.089782 69.800127 \nL 158.385191 65.405796 \nL 158.6806 64.018111 \nL 158.976009 67.487313 \nL 159.271418 63.092982 \nL 159.862236 66.562203 \nL 160.157645 64.480667 \nL 160.453054 63.555538 \nL 160.748463 63.32426 \nL 161.043872 64.943222 \nL 161.339281 63.32426 \nL 161.63469 66.562203 \nL 161.930099 62.167853 \nL 162.225508 65.868351 \nL 162.520917 64.711944 \nL 162.816327 65.174518 \nL 163.111736 63.786815 \nL 163.407145 60.548891 \nL 163.702554 67.024758 \nL 163.997963 65.405796 \nL 164.293372 61.705298 \nL 164.588781 64.711944 \nL 164.88419 58.236077 \nL 165.179599 63.32426 \nL 165.475008 65.868351 \nL 165.770417 65.637074 \nL 166.065826 66.562203 \nL 166.361235 61.936575 \nL 166.656644 63.555538 \nL 166.952053 60.548891 \nL 167.247462 63.32426 \nL 167.542871 64.943222 \nL 167.83828 59.161206 \nL 168.133689 57.310948 \nL 168.429098 59.623762 \nL 168.724508 64.480667 \nL 169.019917 64.711944 \nL 169.315326 66.330907 \nL 169.610735 57.310948 \nL 169.906144 62.861705 \nL 170.201553 64.711944 \nL 170.496962 58.929929 \nL 170.792371 58.929929 \nL 171.08778 63.555538 \nL 171.678598 67.256036 \nL 171.974007 58.236077 \nL 172.269416 64.249389 \nL 172.564825 62.167853 \nL 172.860234 57.310948 \nL 173.155643 64.711944 \nL 173.451052 61.47402 \nL 173.746461 60.548891 \nL 174.04187 58.698632 \nL 174.337279 59.855039 \nL 174.632688 65.174518 \nL 174.928098 60.086317 \nL 175.223507 58.698632 \nL 175.814325 59.161206 \nL 176.109734 59.161206 \nL 176.405143 59.623762 \nL 176.700552 58.698632 \nL 176.995961 58.467355 \nL 177.29137 61.705298 \nL 177.586779 52.916617 \nL 178.177597 62.167853 \nL 178.473006 61.47402 \nL 178.768415 58.236077 \nL 179.063824 57.310948 \nL 179.359233 57.773522 \nL 179.654642 61.705298 \nL 179.950051 59.855039 \nL 180.24546 59.161206 \nL 180.540869 60.548891 \nL 180.836279 60.086317 \nL 181.131688 56.154541 \nL 181.427097 62.399131 \nL 181.722506 59.855039 \nL 182.017915 55.691986 \nL 182.313324 60.317613 \nL 182.608733 57.542225 \nL 182.904142 53.147894 \nL 183.199551 60.086317 \nL 183.790369 56.385837 \nL 184.085778 58.236077 \nL 184.381187 58.004799 \nL 184.676596 61.011446 \nL 184.972005 55.923263 \nL 185.267414 61.242724 \nL 185.562823 56.385837 \nL 185.858232 58.467355 \nL 186.153641 63.32426 \nL 186.44905 61.242724 \nL 186.74446 58.236077 \nL 187.039869 58.698632 \nL 187.335278 61.242724 \nL 187.630687 55.22943 \nL 187.926096 60.548891 \nL 188.221505 54.998134 \nL 188.516914 61.936575 \nL 188.812323 57.07967 \nL 189.107732 59.855039 \nL 189.403141 57.310948 \nL 189.69855 60.317613 \nL 189.993959 58.929929 \nL 190.289368 53.147894 \nL 190.584777 57.773522 \nL 190.880186 57.310948 \nL 191.175595 54.304301 \nL 191.471004 58.467355 \nL 192.061822 54.766856 \nL 192.357231 62.167853 \nL 192.652641 54.998134 \nL 192.94805 57.542225 \nL 193.243459 54.535579 \nL 193.538868 55.460708 \nL 193.834277 52.222765 \nL 194.129686 57.310948 \nL 194.425095 58.004799 \nL 194.720504 56.154541 \nL 195.015913 57.310948 \nL 195.311322 54.766856 \nL 195.606731 55.460708 \nL 195.90214 56.848392 \nL 196.197549 54.766856 \nL 196.492958 58.004799 \nL 196.788367 52.685339 \nL 197.083776 56.617115 \nL 197.379185 56.154541 \nL 197.674594 59.855039 \nL 197.970003 53.379172 \nL 198.265412 54.073023 \nL 198.560822 54.073023 \nL 198.856231 56.617115 \nL 199.15164 56.617115 \nL 199.447049 54.998134 \nL 199.742458 50.141247 \nL 200.037867 52.685339 \nL 200.333276 53.841746 \nL 200.628685 51.76021 \nL 200.924094 55.691986 \nL 201.219503 54.766856 \nL 201.514912 54.535579 \nL 201.810321 54.073023 \nL 202.10573 50.603803 \nL 202.401139 54.304301 \nL 202.696548 60.548891 \nL 202.991957 53.379172 \nL 203.287366 56.848392 \nL 203.582775 56.617115 \nL 203.878184 55.460708 \nL 204.173593 53.147894 \nL 204.469003 52.916617 \nL 205.059821 60.086317 \nL 205.35523 54.073023 \nL 205.650639 52.916617 \nL 205.946048 54.073023 \nL 206.241457 54.535579 \nL 206.536866 55.460708 \nL 206.832275 57.07967 \nL 207.127684 55.22943 \nL 207.423093 56.385837 \nL 207.718502 54.073023 \nL 208.013911 50.83508 \nL 208.30932 55.460708 \nL 208.900138 51.297654 \nL 209.195547 57.542225 \nL 209.490956 51.76021 \nL 209.786365 52.916617 \nL 210.081774 50.603803 \nL 210.377183 53.610449 \nL 210.672593 54.535579 \nL 210.968002 53.610449 \nL 211.263411 58.467355 \nL 211.55882 52.685339 \nL 211.854229 50.372525 \nL 212.149638 57.542225 \nL 212.445047 55.923263 \nL 212.740456 50.372525 \nL 213.035865 54.766856 \nL 213.331274 52.916617 \nL 213.626683 52.454043 \nL 213.922092 54.535579 \nL 214.217501 52.916617 \nL 214.51291 54.766856 \nL 214.808319 47.828434 \nL 215.103728 55.22943 \nL 215.399137 54.304301 \nL 215.694546 55.22943 \nL 215.989955 51.066358 \nL 216.580774 58.004799 \nL 216.876183 50.141247 \nL 217.171592 55.22943 \nL 217.467001 52.454043 \nL 217.76241 53.610449 \nL 218.057819 59.855039 \nL 218.353228 51.297654 \nL 218.648637 50.141247 \nL 218.944046 54.535579 \nL 219.239455 49.909951 \nL 219.534864 49.678674 \nL 219.830273 51.528932 \nL 220.125682 49.216118 \nL 220.421091 50.83508 \nL 220.7165 55.923263 \nL 221.011909 49.447396 \nL 221.307318 53.379172 \nL 221.602727 54.766856 \nL 221.898136 51.297654 \nL 222.193545 50.141247 \nL 222.488955 55.691986 \nL 223.375182 46.672027 \nL 223.670591 58.929929 \nL 223.966 47.36586 \nL 224.556818 56.848392 \nL 224.852227 55.22943 \nL 225.147636 48.059711 \nL 225.443045 50.83508 \nL 225.738454 52.222765 \nL 226.033863 51.991487 \nL 226.329272 48.984841 \nL 226.624681 51.528932 \nL 226.92009 49.447396 \nL 227.510908 55.22943 \nL 227.806317 47.828434 \nL 228.101726 47.828434 \nL 228.397136 49.216118 \nL 228.692545 47.597156 \nL 228.987954 48.290989 \nL 229.283363 51.297654 \nL 229.578772 51.528932 \nL 229.874181 50.141247 \nL 230.16959 51.066358 \nL 230.464999 49.909951 \nL 230.760408 50.83508 \nL 231.055817 51.297654 \nL 231.351226 51.991487 \nL 231.646635 50.603803 \nL 231.942044 50.141247 \nL 232.532862 52.916617 \nL 232.828271 51.297654 \nL 233.12368 51.76021 \nL 233.419089 54.073023 \nL 233.714498 58.929929 \nL 234.009907 49.909951 \nL 234.305317 45.978175 \nL 234.600726 51.297654 \nL 234.896135 45.51562 \nL 235.191544 50.141247 \nL 235.486953 51.991487 \nL 236.077771 49.909951 \nL 236.668589 50.372525 \nL 236.963998 51.066358 \nL 237.259407 44.821768 \nL 237.554816 48.984841 \nL 238.145634 52.685339 \nL 238.736452 45.51562 \nL 239.031861 45.284342 \nL 239.622679 50.603803 \nL 239.918088 51.066358 \nL 240.213497 50.603803 \nL 241.099725 44.821768 \nL 241.690543 52.916617 \nL 241.985952 54.535579 \nL 242.281361 48.753544 \nL 242.57677 52.685339 \nL 242.872179 48.522267 \nL 243.167588 48.984841 \nL 243.462997 52.454043 \nL 243.758406 51.528932 \nL 244.053815 52.454043 \nL 244.349224 49.447396 \nL 244.644633 52.454043 \nL 245.235451 43.665361 \nL 245.53086 45.746898 \nL 246.121678 51.991487 \nL 246.417088 50.603803 \nL 246.712497 44.821768 \nL 247.007906 50.372525 \nL 247.303315 51.76021 \nL 247.894133 48.059711 \nL 248.189542 51.066358 \nL 248.484951 44.127935 \nL 249.075769 48.290989 \nL 249.371178 47.134582 \nL 249.666587 51.528932 \nL 249.961996 47.36586 \nL 250.257405 48.522267 \nL 250.552814 47.134582 \nL 250.848223 44.127935 \nL 251.143632 48.984841 \nL 251.439041 50.372525 \nL 251.73445 43.434084 \nL 252.029859 45.978175 \nL 252.620678 48.290989 \nL 252.916087 50.141247 \nL 253.211496 42.740251 \nL 253.506905 43.896658 \nL 253.802314 50.372525 \nL 254.097723 46.672027 \nL 254.393132 52.916617 \nL 254.688541 45.053065 \nL 254.98395 43.896658 \nL 255.279359 46.440749 \nL 255.574768 47.36586 \nL 255.870177 45.284342 \nL 256.165586 48.753544 \nL 256.460995 48.059711 \nL 256.756404 51.297654 \nL 257.051813 47.134582 \nL 257.347222 45.978175 \nL 257.642631 47.134582 \nL 257.93804 49.909951 \nL 258.23345 45.51562 \nL 258.528859 46.672027 \nL 258.824268 48.753544 \nL 259.119677 51.76021 \nL 259.415086 44.127935 \nL 259.710495 45.978175 \nL 260.005904 45.284342 \nL 260.301313 46.209453 \nL 260.596722 44.127935 \nL 260.892131 48.290989 \nL 261.18754 47.36586 \nL 261.482949 39.964882 \nL 261.778358 48.522267 \nL 262.073767 51.528932 \nL 262.369176 49.447396 \nL 262.664585 45.053065 \nL 262.959994 51.297654 \nL 263.550812 44.590491 \nL 263.846221 50.372525 \nL 264.141631 49.216118 \nL 264.43704 43.202806 \nL 264.732449 45.284342 \nL 265.027858 43.896658 \nL 265.323267 47.134582 \nL 265.618676 47.36586 \nL 265.914085 50.372525 \nL 266.209494 45.053065 \nL 266.504903 43.434084 \nL 266.800312 43.665361 \nL 267.095721 52.222765 \nL 268.277357 41.12127 \nL 268.572766 42.740251 \nL 268.868175 45.053065 \nL 269.163584 50.83508 \nL 269.458993 46.672027 \nL 269.754402 49.447396 \nL 270.049812 49.447396 \nL 270.64063 44.821768 \nL 270.936039 45.978175 \nL 271.231448 48.984841 \nL 271.822266 40.658715 \nL 272.117675 44.590491 \nL 272.413084 43.896658 \nL 272.708493 41.583844 \nL 273.299311 47.597156 \nL 273.59472 44.590491 \nL 273.890129 45.053065 \nL 274.185538 52.685339 \nL 274.480947 44.590491 \nL 274.776356 46.903304 \nL 275.071765 45.746898 \nL 275.367174 44.127935 \nL 275.957992 49.447396 \nL 276.253402 43.202806 \nL 276.548811 48.059711 \nL 276.84422 44.590491 \nL 277.139629 44.127935 \nL 277.435038 50.83508 \nL 277.730447 52.222765 \nL 278.025856 45.746898 \nL 278.321265 44.359213 \nL 278.616674 49.909951 \nL 278.912083 50.83508 \nL 279.207492 49.678674 \nL 279.502901 40.427437 \nL 279.79831 45.51562 \nL 280.093719 44.821768 \nL 280.389128 43.202806 \nL 280.684537 50.603803 \nL 280.979946 44.359213 \nL 281.275355 53.147894 \nL 281.570764 43.896658 \nL 281.866173 46.440749 \nL 282.161583 42.740251 \nL 282.456992 47.597156 \nL 282.752401 42.971529 \nL 283.04781 49.447396 \nL 283.638628 47.828434 \nL 283.934037 41.352566 \nL 284.524855 46.672027 \nL 284.820264 45.51562 \nL 285.115673 42.046399 \nL 285.411082 41.815122 \nL 285.706491 47.597156 \nL 286.0019 41.583844 \nL 286.297309 43.202806 \nL 286.592718 42.046399 \nL 286.888127 43.434084 \nL 287.183536 45.978175 \nL 287.774354 41.583844 \nL 288.365173 47.134582 \nL 288.660582 44.127935 \nL 288.955991 49.216118 \nL 289.2514 46.672027 \nL 289.546809 42.277677 \nL 289.842218 43.434084 \nL 290.137627 46.672027 \nL 290.433036 45.053065 \nL 290.728445 44.127935 \nL 291.023854 45.746898 \nL 291.319263 42.046399 \nL 291.614672 42.508973 \nL 291.910081 46.209453 \nL 292.20549 41.815122 \nL 292.500899 44.590491 \nL 292.796308 44.821768 \nL 293.091717 46.440749 \nL 293.977945 39.502308 \nL 294.273354 40.658715 \nL 294.568763 39.502308 \nL 295.159581 48.522267 \nL 295.45499 47.36586 \nL 295.750399 48.753544 \nL 296.045808 42.740251 \nL 296.341217 40.889992 \nL 296.636626 46.672027 \nL 296.932035 46.440749 \nL 297.227444 42.740251 \nL 297.522853 44.127935 \nL 297.818262 47.36586 \nL 298.113671 45.284342 \nL 298.40908 44.127935 \nL 298.704489 41.815122 \nL 298.999898 42.740251 \nL 299.295307 42.046399 \nL 299.590716 42.740251 \nL 299.886126 44.821768 \nL 300.181535 38.808475 \nL 300.476944 40.427437 \nL 300.772353 48.753544 \nL 301.067762 42.740251 \nL 301.363171 45.746898 \nL 301.65858 43.202806 \nL 301.953989 42.046399 \nL 302.249398 44.127935 \nL 302.544807 48.290989 \nL 302.840216 42.046399 \nL 303.135625 42.508973 \nL 303.431034 41.352566 \nL 303.726443 47.36586 \nL 304.021852 40.658715 \nL 304.317261 42.046399 \nL 304.61267 41.352566 \nL 304.908079 45.053065 \nL 305.498897 40.427437 \nL 305.794306 39.502308 \nL 306.089716 42.740251 \nL 306.385125 41.352566 \nL 306.680534 45.51562 \nL 306.975943 40.889992 \nL 307.271352 45.978175 \nL 307.566761 42.740251 \nL 307.86217 41.815122 \nL 308.157579 44.590491 \nL 308.748397 42.508973 \nL 309.043806 43.896658 \nL 309.339215 37.652068 \nL 309.634624 38.808475 \nL 309.930033 46.440749 \nL 310.225442 45.978175 \nL 310.520851 42.277677 \nL 310.81626 40.427437 \nL 311.111669 45.51562 \nL 311.407078 45.284342 \nL 311.702487 43.665361 \nL 311.997897 41.352566 \nL 312.293306 41.583844 \nL 312.588715 43.434084 \nL 312.884124 42.740251 \nL 313.179533 43.202806 \nL 313.770351 46.903304 \nL 314.06576 46.440749 \nL 314.361169 45.053065 \nL 314.656578 39.039753 \nL 315.247396 44.359213 \nL 315.542805 40.196159 \nL 315.838214 42.508973 \nL 316.133623 46.672027 \nL 316.429032 45.746898 \nL 316.724441 43.202806 \nL 317.01985 49.216118 \nL 317.315259 42.740251 \nL 317.610668 40.427437 \nL 317.906078 47.36586 \nL 318.201487 37.420772 \nL 318.496896 39.27103 \nL 318.792305 44.127935 \nL 319.087714 43.434084 \nL 319.383123 45.978175 \nL 319.678532 42.508973 \nL 319.973941 43.896658 \nL 320.26935 43.202806 \nL 320.564759 45.746898 \nL 320.860168 38.808475 \nL 321.155577 45.746898 \nL 321.450986 43.434084 \nL 321.746395 36.495661 \nL 322.041804 43.434084 \nL 322.337213 41.583844 \nL 322.632622 35.570532 \nL 322.928031 40.196159 \nL 323.22344 39.733585 \nL 323.518849 45.746898 \nL 323.814259 40.427437 \nL 324.109668 42.277677 \nL 324.405077 41.815122 \nL 324.700486 42.971529 \nL 324.995895 45.978175 \nL 325.291304 36.033087 \nL 325.586713 48.522267 \nL 325.882122 41.815122 \nL 326.177531 45.284342 \nL 326.47294 42.277677 \nL 326.768349 43.434084 \nL 327.063758 42.508973 \nL 327.359167 37.883346 \nL 327.654576 44.359213 \nL 327.949985 40.427437 \nL 328.245394 43.434084 \nL 328.540803 43.434084 \nL 328.836212 42.971529 \nL 329.131621 38.114623 \nL 329.42703 38.808475 \nL 329.72244 42.277677 \nL 330.017849 42.740251 \nL 330.313258 42.046399 \nL 330.608667 42.046399 \nL 330.904076 43.202806 \nL 331.199485 38.577179 \nL 331.494894 41.583844 \nL 331.790303 43.202806 \nL 332.381121 39.964882 \nL 332.971939 40.427437 \nL 333.267348 41.815122 \nL 333.562757 41.352566 \nL 333.858166 37.420772 \nL 334.153575 43.896658 \nL 334.448984 43.202806 \nL 334.744393 43.434084 \nL 335.039802 43.434084 \nL 335.335211 42.046399 \nL 335.630621 42.046399 \nL 335.92603 43.434084 \nL 336.221439 40.889992 \nL 336.516848 44.590491 \nL 336.812257 36.958216 \nL 337.107666 38.577179 \nL 337.403075 42.508973 \nL 337.698484 44.821768 \nL 337.993893 42.740251 \nL 338.289302 38.577179 \nL 338.584711 41.352566 \nL 338.88012 46.672027 \nL 339.175529 38.114623 \nL 339.470938 43.896658 \nL 339.766347 41.815122 \nL 340.061756 41.352566 \nL 340.357165 41.12127 \nL 340.652574 39.964882 \nL 340.947983 43.665361 \nL 341.243392 41.12127 \nL 341.538801 42.508973 \nL 341.834211 41.583844 \nL 342.12962 38.345901 \nL 342.720438 42.971529 \nL 343.015847 37.420772 \nL 343.311256 40.196159 \nL 343.606665 44.821768 \nL 343.902074 39.502308 \nL 344.197483 39.964882 \nL 344.492892 42.046399 \nL 344.788301 40.427437 \nL 345.08371 40.889992 \nL 345.379119 42.971529 \nL 345.674528 43.202806 \nL 345.969937 44.590491 \nL 346.265346 38.114623 \nL 346.560755 41.352566 \nL 346.856164 41.12127 \nL 347.151573 43.202806 \nL 347.446982 41.815122 \nL 347.742392 38.808475 \nL 348.037801 39.27103 \nL 348.33321 41.352566 \nL 348.628619 41.583844 \nL 348.924028 41.12127 \nL 349.219437 43.434084 \nL 349.514846 42.508973 \nL 349.810255 44.359213 \nL 350.105664 41.815122 \nL 350.401073 41.583844 \nL 350.696482 43.202806 \nL 350.991891 39.27103 \nL 351.2873 42.277677 \nL 351.582709 39.964882 \nL 351.878118 42.740251 \nL 352.173527 44.127935 \nL 352.468936 40.196159 \nL 353.059754 41.12127 \nL 353.355163 39.039753 \nL 353.945982 37.420772 \nL 354.241391 41.12127 \nL 354.5368 38.577179 \nL 354.832209 41.12127 \nL 355.127618 37.652068 \nL 355.423027 42.971529 \nL 355.718436 41.352566 \nL 356.013845 41.352566 \nL 356.309254 42.508973 \nL 356.604663 39.502308 \nL 356.900072 43.434084 \nL 357.195481 42.277677 \nL 357.49089 36.726939 \nL 357.786299 42.740251 \nL 358.081708 38.577179 \nL 358.377117 39.964882 \nL 358.672526 39.039753 \nL 358.967935 40.427437 \nL 359.263344 38.577179 \nL 359.558754 43.896658 \nL 359.854163 39.733585 \nL 360.149572 43.665361 \nL 360.74039 38.345901 \nL 361.035799 40.427437 \nL 361.331208 37.420772 \nL 361.626617 41.815122 \nL 361.922026 38.577179 \nL 362.217435 41.583844 \nL 362.512844 42.508973 \nL 362.808253 42.971529 \nL 363.103662 42.740251 \nL 363.399071 38.808475 \nL 363.69448 38.808475 \nL 363.989889 44.821768 \nL 364.285298 40.658715 \nL 364.876116 42.740251 \nL 365.466935 40.658715 \nL 365.762344 38.114623 \nL 366.057753 43.896658 \nL 366.94398 37.189494 \nL 367.239389 41.352566 \nL 367.534798 38.577179 \nL 367.830207 37.652068 \nL 368.125616 34.414125 \nL 368.716434 40.196159 \nL 369.011843 42.508973 \nL 369.307252 40.196159 \nL 369.602661 39.964882 \nL 369.89807 37.420772 \nL 370.193479 43.896658 \nL 370.488888 39.733585 \nL 370.784297 41.12127 \nL 371.079706 35.339254 \nL 371.375115 41.12127 \nL 371.670525 38.808475 \nL 371.965934 43.896658 \nL 372.261343 40.889992 \nL 372.556752 42.277677 \nL 372.852161 38.577179 \nL 373.14757 42.277677 \nL 373.442979 41.352566 \nL 373.738388 39.964882 \nL 374.033797 35.80181 \nL 374.329206 39.039753 \nL 374.624615 44.590491 \nL 374.920024 42.740251 \nL 375.215433 38.345901 \nL 375.510842 40.427437 \nL 375.806251 37.883346 \nL 376.10166 41.352566 \nL 376.692478 37.652068 \nL 376.987887 40.658715 \nL 377.283296 35.339254 \nL 377.578706 40.196159 \nL 377.874115 41.352566 \nL 378.169524 36.726939 \nL 378.464933 42.277677 \nL 378.760342 41.352566 \nL 379.055751 38.577179 \nL 379.35116 37.420772 \nL 379.646569 42.046399 \nL 379.941978 42.046399 \nL 380.237387 34.87668 \nL 380.532796 42.971529 \nL 380.828205 35.570532 \nL 381.123614 42.046399 \nL 381.419023 39.733585 \nL 381.714432 42.740251 \nL 382.009841 39.502308 \nL 382.30525 37.652068 \nL 382.600659 38.114623 \nL 383.191477 41.12127 \nL 383.486887 40.196159 \nL 383.782296 40.427437 \nL 384.077705 42.508973 \nL 384.373114 37.883346 \nL 384.668523 42.740251 \nL 384.668523 42.740251 \n\" clip-path=\"url(#pe73b653f20)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 60.013977 245.146029 \nL 60.309386 206.594958 \nL 60.604795 219.445321 \nL 60.900204 202.311513 \nL 61.195613 198.028059 \nL 61.491023 189.46115 \nL 62.081841 198.028059 \nL 62.37725 189.46115 \nL 62.672659 189.46115 \nL 62.968068 193.744604 \nL 63.558886 172.327341 \nL 63.854295 168.043896 \nL 64.149704 159.476997 \nL 64.445113 189.46115 \nL 64.740522 163.760442 \nL 65.035931 168.043896 \nL 65.33134 168.043896 \nL 65.626749 172.327341 \nL 65.922158 155.193542 \nL 66.808385 142.343179 \nL 67.103794 150.910088 \nL 67.399204 133.77627 \nL 67.694613 142.343179 \nL 67.990022 142.343179 \nL 68.285431 138.059725 \nL 68.58084 138.059725 \nL 68.876249 142.343179 \nL 69.171658 133.77627 \nL 69.467067 142.343179 \nL 69.762476 138.059725 \nL 70.057885 138.059725 \nL 70.353294 125.20938 \nL 70.648703 142.343179 \nL 71.53493 142.343179 \nL 71.830339 133.77627 \nL 72.125748 129.492816 \nL 72.421157 133.77627 \nL 72.716566 142.343179 \nL 73.011975 133.77627 \nL 73.307385 129.492816 \nL 73.602794 129.492816 \nL 73.898203 138.059725 \nL 74.193612 125.20938 \nL 74.489021 138.059725 \nL 74.78443 129.492816 \nL 75.079839 133.77627 \nL 75.375248 116.642471 \nL 75.670657 129.492816 \nL 75.966066 116.642471 \nL 76.261475 125.20938 \nL 76.556884 125.20938 \nL 76.852293 112.359017 \nL 77.443111 129.492816 \nL 77.73852 133.77627 \nL 78.033929 120.925926 \nL 78.329338 116.642471 \nL 78.624747 120.925926 \nL 78.920156 120.925926 \nL 79.215565 116.642471 \nL 79.510975 108.075563 \nL 79.806384 125.20938 \nL 80.101793 133.77627 \nL 80.397202 120.925926 \nL 80.98802 120.925926 \nL 81.283429 138.059725 \nL 81.578838 120.925926 \nL 81.874247 133.77627 \nL 82.169656 125.20938 \nL 82.465065 120.925926 \nL 83.351292 120.925926 \nL 83.94211 112.359017 \nL 84.237519 116.642471 \nL 84.532928 112.359017 \nL 84.828337 116.642471 \nL 85.123746 116.642471 \nL 85.419156 120.925926 \nL 85.714565 103.792108 \nL 86.009974 108.075563 \nL 86.305383 108.075563 \nL 86.600792 116.642471 \nL 86.896201 112.359017 \nL 87.19161 112.359017 \nL 87.487019 108.075563 \nL 88.077837 116.642471 \nL 88.373246 112.359017 \nL 89.259473 112.359017 \nL 89.554882 103.792108 \nL 89.850291 112.359017 \nL 90.1457 99.508654 \nL 90.441109 103.792108 \nL 90.736518 112.359017 \nL 91.327337 112.359017 \nL 91.622746 108.075563 \nL 91.918155 112.359017 \nL 92.508973 112.359017 \nL 92.804382 108.075563 \nL 93.099791 112.359017 \nL 93.3952 108.075563 \nL 93.690609 112.359017 \nL 93.986018 95.225199 \nL 94.281427 108.075563 \nL 94.576836 103.792108 \nL 94.872245 95.225199 \nL 95.167654 108.075563 \nL 95.463063 112.359017 \nL 96.053881 103.792108 \nL 96.34929 112.359017 \nL 96.940108 103.792108 \nL 97.235518 112.359017 \nL 97.530927 116.642471 \nL 97.826336 108.075563 \nL 98.121745 95.225199 \nL 98.417154 103.792108 \nL 98.712563 95.225199 \nL 99.007972 99.508654 \nL 99.303381 108.075563 \nL 99.59879 112.359017 \nL 99.894199 108.075563 \nL 100.189608 99.508654 \nL 100.485017 95.225199 \nL 101.075835 95.225199 \nL 101.371244 86.658309 \nL 101.666653 95.225199 \nL 101.962062 95.225199 \nL 102.257471 116.642471 \nL 102.55288 112.359017 \nL 102.848289 103.792108 \nL 103.439108 95.225199 \nL 103.734517 108.075563 \nL 104.029926 99.508654 \nL 104.325335 103.792108 \nL 104.916153 103.792108 \nL 105.211562 95.225199 \nL 105.506971 99.508654 \nL 105.80238 95.225199 \nL 106.097789 103.792108 \nL 106.393198 103.792108 \nL 106.688607 99.508654 \nL 107.279425 78.0914 \nL 107.574834 90.941763 \nL 107.870243 82.374855 \nL 108.165652 90.941763 \nL 108.461061 95.225199 \nL 108.75647 108.075563 \nL 109.051879 99.508654 \nL 109.347289 95.225199 \nL 109.642698 95.225199 \nL 109.938107 99.508654 \nL 110.233516 95.225199 \nL 110.528925 99.508654 \nL 110.824334 90.941763 \nL 111.119743 90.941763 \nL 111.415152 95.225199 \nL 111.710561 86.658309 \nL 112.00597 86.658309 \nL 112.301379 90.941763 \nL 112.596788 90.941763 \nL 112.892197 86.658309 \nL 113.778424 86.658309 \nL 114.073833 90.941763 \nL 114.664651 90.941763 \nL 115.25547 82.374855 \nL 115.550879 95.225199 \nL 115.846288 86.658309 \nL 116.437106 86.658309 \nL 116.732515 82.374855 \nL 117.027924 90.941763 \nL 117.323333 82.374855 \nL 117.618742 78.0914 \nL 117.914151 86.658309 \nL 118.20956 90.941763 \nL 118.504969 78.0914 \nL 118.800378 86.658309 \nL 119.095787 86.658309 \nL 119.391196 82.374855 \nL 119.686605 82.374855 \nL 119.982014 78.0914 \nL 120.277423 82.374855 \nL 120.868241 82.374855 \nL 121.163651 90.941763 \nL 121.45906 82.374855 \nL 121.754469 82.374855 \nL 122.049878 69.524491 \nL 122.345287 82.374855 \nL 122.640696 78.0914 \nL 122.936105 82.374855 \nL 123.231514 73.807946 \nL 123.526923 78.0914 \nL 123.822332 73.807946 \nL 124.117741 78.0914 \nL 124.41315 69.524491 \nL 124.708559 78.0914 \nL 125.003968 65.241037 \nL 125.299377 78.0914 \nL 125.594786 65.241037 \nL 126.185604 86.658309 \nL 126.776422 86.658309 \nL 127.071832 78.0914 \nL 127.958059 78.0914 \nL 128.253468 73.807946 \nL 128.548877 73.807946 \nL 128.844286 86.658309 \nL 129.139695 82.374855 \nL 129.730513 82.374855 \nL 130.025922 78.0914 \nL 130.321331 78.0914 \nL 130.61674 73.807946 \nL 130.912149 73.807946 \nL 131.207558 69.524491 \nL 131.502967 69.524491 \nL 131.798376 78.0914 \nL 132.093785 69.524491 \nL 132.389194 69.524491 \nL 132.684603 65.241037 \nL 133.570831 78.0914 \nL 133.86624 78.0914 \nL 134.161649 69.524491 \nL 134.752467 78.0914 \nL 135.343285 78.0914 \nL 135.934103 69.524491 \nL 136.524921 69.524491 \nL 136.82033 73.807946 \nL 137.115739 69.524491 \nL 137.411148 73.807946 \nL 137.706557 65.241037 \nL 138.001966 65.241037 \nL 138.297375 73.807946 \nL 138.592784 78.0914 \nL 139.479012 65.241037 \nL 139.774421 69.524491 \nL 140.06983 69.524491 \nL 140.365239 65.241037 \nL 140.660648 65.241037 \nL 140.956057 69.524491 \nL 141.251466 82.374855 \nL 141.546875 73.807946 \nL 141.842284 78.0914 \nL 142.433102 78.0914 \nL 142.728511 86.658309 \nL 143.02392 82.374855 \nL 143.319329 86.658309 \nL 144.500965 69.524491 \nL 144.796374 73.807946 \nL 145.387193 73.807946 \nL 145.682602 78.0914 \nL 146.27342 69.524491 \nL 146.864238 78.0914 \nL 147.159647 78.0914 \nL 147.455056 82.374855 \nL 147.750465 69.524491 \nL 148.045874 78.0914 \nL 148.341283 78.0914 \nL 148.636692 73.807946 \nL 148.932101 78.0914 \nL 149.22751 69.524491 \nL 149.522919 78.0914 \nL 149.818328 78.0914 \nL 150.113737 73.807946 \nL 150.409146 78.0914 \nL 151.886192 78.0914 \nL 152.181601 73.807946 \nL 152.47701 73.807946 \nL 152.772419 82.374855 \nL 153.067828 73.807946 \nL 153.363237 73.807946 \nL 153.658646 82.374855 \nL 153.954055 78.0914 \nL 154.249464 86.658309 \nL 154.544873 73.807946 \nL 154.840282 73.807946 \nL 155.135691 69.524491 \nL 155.4311 78.0914 \nL 156.021918 78.0914 \nL 156.317327 86.658309 \nL 156.612736 78.0914 \nL 156.908146 78.0914 \nL 157.498964 69.524491 \nL 157.794373 69.524491 \nL 158.089782 73.807946 \nL 158.6806 73.807946 \nL 158.976009 69.524491 \nL 159.271418 73.807946 \nL 159.566827 73.807946 \nL 160.157645 65.241037 \nL 161.043872 65.241037 \nL 161.339281 73.807946 \nL 161.930099 65.241037 \nL 162.225508 69.524491 \nL 162.520917 65.241037 \nL 163.111736 65.241037 \nL 163.407145 73.807946 \nL 163.702554 69.524491 \nL 164.88419 69.524491 \nL 165.179599 60.957583 \nL 165.475008 65.241037 \nL 166.065826 65.241037 \nL 166.361235 69.524491 \nL 166.656644 69.524491 \nL 166.952053 65.241037 \nL 167.542871 65.241037 \nL 167.83828 69.524491 \nL 168.133689 65.241037 \nL 168.429098 69.524491 \nL 168.724508 69.524491 \nL 169.019917 73.807946 \nL 169.315326 73.807946 \nL 169.906144 65.241037 \nL 170.201553 69.524491 \nL 170.496962 65.241037 \nL 170.792371 65.241037 \nL 171.08778 69.524491 \nL 171.678598 69.524491 \nL 171.974007 65.241037 \nL 172.564825 65.241037 \nL 172.860234 60.957583 \nL 173.155643 65.241037 \nL 173.746461 65.241037 \nL 174.04187 60.957583 \nL 174.337279 65.241037 \nL 175.223507 65.241037 \nL 175.518916 69.524491 \nL 175.814325 65.241037 \nL 176.995961 65.241037 \nL 177.29137 69.524491 \nL 177.586779 65.241037 \nL 178.177597 65.241037 \nL 178.473006 69.524491 \nL 178.768415 65.241037 \nL 179.063824 65.241037 \nL 179.359233 56.674147 \nL 179.654642 69.524491 \nL 179.950051 69.524491 \nL 180.24546 60.957583 \nL 181.427097 60.957583 \nL 181.722506 69.524491 \nL 182.017915 60.957583 \nL 182.313324 65.241037 \nL 182.904142 65.241037 \nL 183.199551 60.957583 \nL 183.49496 69.524491 \nL 183.790369 69.524491 \nL 184.085778 65.241037 \nL 184.676596 65.241037 \nL 184.972005 60.957583 \nL 185.267414 60.957583 \nL 185.562823 69.524491 \nL 185.858232 69.524491 \nL 186.153641 56.674147 \nL 186.44905 60.957583 \nL 187.039869 60.957583 \nL 187.630687 69.524491 \nL 187.926096 65.241037 \nL 188.812323 65.241037 \nL 189.69855 78.0914 \nL 189.993959 78.0914 \nL 190.289368 65.241037 \nL 190.880186 73.807946 \nL 191.175595 69.524491 \nL 192.061822 69.524491 \nL 192.357231 73.807946 \nL 192.94805 65.241037 \nL 193.243459 65.241037 \nL 193.538868 60.957583 \nL 193.834277 69.524491 \nL 194.425095 69.524491 \nL 194.720504 65.241037 \nL 195.015913 69.524491 \nL 195.311322 69.524491 \nL 195.606731 73.807946 \nL 196.197549 65.241037 \nL 196.492958 69.524491 \nL 196.788367 65.241037 \nL 197.674594 65.241037 \nL 197.970003 73.807946 \nL 198.560822 73.807946 \nL 198.856231 69.524491 \nL 199.15164 73.807946 \nL 199.447049 69.524491 \nL 199.742458 60.957583 \nL 200.037867 69.524491 \nL 200.333276 73.807946 \nL 202.696548 73.807946 \nL 203.287366 65.241037 \nL 203.878184 65.241037 \nL 204.173593 73.807946 \nL 204.469003 65.241037 \nL 205.35523 65.241037 \nL 205.650639 69.524491 \nL 205.946048 65.241037 \nL 206.241457 69.524491 \nL 206.536866 65.241037 \nL 206.832275 69.524491 \nL 207.423093 69.524491 \nL 207.718502 65.241037 \nL 208.013911 65.241037 \nL 208.30932 69.524491 \nL 208.604729 65.241037 \nL 209.786365 65.241037 \nL 210.081774 69.524491 \nL 210.968002 69.524491 \nL 211.263411 73.807946 \nL 211.55882 69.524491 \nL 211.854229 78.0914 \nL 212.149638 69.524491 \nL 212.445047 69.524491 \nL 212.740456 73.807946 \nL 213.035865 69.524491 \nL 213.331274 78.0914 \nL 213.922092 69.524491 \nL 214.217501 73.807946 \nL 214.808319 65.241037 \nL 215.103728 69.524491 \nL 218.057819 69.524491 \nL 218.353228 65.241037 \nL 218.648637 69.524491 \nL 219.239455 69.524491 \nL 219.534864 65.241037 \nL 220.125682 65.241037 \nL 220.421091 69.524491 \nL 220.7165 69.524491 \nL 221.011909 65.241037 \nL 221.307318 65.241037 \nL 221.602727 60.957583 \nL 221.898136 65.241037 \nL 222.193545 60.957583 \nL 222.488955 60.957583 \nL 222.784364 69.524491 \nL 223.079773 69.524491 \nL 223.375182 73.807946 \nL 223.670591 69.524491 \nL 223.966 69.524491 \nL 224.261409 73.807946 \nL 224.556818 69.524491 \nL 224.852227 78.0914 \nL 225.443045 69.524491 \nL 225.738454 73.807946 \nL 226.329272 73.807946 \nL 226.624681 69.524491 \nL 226.92009 69.524491 \nL 227.215499 73.807946 \nL 227.510908 69.524491 \nL 229.578772 69.524491 \nL 229.874181 65.241037 \nL 230.16959 69.524491 \nL 230.464999 69.524491 \nL 230.760408 65.241037 \nL 231.646635 65.241037 \nL 231.942044 60.957583 \nL 232.532862 60.957583 \nL 232.828271 65.241037 \nL 233.419089 65.241037 \nL 233.714498 69.524491 \nL 234.009907 60.957583 \nL 234.305317 65.241037 \nL 234.600726 60.957583 \nL 234.896135 60.957583 \nL 235.191544 65.241037 \nL 240.213497 65.241037 \nL 240.508907 60.957583 \nL 241.395134 60.957583 \nL 241.690543 65.241037 \nL 242.57677 65.241037 \nL 242.872179 69.524491 \nL 243.167588 60.957583 \nL 243.462997 65.241037 \nL 244.349224 65.241037 \nL 244.644633 69.524491 \nL 245.235451 69.524491 \nL 245.826269 60.957583 \nL 246.121678 65.241037 \nL 246.712497 65.241037 \nL 247.007906 73.807946 \nL 247.303315 73.807946 \nL 247.598724 69.524491 \nL 249.075769 69.524491 \nL 249.371178 65.241037 \nL 249.666587 69.524491 \nL 249.961996 65.241037 \nL 250.257405 69.524491 \nL 250.552814 69.524491 \nL 250.848223 73.807946 \nL 251.143632 69.524491 \nL 251.73445 69.524491 \nL 252.029859 73.807946 \nL 253.506905 73.807946 \nL 253.802314 78.0914 \nL 254.097723 69.524491 \nL 254.393132 73.807946 \nL 254.688541 69.524491 \nL 258.528859 69.524491 \nL 258.824268 65.241037 \nL 259.119677 65.241037 \nL 259.710495 73.807946 \nL 260.596722 60.957583 \nL 260.892131 65.241037 \nL 261.18754 65.241037 \nL 261.482949 69.524491 \nL 261.778358 69.524491 \nL 262.073767 65.241037 \nL 262.369176 65.241037 \nL 262.664585 69.524491 \nL 262.959994 69.524491 \nL 263.255403 65.241037 \nL 263.550812 65.241037 \nL 263.846221 69.524491 \nL 265.027858 69.524491 \nL 265.323267 65.241037 \nL 265.618676 69.524491 \nL 265.914085 65.241037 \nL 266.209494 69.524491 \nL 267.686539 69.524491 \nL 267.981948 65.241037 \nL 268.277357 69.524491 \nL 268.868175 69.524491 \nL 269.163584 65.241037 \nL 270.64063 65.241037 \nL 270.936039 60.957583 \nL 271.231448 60.957583 \nL 271.526857 65.241037 \nL 271.822266 65.241037 \nL 272.117675 69.524491 \nL 272.413084 65.241037 \nL 273.003902 65.241037 \nL 273.59472 56.674147 \nL 273.890129 60.957583 \nL 274.185538 60.957583 \nL 274.480947 69.524491 \nL 275.071765 60.957583 \nL 275.367174 65.241037 \nL 275.662583 60.957583 \nL 275.957992 60.957583 \nL 276.253402 56.674147 \nL 276.548811 56.674147 \nL 276.84422 60.957583 \nL 277.139629 60.957583 \nL 277.435038 69.524491 \nL 277.730447 65.241037 \nL 278.321265 65.241037 \nL 278.616674 60.957583 \nL 278.912083 60.957583 \nL 279.207492 65.241037 \nL 279.79831 65.241037 \nL 280.093719 69.524491 \nL 280.684537 60.957583 \nL 280.979946 65.241037 \nL 281.866173 65.241037 \nL 282.161583 69.524491 \nL 283.04781 56.674147 \nL 283.343219 60.957583 \nL 284.229446 60.957583 \nL 284.524855 65.241037 \nL 285.411082 65.241037 \nL 285.706491 60.957583 \nL 286.0019 60.957583 \nL 286.297309 65.241037 \nL 286.592718 65.241037 \nL 286.888127 69.524491 \nL 287.183536 65.241037 \nL 287.774354 65.241037 \nL 288.069764 60.957583 \nL 288.660582 60.957583 \nL 288.955991 65.241037 \nL 291.023854 65.241037 \nL 291.319263 69.524491 \nL 291.614672 69.524491 \nL 291.910081 65.241037 \nL 294.864172 65.241037 \nL 295.159581 60.957583 \nL 295.45499 65.241037 \nL 295.750399 65.241037 \nL 296.045808 60.957583 \nL 296.636626 60.957583 \nL 296.932035 65.241037 \nL 297.227444 65.241037 \nL 297.522853 60.957583 \nL 298.113671 60.957583 \nL 298.40908 65.241037 \nL 298.704489 65.241037 \nL 298.999898 60.957583 \nL 300.476944 60.957583 \nL 300.772353 65.241037 \nL 301.067762 65.241037 \nL 301.363171 69.524491 \nL 301.65858 65.241037 \nL 301.953989 65.241037 \nL 302.249398 60.957583 \nL 302.544807 60.957583 \nL 302.840216 65.241037 \nL 303.135625 65.241037 \nL 303.431034 69.524491 \nL 303.726443 69.524491 \nL 304.021852 60.957583 \nL 304.317261 60.957583 \nL 304.61267 65.241037 \nL 305.203488 65.241037 \nL 305.498897 60.957583 \nL 306.680534 60.957583 \nL 306.975943 65.241037 \nL 307.271352 60.957583 \nL 307.566761 60.957583 \nL 307.86217 65.241037 \nL 308.157579 65.241037 \nL 308.452988 60.957583 \nL 309.043806 60.957583 \nL 309.339215 65.241037 \nL 309.634624 65.241037 \nL 309.930033 60.957583 \nL 310.225442 60.957583 \nL 310.520851 65.241037 \nL 311.702487 65.241037 \nL 311.997897 69.524491 \nL 312.293306 65.241037 \nL 314.06576 65.241037 \nL 314.361169 60.957583 \nL 314.656578 60.957583 \nL 314.951987 65.241037 \nL 315.247396 60.957583 \nL 315.542805 65.241037 \nL 315.838214 60.957583 \nL 316.724441 60.957583 \nL 317.01985 65.241037 \nL 317.610668 65.241037 \nL 317.906078 60.957583 \nL 318.496896 60.957583 \nL 318.792305 65.241037 \nL 319.678532 65.241037 \nL 319.973941 60.957583 \nL 320.564759 69.524491 \nL 320.860168 65.241037 \nL 322.337213 65.241037 \nL 322.632622 69.524491 \nL 322.928031 65.241037 \nL 323.22344 69.524491 \nL 324.700486 69.524491 \nL 325.291304 60.957583 \nL 325.586713 65.241037 \nL 325.882122 60.957583 \nL 326.177531 65.241037 \nL 328.245394 65.241037 \nL 328.540803 60.957583 \nL 328.836212 65.241037 \nL 329.131621 65.241037 \nL 329.42703 60.957583 \nL 329.72244 60.957583 \nL 330.017849 65.241037 \nL 330.904076 65.241037 \nL 331.199485 60.957583 \nL 331.790303 60.957583 \nL 332.085712 56.674147 \nL 332.381121 56.674147 \nL 332.67653 60.957583 \nL 332.971939 60.957583 \nL 333.267348 65.241037 \nL 335.630621 65.241037 \nL 335.92603 69.524491 \nL 336.221439 65.241037 \nL 336.812257 65.241037 \nL 337.107666 60.957583 \nL 337.993893 60.957583 \nL 338.289302 65.241037 \nL 338.584711 60.957583 \nL 339.175529 60.957583 \nL 339.470938 65.241037 \nL 339.766347 60.957583 \nL 340.061756 60.957583 \nL 340.357165 56.674147 \nL 340.652574 56.674147 \nL 340.947983 60.957583 \nL 341.243392 60.957583 \nL 341.538801 56.674147 \nL 341.834211 65.241037 \nL 342.12962 69.524491 \nL 343.311256 69.524491 \nL 343.606665 65.241037 \nL 343.902074 65.241037 \nL 344.197483 60.957583 \nL 345.08371 60.957583 \nL 345.379119 65.241037 \nL 345.674528 60.957583 \nL 345.969937 60.957583 \nL 346.265346 65.241037 \nL 346.560755 65.241037 \nL 346.856164 69.524491 \nL 347.151573 60.957583 \nL 347.446982 65.241037 \nL 349.810255 65.241037 \nL 350.105664 69.524491 \nL 350.401073 65.241037 \nL 351.582709 65.241037 \nL 351.878118 60.957583 \nL 352.468936 60.957583 \nL 352.764345 65.241037 \nL 353.059754 60.957583 \nL 353.945982 60.957583 \nL 354.241391 65.241037 \nL 354.5368 60.957583 \nL 355.423027 60.957583 \nL 355.718436 65.241037 \nL 356.013845 60.957583 \nL 356.309254 65.241037 \nL 356.604663 65.241037 \nL 356.900072 60.957583 \nL 359.854163 60.957583 \nL 360.149572 65.241037 \nL 360.444981 65.241037 \nL 360.74039 60.957583 \nL 361.035799 60.957583 \nL 361.331208 65.241037 \nL 361.626617 60.957583 \nL 365.466935 60.957583 \nL 365.762344 65.241037 \nL 366.057753 65.241037 \nL 366.353162 60.957583 \nL 366.648571 69.524491 \nL 366.94398 60.957583 \nL 367.239389 65.241037 \nL 368.125616 65.241037 \nL 368.421025 69.524491 \nL 369.89807 69.524491 \nL 370.193479 73.807946 \nL 371.670525 73.807946 \nL 371.965934 69.524491 \nL 372.261343 73.807946 \nL 372.556752 65.241037 \nL 374.329206 65.241037 \nL 374.624615 60.957583 \nL 375.215433 60.957583 \nL 375.510842 69.524491 \nL 376.397069 69.524491 \nL 376.692478 73.807946 \nL 376.987887 69.524491 \nL 377.578706 69.524491 \nL 377.874115 65.241037 \nL 378.169524 69.524491 \nL 378.464933 65.241037 \nL 379.35116 65.241037 \nL 379.646569 69.524491 \nL 379.941978 65.241037 \nL 380.237387 69.524491 \nL 380.532796 65.241037 \nL 381.419023 65.241037 \nL 381.714432 73.807946 \nL 382.30525 73.807946 \nL 382.600659 65.241037 \nL 382.896068 60.957583 \nL 383.191477 65.241037 \nL 383.486887 60.957583 \nL 384.668523 60.957583 \nL 384.668523 60.957583 \n\" clip-path=\"url(#pe73b653f20)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 288.430125 \nL 43.78125 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 400.90125 288.430125 \nL 400.90125 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 288.430125 \nL 400.90125 288.430125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 400.90125 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- model accuracy -->\n    <g transform=\"translate(174.679688 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-6d\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"97.412109\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"158.59375\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"222.070312\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"283.59375\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"311.376953\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"343.164062\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"404.443359\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"459.423828\"/>\n     <use xlink:href=\"#DejaVuSans-75\" x=\"514.404297\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"577.783203\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"618.896484\"/>\n     <use xlink:href=\"#DejaVuSans-63\" x=\"680.175781\"/>\n     <use xlink:href=\"#DejaVuSans-79\" x=\"735.15625\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 50.78125 59.674375 \nL 132.015625 59.674375 \nQ 134.015625 59.674375 134.015625 57.674375 \nL 134.015625 29.318125 \nQ 134.015625 27.318125 132.015625 27.318125 \nL 50.78125 27.318125 \nQ 48.78125 27.318125 48.78125 29.318125 \nL 48.78125 57.674375 \nQ 48.78125 59.674375 50.78125 59.674375 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 52.78125 35.416562 \nL 62.78125 35.416562 \nL 72.78125 35.416562 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- train -->\n     <g transform=\"translate(80.78125 38.916562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 52.78125 50.094687 \nL 62.78125 50.094687 \nL 72.78125 50.094687 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_20\">\n     <!-- validation -->\n     <g transform=\"translate(80.78125 53.594687) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe73b653f20\">\n   <rect x=\"43.78125\" y=\"22.318125\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ2_9JAYeMQn",
        "outputId": "f0ee4567-8d21-46f1-c00f-48d7f4f7a3ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5032 - categorical_accuracy: 0.8571\n",
            "Model Accuracy: 85.71%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(X_test, Y_test)\n",
        "print(\"Model Accuracy: {:5.2f}%\".format(100*acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacez 'chemin_vers_votre_audio.wav' par le chemin réel vers votre fichier audio WAV\n",
        "audio_path_test = '/content/W.wav'\n",
        "\n",
        "# Charger l'audio\n",
        "audio_test, _ = librosa.load(audio_path_test, sr=sample_rate, res_type='fft', offset=0.5)\n",
        "\n",
        "# Appliquer le VAD\n",
        "waveform_test = VAD(audio_test, sample_rate, int(db))\n",
        "waveform_pad_test = np.zeros((int(sample_rate * 5,)))\n",
        "waveform_pad_test[:len(waveform_test)] = waveform_test\n",
        "\n",
        "# Calculer les caractéristiques\n",
        "mfccs_test = np.mean(librosa.feature.mfcc(y=waveform_pad_test, sr=sample_rate, n_mfcc=n_mfccs, n_fft=1024, win_length=FRAME_LENGTH, hop_length=HOP_LENGTH, window='hamming', n_mels=128, fmax=sample_rate/2).T, axis=0)\n",
        "zcr_test = np.mean(librosa.feature.zero_crossing_rate(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "rmse_test = np.mean(librosa.feature.rms(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "pitch_test, magnitude_test = librosa.piptrack(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming')\n",
        "pitch_test = np.mean(pitch_test[np.where(magnitude_test > 0)])\n",
        "centroid_test = np.mean(librosa.feature.spectral_centroid(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming'))\n",
        "audio_features_test = np.append(mfccs_test, (rmse_test, centroid_test, zcr_test, pitch_test))\n",
        "\n",
        "# Normaliser les caractéristiques\n",
        "audio_features_test = scaler.transform(audio_features_test.reshape(1, -1))\n",
        "audio_features_test = audio_features_test[..., np.newaxis]\n",
        "\n",
        "# Effectuer la prédiction\n",
        "predictions = model.predict(audio_features_test)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Probabilités pour chaque classe :\", predictions)\n",
        "predicted_class = np.argmax(predictions)\n",
        "predicted_label = label_encoder.classes_[predicted_class]\n",
        "print(\"Classe prédite :\", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hNnp4H2fcxR",
        "outputId": "fa04f5fd-f92d-4e69-e59c-5a5190908d82"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78ad7560b130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 499ms/step\n",
            "Probabilités pour chaque classe : [[7.5074697e-01 3.0883902e-03 2.4527468e-01 2.0611694e-04 3.7748126e-05\n",
            "  8.4806170e-06 6.3734601e-04 2.3281486e-07]]\n",
            "Classe prédite : anger\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacez 'chemin_vers_votre_audio.wav' par le chemin réel vers votre fichier audio WAV\n",
        "audio_path_test = '/content/videos_cnn-audio_data_cat_004ae714_nohash_0.wav'\n",
        "\n",
        "# Charger l'audio\n",
        "audio_test, _ = librosa.load(audio_path_test, sr=sample_rate, res_type='fft', offset=0.5)\n",
        "\n",
        "# Appliquer le VAD\n",
        "waveform_test = VAD(audio_test, sample_rate, int(db))\n",
        "waveform_pad_test = np.zeros((int(sample_rate * 5,)))\n",
        "waveform_pad_test[:len(waveform_test)] = waveform_test\n",
        "\n",
        "# Calculer les caractéristiques\n",
        "mfccs_test = np.mean(librosa.feature.mfcc(y=waveform_pad_test, sr=sample_rate, n_mfcc=n_mfccs, n_fft=1024, win_length=FRAME_LENGTH, hop_length=HOP_LENGTH, window='hamming', n_mels=128, fmax=sample_rate/2).T, axis=0)\n",
        "zcr_test = np.mean(librosa.feature.zero_crossing_rate(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "rmse_test = np.mean(librosa.feature.rms(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "pitch_test, magnitude_test = librosa.piptrack(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming')\n",
        "pitch_test = np.mean(pitch_test[np.where(magnitude_test > 0)])\n",
        "centroid_test = np.mean(librosa.feature.spectral_centroid(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming'))\n",
        "audio_features_test = np.append(mfccs_test, (rmse_test, centroid_test, zcr_test, pitch_test))\n",
        "\n",
        "# Normaliser les caractéristiques\n",
        "audio_features_test = scaler.transform(audio_features_test.reshape(1, -1))\n",
        "audio_features_test = audio_features_test[..., np.newaxis]\n",
        "\n",
        "# Effectuer la prédiction\n",
        "predictions = model.predict(audio_features_test)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Probabilités pour chaque classe :\", predictions)\n",
        "predicted_class = np.argmax(predictions)\n",
        "predicted_label = label_encoder.classes_[predicted_class]\n",
        "print(\"Classe prédite :\", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCBngAG3FoEA",
        "outputId": "53c0c8b8-819c-48cc-d7f6-505516ebe360"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "Probabilités pour chaque classe : [[1.3619615e-04 2.0197664e-05 1.0686836e-04 9.7799975e-01 1.4401572e-04\n",
            "  5.9994218e-05 1.8113419e-04 2.1351853e-02]]\n",
            "Classe prédite : fear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacez 'chemin_vers_votre_audio.wav' par le chemin réel vers votre fichier audio WAV\n",
        "audio_path_test = '/content/sadness_audio.wav'\n",
        "\n",
        "# Charger l'audio\n",
        "audio_test, _ = librosa.load(audio_path_test, sr=sample_rate, res_type='fft', offset=0.5)\n",
        "\n",
        "# Appliquer le VAD\n",
        "waveform_test = VAD(audio_test, sample_rate, int(db))\n",
        "waveform_pad_test = np.zeros((int(sample_rate * 5,)))\n",
        "waveform_pad_test[:len(waveform_test)] = waveform_test\n",
        "\n",
        "# Calculer les caractéristiques\n",
        "mfccs_test = np.mean(librosa.feature.mfcc(y=waveform_pad_test, sr=sample_rate, n_mfcc=n_mfccs, n_fft=1024, win_length=FRAME_LENGTH, hop_length=HOP_LENGTH, window='hamming', n_mels=128, fmax=sample_rate/2).T, axis=0)\n",
        "zcr_test = np.mean(librosa.feature.zero_crossing_rate(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "rmse_test = np.mean(librosa.feature.rms(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "pitch_test, magnitude_test = librosa.piptrack(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming')\n",
        "pitch_test = np.mean(pitch_test[np.where(magnitude_test > 0)])\n",
        "centroid_test = np.mean(librosa.feature.spectral_centroid(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming'))\n",
        "audio_features_test = np.append(mfccs_test, (rmse_test, centroid_test, zcr_test, pitch_test))\n",
        "\n",
        "# Normaliser les caractéristiques\n",
        "audio_features_test = scaler.transform(audio_features_test.reshape(1, -1))\n",
        "audio_features_test = audio_features_test[..., np.newaxis]\n",
        "\n",
        "# Effectuer la prédiction\n",
        "predictions = model.predict(audio_features_test)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Probabilités pour chaque classe :\", predictions)\n",
        "predicted_class = np.argmax(predictions)\n",
        "predicted_label = label_encoder.classes_[predicted_class]\n",
        "print(\"Classe prédite :\", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvZwMXduF4Vp",
        "outputId": "be00934e-29cd-4cf8-f6e6-2762bbe52d09"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "Probabilités pour chaque classe : [[7.6512997e-06 2.1828872e-03 3.9141618e-05 1.3968818e-02 4.4191707e-04\n",
            "  3.5773015e-03 9.7965688e-01 1.2534960e-04]]\n",
            "Classe prédite : sadness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacez 'chemin_vers_votre_audio.wav' par le chemin réel vers votre fichier audio WAV\n",
        "audio_path_test = '/content/surprised_audio.wav'\n",
        "\n",
        "# Charger l'audio\n",
        "audio_test, _ = librosa.load(audio_path_test, sr=sample_rate, res_type='fft', offset=0.5)\n",
        "\n",
        "# Appliquer le VAD\n",
        "waveform_test = VAD(audio_test, sample_rate, int(db))\n",
        "waveform_pad_test = np.zeros((int(sample_rate * 5,)))\n",
        "waveform_pad_test[:len(waveform_test)] = waveform_test\n",
        "\n",
        "# Calculer les caractéristiques\n",
        "mfccs_test = np.mean(librosa.feature.mfcc(y=waveform_pad_test, sr=sample_rate, n_mfcc=n_mfccs, n_fft=1024, win_length=FRAME_LENGTH, hop_length=HOP_LENGTH, window='hamming', n_mels=128, fmax=sample_rate/2).T, axis=0)\n",
        "zcr_test = np.mean(librosa.feature.zero_crossing_rate(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "rmse_test = np.mean(librosa.feature.rms(y=waveform_pad_test, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH))\n",
        "pitch_test, magnitude_test = librosa.piptrack(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming')\n",
        "pitch_test = np.mean(pitch_test[np.where(magnitude_test > 0)])\n",
        "centroid_test = np.mean(librosa.feature.spectral_centroid(y=waveform_test, sr=sample_rate, n_fft=1024, hop_length=HOP_LENGTH, win_length=FRAME_LENGTH, window='hamming'))\n",
        "audio_features_test = np.append(mfccs_test, (rmse_test, centroid_test, zcr_test, pitch_test))\n",
        "\n",
        "# Normaliser les caractéristiques\n",
        "audio_features_test = scaler.transform(audio_features_test.reshape(1, -1))\n",
        "audio_features_test = audio_features_test[..., np.newaxis]\n",
        "\n",
        "# Effectuer la prédiction\n",
        "predictions = model.predict(audio_features_test)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Probabilités pour chaque classe :\", predictions)\n",
        "predicted_class = np.argmax(predictions)\n",
        "predicted_label = label_encoder.classes_[predicted_class]\n",
        "print(\"Classe prédite :\", predicted_label)\n"
      ],
      "metadata": {
        "id": "6y5q01qIHO7i",
        "outputId": "c0ff0eb6-337b-428e-c2f9-218fea0d474b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "Probabilités pour chaque classe : [[3.0300656e-04 4.2291549e-03 2.6474059e-05 3.9887428e-03 5.5739332e-02\n",
            "  5.0402127e-02 7.7046995e-04 8.8454062e-01]]\n",
            "Classe prédite : surprised\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}